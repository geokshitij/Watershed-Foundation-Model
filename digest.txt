Directory structure:
└── lightly/
    ├── README.md
    ├── pyproject.toml
    ├── benchmarks/
    │   └── imagenet/
    │       └── resnet50/
    │           └── README.md
    ├── docs/
    │   └── README.md
    ├── examples/
    │   └── README.md
    ├── lightly/
    │   ├── core.py
    │   ├── api/
    │   │   ├── __init__.py
    │   │   ├── _version_checking.py
    │   │   ├── api_workflow_artifacts.py
    │   │   ├── api_workflow_client.py
    │   │   ├── api_workflow_collaboration.py
    │   │   ├── api_workflow_compute_worker.py
    │   │   ├── api_workflow_datasets.py
    │   │   ├── api_workflow_datasource_listing.py
    │   │   ├── api_workflow_datasources.py
    │   │   ├── api_workflow_download_dataset.py
    │   │   ├── api_workflow_export.py
    │   │   ├── api_workflow_predictions.py
    │   │   ├── api_workflow_selection.py
    │   │   ├── api_workflow_tags.py
    │   │   ├── api_workflow_upload_embeddings.py
    │   │   ├── api_workflow_upload_metadata.py
    │   │   ├── bitmask.py
    │   │   ├── download.py
    │   │   ├── patch.py
    │   │   ├── retry_utils.py
    │   │   ├── serve.py
    │   │   ├── swagger_api_client.py
    │   │   ├── swagger_rest_client.py
    │   │   └── utils.py
    │   ├── cli/
    │   │   ├── __init__.py
    │   │   ├── _cli_simclr.py
    │   │   ├── _helpers.py
    │   │   ├── crop_cli.py
    │   │   ├── download_cli.py
    │   │   ├── embed_cli.py
    │   │   ├── lightly_cli.py
    │   │   ├── serve_cli.py
    │   │   ├── train_cli.py
    │   │   ├── version_cli.py
    │   │   └── config/
    │   │       ├── __init__.py
    │   │       └── get_config.py
    │   ├── data/
    │   │   ├── __init__.py
    │   │   ├── _helpers.py
    │   │   ├── _image.py
    │   │   ├── _image_loaders.py
    │   │   ├── _utils.py
    │   │   ├── _video.py
    │   │   ├── collate.py
    │   │   ├── dataset.py
    │   │   ├── lightly_subset.py
    │   │   └── multi_view_collate.py
    │   ├── loss/
    │   │   ├── __init__.py
    │   │   ├── barlow_twins_loss.py
    │   │   ├── dcl_loss.py
    │   │   ├── detcon_loss.py
    │   │   ├── dino_loss.py
    │   │   ├── directclr_loss.py
    │   │   ├── emp_ssl_loss.py
    │   │   ├── hypersphere_loss.py
    │   │   ├── ibot_loss.py
    │   │   ├── koleo_loss.py
    │   │   ├── macl_loss.py
    │   │   ├── memory_bank.py
    │   │   ├── mmcr_loss.py
    │   │   ├── msn_loss.py
    │   │   ├── negative_cosine_similarity.py
    │   │   ├── ntx_ent_loss.py
    │   │   ├── pmsn_loss.py
    │   │   ├── swav_loss.py
    │   │   ├── sym_neg_cos_sim_loss.py
    │   │   ├── tico_loss.py
    │   │   ├── vicreg_loss.py
    │   │   ├── vicregl_loss.py
    │   │   ├── wmse_loss.py
    │   │   └── regularizer/
    │   │       ├── __init__.py
    │   │       └── co2.py
    │   ├── models/
    │   │   ├── __init__.py
    │   │   ├── _momentum.py
    │   │   ├── barlowtwins.py
    │   │   ├── batchnorm.py
    │   │   ├── byol.py
    │   │   ├── moco.py
    │   │   ├── nnclr.py
    │   │   ├── resnet.py
    │   │   ├── simclr.py
    │   │   ├── simsiam.py
    │   │   ├── utils.py
    │   │   ├── zoo.py
    │   │   └── modules/
    │   │       ├── __init__.py
    │   │       ├── center.py
    │   │       ├── heads.py
    │   │       ├── heads_timm.py
    │   │       ├── ijepa.py
    │   │       ├── ijepa_timm.py
    │   │       ├── masked_autoencoder.py
    │   │       ├── masked_autoencoder_timm.py
    │   │       ├── masked_causal_vision_transformer.py
    │   │       ├── masked_vision_transformer.py
    │   │       ├── masked_vision_transformer_timm.py
    │   │       ├── masked_vision_transformer_torchvision.py
    │   │       ├── memory_bank.py
    │   │       └── nn_memory_bank.py
    │   ├── transforms/
    │   │   ├── __init__.py
    │   │   ├── add_grid_transform.py
    │   │   ├── aim_transform.py
    │   │   ├── amplitude_rescale_transform.py
    │   │   ├── byol_transform.py
    │   │   ├── densecl_transform.py
    │   │   ├── detcon_transform.py
    │   │   ├── dino_transform.py
    │   │   ├── fast_siam_transform.py
    │   │   ├── fda_transform.py
    │   │   ├── gaussian_blur.py
    │   │   ├── gaussian_mixture_masks_transform.py
    │   │   ├── ibot_transform.py
    │   │   ├── ijepa_transform.py
    │   │   ├── image_grid_transform.py
    │   │   ├── irfft2d_transform.py
    │   │   ├── jigsaw.py
    │   │   ├── mae_transform.py
    │   │   ├── mmcr_transform.py
    │   │   ├── moco_transform.py
    │   │   ├── msn_transform.py
    │   │   ├── multi_crop_transform.py
    │   │   ├── multi_view_transform.py
    │   │   ├── multi_view_transform_v2.py
    │   │   ├── phase_shift_transform.py
    │   │   ├── pirl_transform.py
    │   │   ├── random_crop_and_flip_with_grid.py
    │   │   ├── random_frequency_mask_transform.py
    │   │   ├── rfft2d_transform.py
    │   │   ├── rotation.py
    │   │   ├── simclr_transform.py
    │   │   ├── simsiam_transform.py
    │   │   ├── smog_transform.py
    │   │   ├── solarize.py
    │   │   ├── swav_transform.py
    │   │   ├── tico_transform.py
    │   │   ├── torchvision_v2_compatibility.py
    │   │   ├── utils.py
    │   │   ├── vicreg_transform.py
    │   │   ├── vicregl_transform.py
    │   │   └── wmse_transform.py
    │   └── utils/
    │       ├── __init__.py
    │       ├── bounding_box.py
    │       ├── debug.py
    │       ├── dependency.py
    │       ├── dist.py
    │       ├── embeddings_2d.py
    │       ├── hipify.py
    │       ├── io.py
    │       ├── lars.py
    │       ├── optim.py
    │       ├── reordering.py
    │       ├── scheduler.py
    │       ├── version_compare.py
    │       ├── benchmarking/
    │       │   ├── __init__.py
    │       │   ├── benchmark_module.py
    │       │   ├── knn.py
    │       │   ├── knn_classifier.py
    │       │   ├── linear_classifier.py
    │       │   ├── metric_callback.py
    │       │   ├── online_linear_classifier.py
    │       │   └── topk.py
    │       └── cropping/
    │           ├── __init__.py
    │           ├── crop_image_by_bounding_boxes.py
    │           └── read_yolo_label_file.py
    ├── requirements/
    │   ├── README.md
    │   └── base.txt
    └── tests/
        └── UNMOCKED_end2end_tests/
            └── README.md

================================================
FILE: README.md
================================================
<a name="top"></a>
![LightlySSL self-supervised learning Logo](docs/logos/lightly_SSL_logo_crop.png)

[![GitHub](https://img.shields.io/github/license/lightly-ai/lightly)](https://github.com/lightly-ai/lightly/blob/master/LICENSE.txt)
[![Unit Tests](https://github.com/lightly-ai/lightly/workflows/Unit%20Tests/badge.svg)](https://github.com/lightly-ai/lightly/actions/workflows/test.yml)
[![PyPI](https://img.shields.io/pypi/v/lightly)](https://pypi.org/project/lightly/)
[![Downloads](https://static.pepy.tech/badge/lightly)](https://pepy.tech/project/lightly)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Discord](https://img.shields.io/discord/752876370337726585?logo=discord&logoColor=white&label=discord&color=7289da)](https://discord.gg/xvNJW94)
[![Twitter](https://img.shields.io/twitter/follow/LightlyAI)](https://x.com/LightlyAI)
[![codecov.io](https://codecov.io/github/lightly-ai/lightly/coverage.svg?branch=master)](https://app.codecov.io/gh/lightly-ai/lightly)


Lightly**SSL** is a computer vision framework for self-supervised learning.

- [Documentation](https://docs.lightly.ai/self-supervised-learning/)
- [Github](https://github.com/lightly-ai/lightly)
- [Discord](https://discord.gg/xvNJW94)

For a commercial version with more features, including Docker support and pretraining
models for embedding, classification, detection, and segmentation tasks with
a single command, please contact sales@lightly.ai.

We've also built a whole platform on top, with additional features for active learning
and [data curation](https://docs.lightly.ai/docs/what-is-lightly). If you're interested in the
Lightly Worker Solution to easily process millions of samples and run [powerful algorithms](https://docs.lightly.ai/docs/customize-a-selection)
on your data, check out [lightly.ai](https://www.lightly.ai). It's free to get started!

## Big News (April 15th, 2025) 🚀

We are excited to announce that you can now leverage SSL and distillation pretraining in just a few lines of code! We've worked hard to make self-supervised learning even more accessible with our new project [LightlyTrain](https://github.com/lightly-ai/lightly-train). Head over there to get started and supercharge your models! ⚡️

<a href="https://github.com/lightly-ai/lightly-train">
<p align="center">
<img src="https://github.com/lightly-ai/lightly-train/blob/main/docs/source/_static/lightly_train_light.svg" alt="LightlyTrain" width="300"/>
</p>
</a>


## Features

This self-supervised learning framework offers the following features:

- Modular framework, which exposes low-level building blocks such as loss functions and
  model heads.
- Easy to use and written in a PyTorch-like style.
- Supports custom backbone models for self-supervised pre-training.
- Support for distributed training using PyTorch Lightning.

### Supported Models

You can [find sample code for all the supported models here.](https://docs.lightly.ai/self-supervised-learning/examples/models.html) We provide PyTorch, PyTorch Lightning,
and PyTorch Lightning distributed examples for all models to kickstart your project.

**Models**:

| Model          | Year | Paper | Docs | Colab (PyTorch) | Colab (PyTorch Lightning) |
|----------------|------|-------|------|-----------------|----------------------------|
| AIM            | 2024 | [paper](https://arxiv.org/abs/2401.08541) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/aim.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/aim.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/aim.ipynb) |
| Barlow Twins   | 2021 | [paper](https://arxiv.org/abs/2103.03230) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/barlowtwins.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/barlowtwins.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/barlowtwins.ipynb) |
| BYOL           | 2020 | [paper](https://arxiv.org/abs/2006.07733) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/byol.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/byol.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/byol.ipynb) |
| DCL & DCLW     | 2021 | [paper](https://arxiv.org/abs/2110.06848) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/dcl.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/dcl.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/dcl.ipynb) |
| DenseCL        | 2021 | [paper](https://arxiv.org/abs/2011.09157) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/densecl.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/densecl.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/densecl.ipynb) |
| DINO           | 2021 | [paper](https://arxiv.org/abs/2104.14294) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/dino.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/dino.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/dino.ipynb) |
| DINOv2         | 2023 | [paper](https://arxiv.org/abs/2304.07193) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/dinov2.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/dinov2.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/dinov2.ipynb) |
| iBOT           | 2021 | [paper](https://arxiv.org/abs/2111.07832) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/ibot.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/ibot.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/ibot.ipynb) |
| MAE            | 2021 | [paper](https://arxiv.org/abs/2111.06377) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/mae.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/mae.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/mae.ipynb) |
| MSN            | 2022 | [paper](https://arxiv.org/abs/2204.07141) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/msn.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/msn.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/msn.ipynb) |
| MoCo           | 2019 | [paper](https://arxiv.org/abs/1911.05722) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/moco.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/moco.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/moco.ipynb) |
| NNCLR          | 2021 | [paper](https://arxiv.org/abs/2104.14548) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/nnclr.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/nnclr.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/nnclr.ipynb) |
| PMSN           | 2022 | [paper](https://arxiv.org/abs/2210.07277) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/pmsn.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/pmsn.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/pmsn.ipynb) |
| SimCLR         | 2020 | [paper](https://arxiv.org/abs/2002.05709) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/simclr.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/simclr.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/simclr.ipynb) |
| SimMIM         | 2022 | [paper](https://arxiv.org/abs/2111.09886) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/simmim.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/simmim.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/simmim.ipynb) |
| SimSiam        | 2021 | [paper](https://arxiv.org/abs/2011.10566) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/simsiam.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/simsiam.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/simsiam.ipynb) |
| SwaV           | 2020 | [paper](https://arxiv.org/abs/2006.09882) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/swav.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/swav.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/swav.ipynb) |
| VICReg         | 2021 | [paper](https://arxiv.org/abs/2105.04906) | [docs](https://docs.lightly.ai/self-supervised-learning/examples/vicreg.html) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch/vicreg.ipynb) | [![Open In Colab](https://img.shields.io/badge/Colab-PyTorch_Lightning-blue?logo=googlecolab)](https://colab.research.google.com/github/lightly-ai/lightly/blob/master/examples/notebooks/pytorch_lightning/vicreg.ipynb) |

## Tutorials

Want to jump to the tutorials and see Lightly in action?

- [Train MoCo on CIFAR-10](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_moco_memory_bank.html)
- [Train SimCLR on Clothing Data](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_simclr_clothing.html)
- [Train SimSiam on Satellite Images](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_simsiam_esa.html)
- [Use Lightly with Custom Augmentations](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_custom_augmentations.html)
- [Pre-train a Detectron2 Backbone with Lightly](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_pretrain_detectron2.html)
- [Finetuning Lightly Checkpoints](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_checkpoint_finetuning.html)
- [Using timm Models as Backbones](https://docs.lightly.ai/self-supervised-learning/tutorials/package/tutorial_timm_backbone.html)

Community and partner projects:

- [On-Device Deep Learning with Lightly on an ARM microcontroller](https://github.com/ARM-software/EndpointAI/tree/master/ProofOfConcepts/Vision/OpenMvMaskDefaults)

## Quick Start

Lightly requires **Python 3.7+**. We recommend installing Lightly in a **Linux** or **OSX** environment. Python 3.13 is not yet supported, as PyTorch itself lacks Python 3.13 compatibility.

### Dependencies

Due to the modular nature of the Lightly package some modules can be used with older versions of dependencies. However, to use all features as of today lightly requires the following dependencies:

- [PyTorch](https://pytorch.org/)>=1.11.0
- [Torchvision](https://pytorch.org/vision/stable/index.html)>=0.12.0
- [PyTorch Lightning](https://www.pytorchlightning.ai/index.html)>=1.7.1

Lightly is compatible with PyTorch and PyTorch Lightning v2.0+!

### Installation

You can install Lightly and its dependencies from PyPI with:

```
pip3 install lightly
```

We strongly recommend installing Lightly in a dedicated virtualenv to avoid conflicts with your system packages.

### Lightly in Action

With Lightly, you can use the latest self-supervised learning methods in a modular
way using the full power of PyTorch. Experiment with various backbones,
models, and loss functions. The framework has been designed to be easy to use
from the ground up. [Find more examples in our docs](https://docs.lightly.ai/self-supervised-learning/examples/models.html).

```python
import torch
import torchvision

from lightly import loss
from lightly import transforms
from lightly.data import LightlyDataset
from lightly.models.modules import heads


# Create a PyTorch module for the SimCLR model.
class SimCLR(torch.nn.Module):
    def __init__(self, backbone):
        super().__init__()
        self.backbone = backbone
        self.projection_head = heads.SimCLRProjectionHead(
            input_dim=512,  # Resnet18 features have 512 dimensions.
            hidden_dim=512,
            output_dim=128,
        )

    def forward(self, x):
        features = self.backbone(x).flatten(start_dim=1)
        z = self.projection_head(features)
        return z


# Use a resnet backbone from torchvision.
backbone = torchvision.models.resnet18()
# Ignore the classification head as we only want the features.
backbone.fc = torch.nn.Identity()

# Build the SimCLR model.
model = SimCLR(backbone)

# Prepare transform that creates multiple random views for every image.
transform = transforms.SimCLRTransform(input_size=32, cj_prob=0.5)


# Create a dataset from your image folder.
dataset = LightlyDataset(input_dir="./my/cute/cats/dataset/", transform=transform)

# Build a PyTorch dataloader.
dataloader = torch.utils.data.DataLoader(
    dataset,  # Pass the dataset to the dataloader.
    batch_size=128,  # A large batch size helps with the learning.
    shuffle=True,  # Shuffling is important!
)

# Lightly exposes building blocks such as loss functions.
criterion = loss.NTXentLoss(temperature=0.5)

# Get a PyTorch optimizer.
optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-6)

# Train the model.
for epoch in range(10):
    for (view0, view1), targets, filenames in dataloader:
        z0 = model(view0)
        z1 = model(view1)
        loss = criterion(z0, z1)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        print(f"loss: {loss.item():.5f}")
```

You can easily use another model like SimSiam by swapping the model and the
loss function.

```python
# PyTorch module for the SimSiam model.
class SimSiam(torch.nn.Module):
    def __init__(self, backbone):
        super().__init__()
        self.backbone = backbone
        self.projection_head = heads.SimSiamProjectionHead(512, 512, 128)
        self.prediction_head = heads.SimSiamPredictionHead(128, 64, 128)

    def forward(self, x):
        features = self.backbone(x).flatten(start_dim=1)
        z = self.projection_head(features)
        p = self.prediction_head(z)
        z = z.detach()
        return z, p


model = SimSiam(backbone)

# Use the SimSiam loss function.
criterion = loss.NegativeCosineSimilarity()
```

You can [find a more complete example for SimSiam here.](https://docs.lightly.ai/self-supervised-learning/examples/simsiam.html)

Use PyTorch Lightning to train the model:

```python
from pytorch_lightning import LightningModule, Trainer

class SimCLR(LightningModule):
    def __init__(self):
        super().__init__()
        resnet = torchvision.models.resnet18()
        resnet.fc = torch.nn.Identity()
        self.backbone = resnet
        self.projection_head = heads.SimCLRProjectionHead(512, 512, 128)
        self.criterion = loss.NTXentLoss()

    def forward(self, x):
        features = self.backbone(x).flatten(start_dim=1)
        z = self.projection_head(features)
        return z

    def training_step(self, batch, batch_index):
        (view0, view1), _, _ = batch
        z0 = self.forward(view0)
        z1 = self.forward(view1)
        loss = self.criterion(z0, z1)
        return loss

    def configure_optimizers(self):
        optim = torch.optim.SGD(self.parameters(), lr=0.06)
        return optim


model = SimCLR()
trainer = Trainer(max_epochs=10, devices=1, accelerator="gpu")
trainer.fit(model, dataloader)
```

See [our docs for a full PyTorch Lightning example.](https://docs.lightly.ai/self-supervised-learning/examples/simclr.html)

Or train the model on 4 GPUs:

```python

# Use distributed version of loss functions.
criterion = loss.NTXentLoss(gather_distributed=True)

trainer = Trainer(
    max_epochs=10,
    devices=4,
    accelerator="gpu",
    strategy="ddp",
    sync_batchnorm=True,
    use_distributed_sampler=True,  # or replace_sampler_ddp=True for PyTorch Lightning <2.0
)
trainer.fit(model, dataloader)
```

We provide multi-GPU training examples with distributed gather and synchronized BatchNorm.
[Have a look at our docs regarding distributed training.](https://docs.lightly.ai/self-supervised-learning/getting_started/distributed_training.html)

## Benchmarks

Implemented models and their performance on various datasets. Hyperparameters are not
tuned for maximum accuracy. For detailed results and more information about the benchmarks click
[here](https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html).

### ImageNet1k

[ImageNet1k benchmarks](https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html#imagenet1k)

**Note**: Evaluation settings are based on these papers:

- Linear: [SimCLR](https://arxiv.org/abs/2002.05709)
- Finetune: [SimCLR](https://arxiv.org/abs/2002.05709)
- KNN: [InstDisc](https://arxiv.org/abs/1805.01978)

See the [benchmarking scripts](./benchmarks/imagenet/resnet50/) for details.

| Model           | Backbone | Batch Size | Epochs | Linear Top1 | Finetune Top1 | kNN Top1 | Tensorboard                                                                                                                                                                    | Checkpoint                                                                                                                                                              |
| --------------- | -------- | ---------- | ------ | ----------- | ------------- | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| BarlowTwins     | Res50    | 256        | 100    | 62.9        | 72.6          | 45.6     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_barlowtwins_2023-08-18_00-11-03/pretrain/version_0/events.out.tfevents.1692310273.Machine2.569794.0) | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_barlowtwins_2023-08-18_00-11-03/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt) |
| BYOL            | Res50    | 256        | 100    | 62.5        | 74.5          | 46.0     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_byol_2024-02-14_16-10-09/pretrain/version_0/events.out.tfevents.1707923418.Machine2.3205.0)          | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_byol_2024-02-14_16-10-09/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt)        |
| DINO            | Res50    | 128        | 100    | 68.2        | 72.5          | 49.9     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dino_2023-06-06_13-59-48/pretrain/version_0/events.out.tfevents.1686052799.Machine2.482599.0)        | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dino_2023-06-06_13-59-48/pretrain/version_0/checkpoints/epoch%3D99-step%3D1000900.ckpt)       |
| DINO            | ViT-S/16    | 128        | 100    | 73.3        | 79.8          | 67.5     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_vits14_dino_2025-02-16_16-03-14/pretrain/version_0/events.out.tfevents.1739718198.compute-03-ubuntu-4x4090.2832462.0)        | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_vits14_dino_2025-02-16_16-03-14/pretrain/version_0/checkpoints/epoch%3D99-step%3D1000900.ckpt)       |
| iBOT            | ViT-S/16    | 128        | 100    | 72.2        | 78.3          | 65.4     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_vits16_ibot_2025-07-10_13-47-17/pretrain/version_0/events.out.tfevents.1752148040.compute-01-ubuntu-2x4090.253473.0)        | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_vits16_ibot_2025-07-10_13-47-17/pretrain/version_0/checkpoints/epoch%3D99-step%3D1000900.ckpt)       |
| MAE             | ViT-B/16 | 256        | 100    | 46.0        | 81.3          | 11.2     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_vitb16_mae_2024-02-25_19-57-30/pretrain/version_0/events.out.tfevents.1708887459.Machine2.1092409.0)          | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_vitb16_mae_2024-02-25_19-57-30/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt)           |
| MoCoV2          | Res50    | 256        | 100    | 61.5        | 74.3          | 41.8     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_mocov2_2024-02-18_10-29-14/pretrain/version_0/events.out.tfevents.1708248562.Machine2.439033.0)      | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_mocov2_2024-02-18_10-29-14/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt)      |
| SimCLR\*        | Res50    | 256        | 100    | 63.2        | 73.9          | 44.8     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_simclr_2023-06-22_09-11-13/pretrain/version_0/events.out.tfevents.1687417883.Machine2.33270.0)       | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_simclr_2023-06-22_09-11-13/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt)      |
| SimCLR\* + DCL  | Res50    | 256        | 100    | 65.1        | 73.5          | 49.6     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dcl_2023-07-04_16-51-40/pretrain/version_0/events.out.tfevents.1688482310.Machine2.247807.0)         | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dcl_2023-07-04_16-51-40/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt)         |
| SimCLR\* + DCLW | Res50    | 256        | 100    | 64.5        | 73.2          | 48.5     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dclw_2023-07-07_14-57-13/pretrain/version_0/events.out.tfevents.1688734645.Machine2.3176.0)          | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_dclw_2023-07-07_14-57-13/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt)        |
| SwAV            | Res50    | 256        | 100    | 67.2        | 75.4          | 49.5     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_swav_2023-05-25_08-29-14/pretrain/version_0/events.out.tfevents.1684996168.Machine2.1445108.0)       | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_swav_2023-05-25_08-29-14/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt)        |
| TiCo            | Res50    | 256        | 100    | 49.7        | 72.7          | 26.6     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_tico_2024-01-07_18-40-57/pretrain/version_0/events.out.tfevents.1704649265.Machine2.1604956.0)       | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_tico_2024-01-07_18-40-57/pretrain/version_0/checkpoints/epoch%3D99-step%3D250200.ckpt)        |
| VICReg          | Res50    | 256        | 100    | 63.0        | 73.7          | 46.3     | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_vicreg_2023-09-11_10-53-08/pretrain/version_0/events.out.tfevents.1694422401.Machine2.556563.0)      | [link](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_vicreg_2023-09-11_10-53-08/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt)      |

_\*We use square root learning rate scaling instead of linear scaling as it yields
better results for smaller batch sizes. See Appendix B.1 in the [SimCLR paper](https://arxiv.org/abs/2002.05709)._

### ImageNet100

[ImageNet100 benchmarks detailed results](https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html#imagenet100)

### Imagenette

[Imagenette benchmarks detailed results](https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html#imagenette)

### CIFAR-10

[CIFAR-10 benchmarks detailed results](https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html#cifar-10)

## Terminology

Below you can see a schematic overview of the different concepts in the package.
The terms in bold are explained in more detail in our [documentation](https://docs.lightly.ai/self-supervised-learning/).

<img src="/docs/source/getting_started/images/lightly_overview.png" alt="Overview of the Lightly pip package"/></a>

### Next Steps

Head to the [documentation](https://docs.lightly.ai/self-supervised-learning/) and see the things you can achieve with Lightly!

## Development

To install dev dependencies (for example to contribute to the framework) you can use the following command:

```
pip3 install -e ".[dev]"
```

For more information about how to contribute have a look [here](CONTRIBUTING.md).

### Running Tests

Unit tests are within the [tests directory](tests/) and we recommend running them using
[pytest](https://docs.pytest.org/en/stable/). There are two test configurations
available. By default, only a subset will be run:

```
make test-fast
```

To run all tests (including the slow ones) you can use the following command:

```
make test
```

To test a specific file or directory use:

```
pytest <path to file or directory>
```

### Code Formatting

To format code with [black](https://black.readthedocs.io/en/stable/) and [isort](https://docs.pytest.org) run:

```
make format
```

## Further Reading

**Self-Supervised Learning**:

- Have a look at our [#papers channel on discord](https://discord.com/channels/752876370337726585/815153188487299083)
  for the newest self-supervised learning papers.
- [A Cookbook of Self-Supervised Learning, 2023](https://arxiv.org/abs/2304.12210)
- [Masked Autoencoders Are Scalable Vision Learners, 2021](https://arxiv.org/abs/2111.06377)
- [Emerging Properties in Self-Supervised Vision Transformers, 2021](https://arxiv.org/abs/2104.14294)
- [Unsupervised Learning of Visual Features by Contrasting Cluster Assignments, 2021](https://arxiv.org/abs/2006.09882)
- [What Should Not Be Contrastive in Contrastive Learning, 2020](https://arxiv.org/abs/2008.05659)
- [A Simple Framework for Contrastive Learning of Visual Representations, 2020](https://arxiv.org/abs/2002.05709)
- [Momentum Contrast for Unsupervised Visual Representation Learning, 2020](https://arxiv.org/abs/1911.05722)

## FAQ

- Why should I care about self-supervised learning? Aren't pre-trained models from ImageNet much better for transfer learning?

  - Self-supervised learning has become increasingly popular among scientists over the last years because the learned representations perform extraordinarily well on downstream tasks. This means that they capture the important information in an image better than other types of pre-trained models. By training a self-supervised model on _your_ dataset, you can make sure that the representations have all the necessary information about your images.

- How can I contribute?

  - Create an issue if you encounter bugs or have ideas for features we should implement. You can also add your own code by forking this repository and creating a PR. More details about how to contribute with code is in our [contribution guide](CONTRIBUTING.md).

- Is this framework for free?

  - Yes, this framework is completely free to use and we provide the source code. We believe that we need to make training deep learning models more data efficient to achieve widespread adoption. One step to achieve this goal is by leveraging self-supervised learning. The company behind Lightly is committed to keep this framework open-source.

- If this framework is free, how is the company behind Lightly making money?
  - Training self-supervised models is only one part of our solution.
    [The company behind Lightly](https://lightly.ai/) focuses on processing and analyzing embeddings created by self-supervised models.
    By building, what we call a self-supervised active learning loop we help companies understand and work with their data more efficiently.
    As the [Lightly Solution](https://docs.lightly.ai) is a freemium product, you can try it out for free. However, we will charge for some features.
  - In any case this framework will always be free to use, even for commercial purposes.

## Lightly in Research

- [Joint-Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self-Supervised Learning, 2025](https://arxiv.org/abs/2505.12477)
- [Reverse Engineering Self-Supervised Learning, 2023](https://arxiv.org/abs/2305.15614)
- [Learning Visual Representations via Language-Guided Sampling, 2023](https://arxiv.org/pdf/2302.12248.pdf)
- [Self-Supervised Learning Methods for Label-Efficient Dental Caries Classification, 2022](https://www.mdpi.com/2075-4418/12/5/1237)
- [DPCL: Contrastive representation learning with differential privacy, 2022](https://assets.researchsquare.com/files/rs-1516950/v1_covered.pdf?c=1654486158)
- [Decoupled Contrastive Learning, 2021](https://arxiv.org/abs/2110.06848)
- [solo-learn: A Library of Self-supervised Methods for Visual Representation Learning, 2021](https://www.jmlr.org/papers/volume23/21-1155/21-1155.pdf)

## Company behind this Open Source Framework

[Lightly](https://www.lightly.ai) is a spin-off from ETH Zurich that helps companies
build efficient active learning pipelines to select the most relevant data for their models.

You can find out more about the company and it's services by following the links below:

- [Homepage](https://www.lightly.ai)
- [LightlyTrain](https://docs.lightly.ai/train/stable/index.html)
- [Web-App](https://app.lightly.ai)
- [Lightly Solution Documentation (Lightly Worker & API)](https://docs.lightly.ai/)
- [Lightly's AwesomeSSL](https://github.com/lightly-ai/awesome-self-supervised-learning) (collection of SSL papers)

[Back to top🚀](#top)



================================================
FILE: pyproject.toml
================================================
[build-system]
requires = [
    "setuptools>=21",
    "setuptools-scm"
]
build-backend = "setuptools.build_meta"

[project]
name="lightly"
requires-python = ">=3.6"
authors = [
    {name = "Lightly Team", email = "team@lightly.ai"},
]
license = {file = "LICENSE.txt"}
description="A deep learning package for self-supervised learning"
classifiers = [
    "Development Status :: 5 - Production/Stable",
    "Intended Audience :: Developers",
    "Intended Audience :: Education",
    "Intended Audience :: Science/Research",
    "Topic :: Scientific/Engineering",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Scientific/Engineering :: Image Processing",
    "Topic :: Scientific/Engineering :: Mathematics",
    "Topic :: Software Development",
    "Topic :: Software Development :: Libraries",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.6",
    "Programming Language :: Python :: 3.7",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "License :: OSI Approved :: MIT License",
]
dependencies = [
  "certifi>=14.05.14",
  "hydra-core>=1.0.0",
  "lightly_utils~=0.0.0",
  "numpy>=1.18.1",
  "python_dateutil>=2.5.3",
  "requests>=2.27.0",
  "six>=1.10",
  "tqdm>=4.44",
  "torch",
  "torchvision",
  "pydantic>=1.10.5",
  "pytorch_lightning>=1.0.4",
  "urllib3>=1.25.3",
  "aenum>=3.1.11"
]
dynamic = ["version", "readme"]

[project.optional-dependencies]
all = [
  "lightly[dev,matplotlib,minimal,timm,video]"
]
dev = [
  "sphinx",
  "pylint",
  "pytest",
  "pytest-forked",
  "pytest-xdist",
  "pytest-mock",
  "responses",
  "docutils<=0.16",
  "sphinx-copybutton",
  "sphinx-design",
  "sphinx-gallery",
  "sphinx-tabs",
  "sphinx-reredirects",
  "sphinx_rtd_theme",
  "matplotlib",
  "pre-commit",
  "opencv-python",
  "scikit-learn",
  "pandas",
  "toml",
  "torchmetrics",
  # black, isort and mypy should be the same version as defined in .pre-commit-config.yaml
  "black==23.1.0", # frozen version to avoid differences between CI and local dev machines
  "isort==5.11.5", # frozen version to avoid differences between CI and local dev machines
  "mypy==1.4.1", # frozen version to avoid differences between CI and local dev machines
  "types-python-dateutil",
  "types-toml",
  "types-requests",
  "nbformat",
  "jupytext"
]
# Minimal dependencies against which we test. Older versions might work depending on the
# functionality used.
minimal = [
  "torch>=1.10.0",
  "torchvision>=0.11.0",
  "pytorch_lightning>=1.6",
]
openapi = [
  "python_dateutil>=2.5.3",
  "setuptools>=21.0.0",
  "urllib3>=1.25.3",
  "pydantic>=1.10.5",
  "aenum>=3.1.11"
]
timm = ["timm>=0.9.9"]
video = ["av>=8.0.3"]
matplotlib = ["matplotlib>=3"]


[project.urls]
"Homepage" = "https://www.lightly.ai"
"Web-App" = "https://app.lightly.ai"
"Documentation" = "https://docs.lightly.ai"
"Github" = "https://github.com/lightly-ai/lightly"
"Discord" = "https://discord.gg/xvNJW94"

[project.scripts]
lightly-crop = "lightly.cli.crop_cli:entry"
lightly-download = "lightly.cli.download_cli:entry"
lightly-embed = "lightly.cli.embed_cli:entry"
lightly-magic = "lightly.cli.lightly_cli:entry"
lightly-serve = "lightly.cli.serve_cli:entry"
lightly-ssl-train = "lightly.cli.train_cli:entry"
lightly-version = "lightly.cli.version_cli:entry"

[tool.setuptools.packages.find]
include = ["lightly*"]

[tool.setuptools.dynamic]
readme = {file = ["README.md"], content-type = "text/markdown"}
version = {attr = "lightly.__version__"}

[tool.setuptools.package-data]
lightly = ["lightly/cli/config/*.yaml"]

[tool.black]
extend-exclude = "lightly/openapi_generated/.*"

[tool.isort]
profile = "black"
extend_skip = "lightly/openapi_generated"

[tool.coverage.run]
omit = ["lightly/openapi_generated/*"]

[tool.mypy]
ignore_missing_imports = true
warn_unused_configs = true
strict_equality = true

# Disallow dynamic typing
disallow_any_decorated = true
# TODO(Philipp, 09/23): Remove me!
# disallow_any_explicit = True
disallow_any_generics = true
disallow_subclassing_any = true

# Disallow untyped definitions
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true

# None and optional handling
no_implicit_optional = true
strict_optional = true

# Configuring warnings
warn_unused_ignores = false   # Different ignores are required for different Python versions
warn_no_return = true
warn_return_any = true
warn_redundant_casts = true
warn_unreachable = true

# Print format
show_error_codes = true
show_error_context = true

# Plugins
plugins = ["numpy.typing.mypy_plugin"]

# Excludes
# TODO(Philipp, 09/23): Remove these one by one (start with 300 files).
exclude = '''(?x)(
    lightly/cli/version_cli.py |
    lightly/cli/crop_cli.py |
    lightly/cli/serve_cli.py |
    lightly/cli/embed_cli.py |
    lightly/cli/lightly_cli.py |
    lightly/cli/download_cli.py |
    lightly/cli/config/get_config.py |
    lightly/cli/train_cli.py |
    lightly/cli/_cli_simclr.py |
    lightly/cli/_helpers.py |
    lightly/data/dataset.py |
    lightly/data/collate.py |
    lightly/data/_image_loaders.py |
    lightly/data/_video.py |
    lightly/core.py |
    lightly/api/api_workflow_compute_worker.py |
    lightly/api/api_workflow_predictions.py |
    lightly/api/api_workflow_export.py |
    lightly/api/api_workflow_download_dataset.py |
    lightly/api/bitmask.py |
    lightly/api/_version_checking.py |
    lightly/api/patch.py |
    lightly/api/swagger_api_client.py |
    lightly/api/api_workflow_collaboration.py |
    lightly/api/utils.py |
    lightly/api/api_workflow_datasets.py |
    lightly/api/api_workflow_selection.py |
    lightly/api/swagger_rest_client.py |
    lightly/api/api_workflow_datasources.py |
    lightly/api/api_workflow_datasource_listing.py |
    lightly/api/api_workflow_upload_embeddings.py |
    lightly/api/api_workflow_client.py |
    lightly/api/api_workflow_upload_metadata.py |
    lightly/api/api_workflow_tags.py |
    lightly/api/api_workflow_artifacts.py |
    lightly/utils/cropping/crop_image_by_bounding_boxes.py |
    lightly/utils/cropping/read_yolo_label_file.py |
    lightly/utils/debug.py |
    lightly/utils/benchmarking/benchmark_module.py |
    lightly/utils/benchmarking/knn_classifier.py |
    lightly/utils/benchmarking/online_linear_classifier.py |
    lightly/models/modules/masked_autoencoder.py |
    lightly/models/modules/ijepa.py |
    lightly/models/utils.py |
    tests/cli/test_cli_version.py |
    tests/cli/test_cli_magic.py |
    tests/cli/test_cli_crop.py |
    tests/cli/test_cli_download.py |
    tests/cli/test_cli_train.py |
    tests/cli/test_cli_get_lighty_config.py |
    tests/cli/test_cli_embed.py |
    tests/UNMOCKED_end2end_tests/delete_datasets_test_unmocked_cli.py |
    tests/UNMOCKED_end2end_tests/create_custom_metadata_from_input_dir.py |
    tests/UNMOCKED_end2end_tests/scripts_for_reproducing_problems/test_api_latency.py |
    tests/core/test_Core.py |
    tests/data/test_multi_view_collate.py |
    tests/data/test_data_collate.py |
    tests/data/test_LightlySubset.py |
    tests/data/test_LightlyDataset.py |
    tests/embedding/test_callbacks.py |
    tests/embedding/test_embedding.py |
    tests/api/test_serve.py |
    tests/api/test_swagger_rest_client.py |
    tests/api/test_rest_parser.py |
    tests/api/test_utils.py |
    tests/api/benchmark_video_download.py |
    tests/api/test_BitMask.py |
    tests/api/test_patch.py |
    tests/api/test_version_checking.py |
    tests/api/test_swagger_api_client.py |
    tests/utils/test_debug.py |
    tests/utils/benchmarking/test_benchmark_module.py |
    tests/utils/benchmarking/test_topk.py |
    tests/utils/benchmarking/test_online_linear_classifier.py |
    tests/utils/benchmarking/test_knn_classifier.py |
    tests/utils/benchmarking/test_knn.py |
    tests/utils/benchmarking/test_linear_classifier.py |
    tests/utils/benchmarking/test_metric_callback.py |
    tests/utils/test_dist.py |
    tests/conftest.py |
    tests/api_workflow/test_api_workflow_selection.py |
    tests/api_workflow/test_api_workflow_datasets.py |
    tests/api_workflow/mocked_api_workflow_client.py |
    tests/api_workflow/test_api_workflow_compute_worker.py |
    tests/api_workflow/test_api_workflow_artifacts.py |
    tests/api_workflow/test_api_workflow_download_dataset.py |
    tests/api_workflow/utils.py |
    tests/api_workflow/test_api_workflow_client.py |
    tests/api_workflow/test_api_workflow_export.py |
    tests/api_workflow/test_api_workflow_datasources.py |
    tests/api_workflow/test_api_workflow_datasource_listing.py |
    tests/api_workflow/test_api_workflow_tags.py |
    tests/api_workflow/test_api_workflow_upload_custom_metadata.py |
    tests/api_workflow/test_api_workflow_upload_embeddings.py |
    tests/api_workflow/test_api_workflow_collaboration.py |
    tests/api_workflow/test_api_workflow_predictions.py |
    tests/api_workflow/test_api_workflow.py |
    # Let's not type check deprecated active learning:
    lightly/active_learning |
    # Let's not type deprecated models:
    lightly/models/simclr.py |
    lightly/models/moco.py |
    lightly/models/barlowtwins.py |
    lightly/models/nnclr.py |
    lightly/models/simsiam.py |
    lightly/models/byol.py |
    # Let's not type deprecated models tests:
    tests/models/test_ModelsSimSiam.py |
    tests/models/test_ModelsSimCLR.py |
    tests/models/test_ModelsNNCLR.py |
    tests/models/test_ModelsMoCo.py |
    tests/models/test_ModelsBYOL.py )'''

# Ignore imports from untyped modules.
[[tool.mypy.overrides]]
module = [
    "lightly.api.*",
    "lightly.cli.*",
    "lightly.data.*",
    "lightly.models.*",
    "lightly.utils.benchmarking.*",
    "tests.api_workflow.*",
]
follow_imports = "skip"

# Ignore errors in auto generated code.
[[tool.mypy.overrides]]
module = [
    "lightly.openapi_generated.*",
]
ignore_errors = true

[tool.jupytext]
notebook_metadata_filter="-all"
cell_metadata_filter="-all"



================================================
FILE: benchmarks/imagenet/resnet50/README.md
================================================
# ImageNet ResNet50

Reference implementations for self-supervised learning (SSL) methods on ImageNet with
ResNet50 backbones.

**Note**
> The benchmarks are still in beta phase and there will be breaking changes and
frequent updates. PRs for new methods are highly welcome!

**Goals**
* Provide easy to use/adapt reference implementations of SSL methods.
* Implemented methods should be self-contained and use the Lightly building blocks.
See [simclr.py](simclr.py).
* Remain as framework agnostic as possible. The benchmarks currently only rely on PyTorch and PyTorch Lightning.


**Non-Goals**
* Lightly doesn't strive to be an end-to-end SSL framework with vast configuration options.
Instead, we try to provide building blocks and examples to make it as easy as possible to
build on top of existing SSL methods.

You can find benchmark results in our [docs](https://docs.lightly.ai/self-supervised-learning/getting_started/benchmarks.html).

## Run Benchmark

To run the benchmark first download the ImageNet ILSVRC2012 split from here: https://www.image-net.org/challenges/LSVRC/2012/.


Then start the benchmark with:
```
python main.py --epochs 100 --train-dir /datasets/imagenet/train --val-dir /datasets/imagenet/val --num-workers 12 --devices 2 --batch-size-per-device 128 --skip-finetune-eval
```

Or with SLURM, create the following script (`run_imagenet.sh`):
```
#!/bin/bash

#SBATCH --nodes=1
#SBATCH --gres=gpu:2            # Must match --devices argument
#SBATCH --ntasks-per-node=2     # Must match --devices argument
#SBATCH --cpus-per-task=16      # Must be >= --num-workers argument
#SBATCH --mem=0

eval "$(conda shell.bash hook)"

conda activate lightly-env
srun python main.py --epochs 100 --train-dir /datasets/imagenet/train --val-dir /datasets/imagenet/val --num-workers 12 --devices 2 --batch-size-per-device 128
conda deactivate
```

And run it with sbatch: `sbatch run_imagenet.sh`.


## Configuration

To run the benchmark on specific methods use the `--methods` flag:
```
python main.py --epochs 100 --batch-size-per-device 128 --methods simclr byol
```

Training/evaluation steps can be skipped as follows:
```
python main.py --batch-size-per-device 128 \
    --epochs 0              # no pretraining
    --skip-knn-eval         # no KNN evaluation
    --skip-linear-eval      # no linear evaluation
    --skip-finetune-eval    # no finetune evaluation
```


## ImageNet100

For ImageNet100 you have to adapt the dataset location and set number of classes to 100:
```
python main.py --train-dir /datasets/imagenet100/train --val-dir /datasets/imagenet100/val --num-classes 100 --epochs 100 --num-workers 12 --devices 2 --batch-size-per-device 128
```


## Imagenette

For [Imagenette](https://github.com/fastai/imagenette) you have to adapt the dataset location and set number of classes to 10:

```
python main.py --train-dir /datasets/imagenette2-320/train --val-dir /datasets/imagenette2-320/val --num-classes 10 --epochs 100 --num-workers 12 --devices 2 --batch-size-per-device 128
```


================================================
FILE: docs/README.md
================================================
# Documentation Guide
All the commands in here are assumed to be run from within the `docs` directory.
## Prerequisites
In a virtual environment, make sure that you install the development dependencies:
```bash
pip install -e "..[dev]"
```
Or if your package manager of choice is `uv`, you can run:
```bash
(cd .. && make install-dev)
```

Maintainers will additionally require an installation including `detectron2` for the release, however it is not necessary for contributors. This isn't handled in requirements because the version you'll need depends on your GPU/hardware. For installing `detectron2`, follow the [instructions](https://detectron2.readthedocs.io/en/latest/tutorials/install.html).

## Build the Docs
The `sphinx` documentation generator provides a Makefile. To build the `html` documentation without running python files (tutorials) use:
``` 
make html-noplot
```
This is the recommended way to locally build the docs before creating a PR and will put the build `.html` files inside `docs/build`. Since above command uses caching to speed up the build, some warnings may not appear after the initial build. It is therefore advisable to do a clean build from time to time by running:
```
make clean-html-noplot
```
The built docs can be viewed by calling:
```bash
make serve-local
```
For building the full docs with python files (including tutorials), run (usually not necessary during development):
```bash
make html
```


## Deploy the Docs

Only Lightly core team members will have access to deploy new docs. 

1. Open a terminal and go to the `docs/` folder. 
1. If not done yet, authenticate your account using `gcloud auth login`
1. Deploy to app engine using `gcloud app deploy app.yaml`



================================================
FILE: examples/README.md
================================================
# Examples

We provide example implementations for self-supervised learning models for PyTorch and PyTorch Lightning to give you a headstart when implementing your own model! 


All examples can be run from the terminal with:

```
python <path to example.py>
```

The examples should also run on [Google Colab](https://colab.research.google.com/). Remember to activate the GPU otherwise training will be very slow! You can simply copy paste the code and add the following line at the beginning of the notebook to install lightly:

```
!pip install lightly

# add code from example below
```


You can find additional information for each model in our [Documentation](https://docs.lightly.ai//examples/models.html#)

> [!IMPORTANT]
> The examples notebooks are generated using the [`create_example_nbs.py`](./create_example_nbs.py) script and should not be modified manually. All changes must be made to the respecitve example.




================================================
FILE: lightly/core.py
================================================
""" Contains the core functionality of the lightly Python package. """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved
import os
from typing import List, Tuple

import numpy as np
import yaml

import lightly.cli as cli
from lightly.cli.embed_cli import _embed_cli
from lightly.cli.lightly_cli import _lightly_cli
from lightly.cli.train_cli import _train_cli


def _get_config_path(config_path):
    """Find path to yaml config file

    Args:
        config_path: (str) Path to config.yaml file

    Returns:
        Path to config.yaml if specified else default config.yaml

    Raises:
        ValueError: If the config_path is not None but doesn't exist

    """
    if config_path is None:
        dirname = os.path.dirname(cli.__file__)
        config_path = os.path.join(dirname, "config/config.yaml")
    if not os.path.exists(config_path):
        raise ValueError("Config path {} does not exist!".format(config_path))

    return config_path


def _load_config_file(config_path):
    """Load a yaml config file

    Args:
        config_path: (str) Path to config.yaml file

    Returns:
        Dictionary with configs from config.yaml

    """
    Loader = yaml.FullLoader
    with open(config_path, "r") as config_file:
        cfg = yaml.load(config_file, Loader=Loader)

    return cfg


def _add_kwargs(cfg, kwargs):
    """Add keyword arguments to config

    Args:
        cfg: (dict) Dictionary of configs from config.yaml
        kwargs: (dict) Dictionary of keyword arguments

    Returns:
        Union of cfg and kwargs

    """
    for key, item in kwargs.items():
        if isinstance(item, dict):
            if key in cfg:
                cfg[key] = _add_kwargs(cfg[key], item)
            else:
                cfg[key] = item
        else:
            cfg[key] = item
    return cfg


def train_model_and_embed_images(
    config_path: str = None, **kwargs
) -> Tuple[np.ndarray, List[int], List[str]]:
    """Train a self-supervised model and use it to embed images.

    First trains a modle using the _train_cli(),
    then embeds with the _embed_cli().
    All arguments passed to the CLI functions
    can also be passed to this function (see below for an example).

    Args:
        config_path:
            Path to config.yaml. If None, the default configs will be used.
        **kwargs:
            Overwrite default configs py passing keyword arguments.

    Returns:
        Embeddings, labels, and filenames of the images.
        Embeddings are of shape (n_samples, embedding_size)
        len(labels) = len(filenames) = n_samples

    Examples:
        >>> import lightly
        >>>
        >>> # train a model and embed images with default configs
        >>> embeddings, _, _ = lightly.train_model_and_embed_images(
        >>>     input_dir='path/to/data')
        >>>
        >>> # train a model and embed images with separate config file
        >>> my_config_path = 'my/config/file.yaml'
        >>> embeddings, _, _ = lightly.train_model_and_embed_images(
        >>>     input_dir='path/to/data', config_path=my_config_path)
        >>>
        >>> # train a model and embed images with default settings + overwrites
        >>> my_trainer = {max_epochs: 10}
        >>> embeddings, _, _ = lightly.train_model_and_embed_images(
        >>>     input_dir='path/to/data', trainer=my_trainer)

    """
    config_path = _get_config_path(config_path)
    config_args = _load_config_file(config_path)
    config_args = _add_kwargs(config_args, kwargs)

    checkpoint = _train_cli(config_args, is_cli_call=False)
    config_args["checkpoint"] = checkpoint
    embeddings, labels, filenames = _embed_cli(config_args, is_cli_call=False)
    return embeddings, labels, filenames


def train_embedding_model(config_path: str = None, **kwargs):
    """Train a self-supervised model.

    Calls the same function as lightly-ssl-train. All arguments passed to
    lightly-ssl-train can also be passed to this function (see below for an
    example).

    Args:
        config_path:
            Path to config.yaml. If None, the default configs will be used.
        **kwargs:
            Overwrite default configs py passing keyword arguments.

    Returns:
        Path to checkpoint of the trained embedding model.

    Examples:
        >>> import lightly
        >>>
        >>> # train a model with default configs
        >>> checkpoint_path = lightly.train_embedding_model(
        >>>     input_dir='path/to/data')
        >>>
        >>> # train a model with separate config file
        >>> my_config_path = 'my/config/file.yaml'
        >>> checkpoint_path = lightly.train_embedding_model(
        >>>     input_dir='path/to/data', config_path=my_config_path)
        >>>
        >>> # train a model with default settings and overwrites: large batch
        >>> # sizes are benefitial for self-supervised training and more
        >>> # workers speed up the dataloading process.
        >>> my_loader = {
        >>>     batch_size: 100,
        >>>     num_workers: 8,
        >>> }
        >>> checkpoint_path = lightly.train_embedding_model(
        >>>     input_dir='path/to/data', loader=my_loader)
        >>> # the command above is equivalent to:
        >>> # lightly-ssl-train input_dir='path/to/data' loader.batch_size=100 loader.num_workers=8
    """
    config_path = _get_config_path(config_path)
    config_args = _load_config_file(config_path)
    config_args = _add_kwargs(config_args, kwargs)

    return _train_cli(config_args, is_cli_call=False)


def embed_images(checkpoint: str, config_path: str = None, **kwargs):
    """Embed images with a self-supervised model.

    Calls the same function as lightly-embed. All arguments passed to
    lightly-embed can also be passed to this function (see below for an
    example).

    Args:
        checkpoint:
            Path to the checkpoint file for the embedding model.
        config_path:
            Path to config.yaml. If None, the default configs will be used.
        **kwargs:
            Overwrite default configs py passing keyword arguments.

    Returns:
        Embeddings, labels, and filenames of the images.

    Examples:
        >>> import lightly
        >>> my_checkpoint_path = 'path/to/checkpoint.ckpt'
        >>>
        >>> # embed images with default configs
        >>> embeddings, _, _ = lightly.embed_images(
        >>>     my_checkpoint_path, input_dir='path/to/data')
        >>>
        >>> # embed images with separate config file
        >>> my_config_path = 'my/config/file.yaml'
        >>> embeddings, _, _ = lightly.embed_images(
        >>>     my_checkpoint_path, input_dir='path/to/data', config_path=my_config_path)
        >>>
        >>> # embed images with default settings and overwrites: at inference,
        >>> # we can use larger input_sizes because it requires less memory.
        >>> my_collate = {input_size: 256}
        >>> embeddings, _, _ = lightly.embed_images(
        >>>     my_checkpoint_path, input_dir='path/to/data', collate=my_collate)
        >>> # the command above is equivalent to:
        >>> # lightly-embed input_dir='path/to/data' collate.input_size=256

    """
    config_path = _get_config_path(config_path)
    config_args = _load_config_file(config_path)
    config_args = _add_kwargs(config_args, kwargs)

    config_args["checkpoint"] = checkpoint

    return _embed_cli(config_args, is_cli_call=False)



================================================
FILE: lightly/api/__init__.py
================================================
""" The lightly.api module provides access to the Lightly API."""

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved
from lightly.api import patch as _patch
from lightly.api.api_workflow_artifacts import ArtifactNotExist
from lightly.api.api_workflow_client import ApiWorkflowClient
from lightly.openapi_generated.swagger_client.api_client import (
    Configuration as _Configuration,
)

# Make ApiWorkflowClient and swagger classes picklable.
_patch.make_swagger_configuration_picklable(
    configuration_cls=_Configuration,
)



================================================
FILE: lightly/api/_version_checking.py
================================================
from threading import Thread

from lightly.api import utils
from lightly.api.swagger_api_client import LightlySwaggerApiClient
from lightly.openapi_generated.swagger_client.api import VersioningApi
from lightly.utils import version_compare

# Default timeout for API version verification requests in seconds.
DEFAULT_TIMEOUT_SEC = 2


def is_latest_version(current_version: str) -> bool:
    """Returns True if package version is latest released version."""
    latest_version = get_latest_version(current_version)
    return version_compare.version_compare(current_version, latest_version) >= 0


def is_compatible_version(current_version: str) -> bool:
    """Returns True if package version is compatible with API."""
    minimum_version = get_minimum_compatible_version()
    return version_compare.version_compare(current_version, minimum_version) >= 0


def get_latest_version(
    current_version: str, timeout_sec: float = DEFAULT_TIMEOUT_SEC
) -> str:
    """Returns the latest package version."""
    versioning_api = _get_versioning_api()
    version_number: str = versioning_api.get_latest_pip_version(
        current_version=current_version,
        _request_timeout=timeout_sec,
    )
    return version_number


def get_minimum_compatible_version(
    timeout_sec: float = DEFAULT_TIMEOUT_SEC,
) -> str:
    """Returns minimum package version that is compatible with the API."""
    versioning_api = _get_versioning_api()
    version_number: str = versioning_api.get_minimum_compatible_pip_version(
        _request_timeout=timeout_sec
    )
    return version_number


def check_is_latest_version_in_background(current_version: str) -> None:
    """Checks if the current version is the latest version in a background thread."""

    def _check_version_in_background(current_version: str) -> None:
        try:
            is_latest_version(current_version=current_version)
        except Exception:
            # Ignore failed check.
            pass

    thread = Thread(
        target=_check_version_in_background,
        kwargs=dict(current_version=current_version),
        daemon=True,
    )
    thread.start()


def _get_versioning_api() -> VersioningApi:
    configuration = utils.get_api_client_configuration(
        raise_if_no_token_specified=False,
    )
    # Set retries to 0 to avoid waiting for retries in case of a timeout.
    configuration.retries = 0
    api_client = LightlySwaggerApiClient(configuration=configuration)
    versioning_api = VersioningApi(api_client=api_client)
    return versioning_api



================================================
FILE: lightly/api/api_workflow_artifacts.py
================================================
import os
import warnings

from lightly.api import download
from lightly.openapi_generated.swagger_client.models import (
    DockerRunArtifactData,
    DockerRunArtifactType,
    DockerRunData,
)


class ArtifactNotExist(Exception):
    pass


class _ArtifactsMixin:
    def download_compute_worker_run_artifacts(
        self,
        run: DockerRunData,
        output_dir: str,
        timeout: int = 60,
    ) -> None:
        """Downloads all artifacts from a run.

        Args:
            run:
                Run from which to download artifacts.
            output_dir:
                Output directory where artifacts will be saved.
            timeout:
                Timeout in seconds after which an artifact download is interrupted.

        Examples:
            >>> # schedule run
            >>> scheduled_run_id = client.schedule_compute_worker_run(...)
            >>>
            >>> # wait until run completed
            >>> for run_info in client.compute_worker_run_info_generator(scheduled_run_id=scheduled_run_id):
            >>>     pass
            >>>
            >>> # download artifacts
            >>> run = client.get_compute_worker_run_from_scheduled_run(scheduled_run_id=scheduled_run_id)
            >>> client.download_compute_worker_run_artifacts(run=run, output_dir="my_run/artifacts")

        """
        if run.artifacts is None:
            return
        for artifact in run.artifacts:
            self._download_compute_worker_run_artifact(
                run_id=run.id,
                artifact_id=artifact.id,
                output_path=os.path.join(output_dir, artifact.file_name),
                timeout=timeout,
            )

    def download_compute_worker_run_checkpoint(
        self,
        run: DockerRunData,
        output_path: str,
        timeout: int = 60,
    ) -> None:
        """Downloads the last training checkpoint from a run.

        See our docs for more information regarding checkpoints:
        https://docs.lightly.ai/docs/train-a-self-supervised-model#checkpoints

        Args:
            run:
                Run from which to download the checkpoint.
            output_path:
                Path where checkpoint will be saved.
            timeout:
                Timeout in seconds after which download is interrupted.

        Raises:
            ArtifactNotExist:
                If the run has no checkpoint artifact or the checkpoint has not yet been
                uploaded.

        Examples:
            >>> # schedule run
            >>> scheduled_run_id = client.schedule_compute_worker_run(...)
            >>>
            >>> # wait until run completed
            >>> for run_info in client.compute_worker_run_info_generator(scheduled_run_id=scheduled_run_id):
            >>>     pass
            >>>
            >>> # download checkpoint
            >>> run = client.get_compute_worker_run_from_scheduled_run(scheduled_run_id=scheduled_run_id)
            >>> client.download_compute_worker_run_checkpoint(run=run, output_path="my_checkpoint.ckpt")

        """
        self._download_compute_worker_run_artifact_by_type(
            run=run,
            artifact_type=DockerRunArtifactType.CHECKPOINT,
            output_path=output_path,
            timeout=timeout,
        )

    def download_compute_worker_run_report_pdf(
        self,
        run: DockerRunData,
        output_path: str,
        timeout: int = 60,
    ) -> None:
        """Download the report in pdf format from a run.

        Args:
            run:
                Run from which to download the report.
            output_path:
                Path where report will be saved.
            timeout:
                Timeout in seconds after which download is interrupted.

        Raises:
            ArtifactNotExist:
                If the run has no report artifact or the report has not yet been
                uploaded.

        Examples:
            >>> # schedule run
            >>> scheduled_run_id = client.schedule_compute_worker_run(...)
            >>>
            >>> # wait until run completed
            >>> for run_info in client.compute_worker_run_info_generator(scheduled_run_id=scheduled_run_id):
            >>>     pass
            >>>
            >>> # download report
            >>> run = client.get_compute_worker_run_from_scheduled_run(scheduled_run_id=scheduled_run_id)
            >>> client.download_compute_worker_run_report_pdf(run=run, output_path="report.pdf")

        """
        self._download_compute_worker_run_artifact_by_type(
            run=run,
            artifact_type=DockerRunArtifactType.REPORT_PDF,
            output_path=output_path,
            timeout=timeout,
        )

    def download_compute_worker_run_report_json(
        self,
        run: DockerRunData,
        output_path: str,
        timeout: int = 60,
    ) -> None:
        """Download the report in json format from a run.

        DEPRECATED: This method is deprecated and will be removed in the future. Use
        download_compute_worker_run_report_v2_json to download the new report_v2.json
        instead.

        Args:
            run:
                Run from which to download the report.
            output_path:
                Path where report will be saved.
            timeout:
                Timeout in seconds after which download is interrupted.

        Raises:
            ArtifactNotExist:
                If the run has no report artifact or the report has not yet been
                uploaded.

        Examples:
            >>> # schedule run
            >>> scheduled_run_id = client.schedule_compute_worker_run(...)
            >>>
            >>> # wait until run completed
            >>> for run_info in client.compute_worker_run_info_generator(scheduled_run_id=scheduled_run_id):
            >>>     pass
            >>>
            >>> # download checkpoint
            >>> run = client.get_compute_worker_run_from_scheduled_run(scheduled_run_id=scheduled_run_id)
            >>> client.download_compute_worker_run_report_json(run=run, output_path="report.json")

        """
        warnings.warn(
            DeprecationWarning(
                "This method downloads the deprecated report.json file and will be "
                "removed in the future. Use download_compute_worker_run_report_v2_json "
                "to download the new report_v2.json file instead."
            )
        )
        self._download_compute_worker_run_artifact_by_type(
            run=run,
            artifact_type=DockerRunArtifactType.REPORT_JSON,
            output_path=output_path,
            timeout=timeout,
        )

    def download_compute_worker_run_report_v2_json(
        self,
        run: DockerRunData,
        output_path: str,
        timeout: int = 60,
    ) -> None:
        """Download the report in json format from a run.

        Args:
            run:
                Run from which to download the report.
            output_path:
                Path where report will be saved.
            timeout:
                Timeout in seconds after which download is interrupted.

        Raises:
            ArtifactNotExist:
                If the run has no report artifact or the report has not yet been
                uploaded.

        Examples:
            >>> # schedule run
            >>> scheduled_run_id = client.schedule_compute_worker_run(...)
            >>>
            >>> # wait until run completed
            >>> for run_info in client.compute_worker_run_info_generator(scheduled_run_id=scheduled_run_id):
            >>>     pass
            >>>
            >>> # download checkpoint
            >>> run = client.get_compute_worker_run_from_scheduled_run(scheduled_run_id=scheduled_run_id)
            >>> client.download_compute_worker_run_report_v2_json(run=run, output_path="report_v2.json")

        """
        self._download_compute_worker_run_artifact_by_type(
            run=run,
            artifact_type=DockerRunArtifactType.REPORT_V2_JSON,
            output_path=output_path,
            timeout=timeout,
        )

    def download_compute_worker_run_log(
        self,
        run: DockerRunData,
        output_path: str,
        timeout: int = 60,
    ) -> None:
        """Download the log file from a run.

        Args:
            run:
                Run from which to download the log file.
            output_path:
                Path where log file will be saved.
            timeout:
                Timeout in seconds after which download is interrupted.

        Raises:
            ArtifactNotExist:
                If the run has no log artifact or the log file has not yet been
                uploaded.

        Examples:
            >>> # schedule run
            >>> scheduled_run_id = client.schedule_compute_worker_run(...)
            >>>
            >>> # wait until run completed
            >>> for run_info in client.compute_worker_run_info_generator(scheduled_run_id=scheduled_run_id):
            >>>     pass
            >>>
            >>> # download log file
            >>> run = client.get_compute_worker_run_from_scheduled_run(scheduled_run_id=scheduled_run_id)
            >>> client.download_compute_worker_run_log(run=run, output_path="log.txt")

        """
        self._download_compute_worker_run_artifact_by_type(
            run=run,
            artifact_type=DockerRunArtifactType.LOG,
            output_path=output_path,
            timeout=timeout,
        )

    def download_compute_worker_run_memory_log(
        self,
        run: DockerRunData,
        output_path: str,
        timeout: int = 60,
    ) -> None:
        """Download the memory consumption log file from a run.

        Args:
            run:
                Run from which to download the memory log file.
            output_path:
                Path where memory log file will be saved.
            timeout:
                Timeout in seconds after which download is interrupted.

        Raises:
            ArtifactNotExist:
                If the run has no memory log artifact or the memory log file has not yet
                been uploaded.

        Examples:
            >>> # schedule run
            >>> scheduled_run_id = client.schedule_compute_worker_run(...)
            >>>
            >>> # wait until run completed
            >>> for run_info in client.compute_worker_run_info_generator(scheduled_run_id=scheduled_run_id):
            >>>     pass
            >>>
            >>> # download memory log file
            >>> run = client.get_compute_worker_run_from_scheduled_run(scheduled_run_id=scheduled_run_id)
            >>> client.download_compute_worker_run_memory_log(run=run, output_path="memlog.txt")

        """
        self._download_compute_worker_run_artifact_by_type(
            run=run,
            artifact_type=DockerRunArtifactType.MEMLOG,
            output_path=output_path,
            timeout=timeout,
        )

    def download_compute_worker_run_corruptness_check_information(
        self,
        run: DockerRunData,
        output_path: str,
        timeout: int = 60,
    ) -> None:
        """Download the corruptness check information file from a run.

        Args:
            run:
                Run from which to download the file.
            output_path:
                Path where the file will be saved.
            timeout:
                Timeout in seconds after which download is interrupted.

        Raises:
            ArtifactNotExist:
                If the run has no corruptness check information artifact or the file
                has not yet been uploaded.

        Examples:
            >>> # schedule run
            >>> scheduled_run_id = client.schedule_compute_worker_run(...)
            >>>
            >>> # wait until run completed
            >>> for run_info in client.compute_worker_run_info_generator(scheduled_run_id=scheduled_run_id):
            >>>     pass
            >>>
            >>> # download corruptness check information file
            >>> run = client.get_compute_worker_run_from_scheduled_run(scheduled_run_id=scheduled_run_id)
            >>> client.download_compute_worker_run_corruptness_check_information(run=run, output_path="corruptness_check_information.json")
            >>>
            >>> # print all corrupt samples and corruptions
            >>> with open("corruptness_check_information.json", 'r') as f:
            >>>     corruptness_check_information = json.load(f)
            >>> for sample_name, error in corruptness_check_information["corrupt_samples"].items():
            >>>     print(f"Sample '{sample_name}' is corrupt because of the error '{error}'.")

        """
        self._download_compute_worker_run_artifact_by_type(
            run=run,
            artifact_type=DockerRunArtifactType.CORRUPTNESS_CHECK_INFORMATION,
            output_path=output_path,
            timeout=timeout,
        )

    def download_compute_worker_run_sequence_information(
        self,
        run: DockerRunData,
        output_path: str,
        timeout: int = 60,
    ) -> None:
        """Download the sequence information from a run.

        Args:
            run:
                Run from which to download the the file.
            output_path:
                Path where the file will be saved.
            timeout:
                Timeout in seconds after which download is interrupted.

        Raises:
            ArtifactNotExist:
                If the run has no sequence information artifact or the file has not yet
                been uploaded.

        Examples:
            >>> # schedule run
            >>> scheduled_run_id = client.schedule_compute_worker_run(...)
            >>>
            >>> # wait until run completed
            >>> for run_info in client.compute_worker_run_info_generator(scheduled_run_id=scheduled_run_id):
            >>>     pass
            >>>
            >>> # download sequence information file
            >>> run = client.get_compute_worker_run_from_scheduled_run(scheduled_run_id=scheduled_run_id)
            >>> client.download_compute_worker_run_sequence_information(run=run, output_path="sequence_information.json")

        """
        self._download_compute_worker_run_artifact_by_type(
            run=run,
            artifact_type=DockerRunArtifactType.SEQUENCE_INFORMATION,
            output_path=output_path,
            timeout=timeout,
        )

    def _download_compute_worker_run_artifact_by_type(
        self,
        run: DockerRunData,
        artifact_type: str,
        output_path: str,
        timeout: int,
    ) -> None:
        artifact = self._get_artifact_by_type(artifact_type, run)
        self._download_compute_worker_run_artifact(
            run_id=run.id,
            artifact_id=artifact.id,
            output_path=output_path,
            timeout=timeout,
        )

    def _get_artifact_by_type(
        self, artifact_type: str, run: DockerRunData
    ) -> DockerRunArtifactData:
        if run.artifacts is None:
            raise ArtifactNotExist(f"Run has no artifacts.")
        try:
            artifact = next(art for art in run.artifacts if art.type == artifact_type)
        except StopIteration:
            raise ArtifactNotExist(
                f"No artifact with type '{artifact_type}' in artifacts."
            )
        return artifact

    def _download_compute_worker_run_artifact(
        self,
        run_id: str,
        artifact_id: str,
        output_path: str,
        timeout: int,
    ) -> None:
        read_url = self._compute_worker_api.get_docker_run_artifact_read_url_by_id(
            run_id=run_id,
            artifact_id=artifact_id,
        )
        download.download_and_write_file(
            url=read_url,
            output_path=output_path,
            request_kwargs=dict(timeout=timeout),
        )

    def get_compute_worker_run_checkpoint_url(
        self,
        run: DockerRunData,
    ) -> str:
        """Gets the download url of the last training checkpoint from a run.

        See our docs for more information regarding checkpoints:
        https://docs.lightly.ai/docs/train-a-self-supervised-model#checkpoints

        Args:
            run:
                Run from which to download the checkpoint.

        Returns:
            The url from which the checkpoint can be downloaded.

        Raises:
            ArtifactNotExist:
                If the run has no checkpoint artifact or the checkpoint has not yet been
                uploaded.

        Examples:
            >>> # schedule run
            >>> scheduled_run_id = client.schedule_compute_worker_run(...)
            >>>
            >>> # wait until run completed
            >>> for run_info in client.compute_worker_run_info_generator(scheduled_run_id=scheduled_run_id):
            >>>     pass
            >>>
            >>> # get checkpoint read_url
            >>> run = client.get_compute_worker_run_from_scheduled_run(scheduled_run_id=scheduled_run_id)
            >>> checkpoint_read_url = client.get_compute_worker_run_checkpoint_url(run=run)

        """
        artifact = self._get_artifact_by_type(
            artifact_type=DockerRunArtifactType.CHECKPOINT, run=run
        )
        read_url = self._compute_worker_api.get_docker_run_artifact_read_url_by_id(
            run_id=run.id, artifact_id=artifact.id
        )
        return read_url



================================================
FILE: lightly/api/api_workflow_client.py
================================================
import os
import platform
import warnings
from io import IOBase
from typing import *

import requests
from requests import Response
from urllib3.exceptions import HTTPError

from lightly.__init__ import __version__
from lightly.api import _version_checking, utils
from lightly.api.api_workflow_artifacts import _ArtifactsMixin
from lightly.api.api_workflow_collaboration import _CollaborationMixin
from lightly.api.api_workflow_compute_worker import _ComputeWorkerMixin
from lightly.api.api_workflow_datasets import _DatasetsMixin
from lightly.api.api_workflow_datasource_listing import _DatasourceListingMixin
from lightly.api.api_workflow_datasources import _DatasourcesMixin
from lightly.api.api_workflow_download_dataset import _DownloadDatasetMixin
from lightly.api.api_workflow_export import _ExportDatasetMixin
from lightly.api.api_workflow_predictions import _PredictionsMixin
from lightly.api.api_workflow_selection import _SelectionMixin
from lightly.api.api_workflow_tags import _TagsMixin
from lightly.api.api_workflow_upload_embeddings import _UploadEmbeddingsMixin
from lightly.api.api_workflow_upload_metadata import _UploadCustomMetadataMixin
from lightly.api.swagger_api_client import LightlySwaggerApiClient
from lightly.api.utils import DatasourceType
from lightly.openapi_generated.swagger_client.api import (
    CollaborationApi,
    DatasetsApi,
    DatasourcesApi,
    DockerApi,
    EmbeddingsApi,
    JobsApi,
    MappingsApi,
    MetaDataConfigurationsApi,
    PredictionsApi,
    QuotaApi,
    SamplesApi,
    SamplingsApi,
    ScoresApi,
    TagsApi,
)
from lightly.openapi_generated.swagger_client.models import Creator, DatasetData
from lightly.openapi_generated.swagger_client.rest import ApiException
from lightly.utils import reordering

# Env variable for server side encryption on S3
LIGHTLY_S3_SSE_KMS_KEY = "LIGHTLY_S3_SSE_KMS_KEY"


class ApiWorkflowClient(
    _UploadEmbeddingsMixin,
    _SelectionMixin,
    _DownloadDatasetMixin,
    _DatasetsMixin,
    _UploadCustomMetadataMixin,
    _TagsMixin,
    _DatasourcesMixin,
    _DatasourceListingMixin,
    _ComputeWorkerMixin,
    _CollaborationMixin,
    _PredictionsMixin,
    _ArtifactsMixin,
    _ExportDatasetMixin,
):
    """Provides a uniform interface to communicate with the Lightly API.

    The APIWorkflowClient is used to communicate with the Lightly API. The client
    can run also more complex workflows which include multiple API calls at once.

    The client can be used in combination with the active learning agent.

    Args:
        token:
            The token of the user. If it is not passed in during initialization, the token
            will be read from the environment variable LIGHTLY_TOKEN.
            For further information on how to get a token,
            see: https://docs.lightly.ai/docs/install-lightly#api-token
        dataset_id:
            The id of the dataset. If it is not set, but used by a workflow, the last
            modfied dataset is taken by default.
        embedding_id:
            The id of the embedding to use. If it is not set, but used by a workflow,
            the newest embedding is taken by default
        creator:
            Creator passed to API requests.
    """

    def __init__(
        self,
        token: Optional[str] = None,
        dataset_id: Optional[str] = None,
        embedding_id: Optional[str] = None,
        creator: str = Creator.USER_PIP,
    ):
        try:
            if not _version_checking.is_compatible_version(__version__):
                warnings.warn(
                    UserWarning(
                        (
                            f"Incompatible version of lightly pip package. "
                            f"Please upgrade to the latest version "
                            f"to be able to access the api."
                        )
                    )
                )
        except (
            # Error if version compare fails.
            ValueError,
            # Any error by API client if status not in [200, 299].
            ApiException,
            # Any error by urllib3 from within API client. Happens for failed requests
            # that are not handled by API client. For example if there is no internet
            # connection or a timeout.
            HTTPError,
        ):
            pass

        configuration = utils.get_api_client_configuration(token=token)
        self.api_client = LightlySwaggerApiClient(configuration=configuration)
        self.api_client.user_agent = f"Lightly/{__version__} ({platform.system()}/{platform.release()}; {platform.platform()}; {platform.processor()};) python/{platform.python_version()}"

        self.token = configuration.api_key["ApiKeyAuth"]
        if dataset_id is not None:
            self._dataset_id = dataset_id
        if embedding_id is not None:
            self.embedding_id = embedding_id
        self._creator = creator

        self._collaboration_api = CollaborationApi(api_client=self.api_client)
        self._compute_worker_api = DockerApi(api_client=self.api_client)
        self._datasets_api = DatasetsApi(api_client=self.api_client)
        self._datasources_api = DatasourcesApi(api_client=self.api_client)
        self._selection_api = SamplingsApi(api_client=self.api_client)
        self._jobs_api = JobsApi(api_client=self.api_client)
        self._tags_api = TagsApi(api_client=self.api_client)
        self._embeddings_api = EmbeddingsApi(api_client=self.api_client)
        self._mappings_api = MappingsApi(api_client=self.api_client)
        self._scores_api = ScoresApi(api_client=self.api_client)
        self._samples_api = SamplesApi(api_client=self.api_client)
        self._quota_api = QuotaApi(api_client=self.api_client)
        self._metadata_configurations_api = MetaDataConfigurationsApi(
            api_client=self.api_client
        )
        self._predictions_api = PredictionsApi(api_client=self.api_client)

    @property
    def dataset_id(self) -> str:
        """The current dataset ID.

        Future requests with the client will automatically use this dataset ID.
        If the dataset ID is set, it is returned. Otherwise, the ID of the
        last modified dataset is selected.
        """
        try:
            return self._dataset_id
        except AttributeError:
            all_datasets: List[DatasetData] = self.get_datasets()
            datasets_sorted = sorted(
                all_datasets, key=lambda dataset: dataset.last_modified_at
            )
            last_modified_dataset = datasets_sorted[-1]
            self._dataset_id = last_modified_dataset.id
            warnings.warn(
                UserWarning(
                    f"Dataset has not been specified, "
                    f"taking the last modified dataset {last_modified_dataset.name} as default dataset."
                )
            )
            return self._dataset_id

    @dataset_id.setter
    def dataset_id(self, dataset_id: str) -> None:
        """Sets the current dataset ID for the client.

        Args:
            dataset_id:
                The new dataset id.

        Raises:
            ValueError if the dataset does not exist.
        """
        if not self.dataset_exists(dataset_id):
            raise ValueError(
                f"A dataset with the id {dataset_id} does not exist on the web"
                f"platform."
            )
        self._dataset_id = dataset_id

    def _order_list_by_filenames(
        self, filenames_for_list: List[str], list_to_order: List[object]
    ) -> List[object]:
        """Orders a list such that it is in the order of the filenames specified on the server.

        Args:
            filenames_for_list:
                The filenames of samples in a specific order
            list_to_order:
                Some values belonging to the samples

        Returns:
            The list reordered.
            The same reorder applied on the filenames_for_list would put them
            in the order of the filenames in self.filenames_on_server.
            every filename in self.filenames_on_server must be in the
            filenames_for_list.

        """
        filenames_on_server = self.get_filenames()
        list_ordered = reordering.sort_items_by_keys(
            filenames_for_list, list_to_order, filenames_on_server
        )
        return list_ordered

    def get_filenames(self) -> List[str]:
        """Downloads the list of filenames from the server.

        This is an expensive operation, especially for large datasets.

        Returns:
            Names of files in the current dataset.

        :meta private:  # Skip docstring generation
        """
        filenames_on_server = list(
            utils.paginate_endpoint(
                self._mappings_api.get_sample_mappings_by_dataset_id,
                page_size=25000,
                dataset_id=self.dataset_id,
                field="fileName",
            )
        )
        return filenames_on_server

    def upload_file_with_signed_url(
        self,
        file: Union[IOBase, None],
        signed_write_url: str,
        headers: Optional[Dict] = None,
        session: Optional[requests.Session] = None,
    ) -> Response:
        """Uploads a file to a url via a put request.

        This method cannot upload 0 bytes files due to a limitation of the requests
        library. Set file to None to upload an empty file.

        Args:
            file:
                The file to upload. If None, an empty file is uploaded.
            signed_write_url:
                The url to upload the file to. As no authorization is used,
                the url must be a signed write url.
            headers:
                Specific headers for the request. Defaults to None.
            session:
                Optional requests session used to upload the file.

        Returns:
            The response of the put request.

        :meta private:  # Skip docstring generation
        """

        # check to see if server side encryption for S3 is desired
        # see https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html
        # see https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html
        lightly_s3_sse_kms_key = os.environ.get(LIGHTLY_S3_SSE_KMS_KEY, "").strip()
        # Only set s3 related headers when we are talking with s3
        if (
            utils.get_signed_url_destination(signed_write_url) == DatasourceType.S3
            and lightly_s3_sse_kms_key
        ):
            if headers is None:
                headers = {}
            # don't override previously set SSE
            if "x-amz-server-side-encryption" not in headers:
                if lightly_s3_sse_kms_key.lower() == "true":
                    # enable SSE with the key of amazon
                    headers["x-amz-server-side-encryption"] = "AES256"
                else:
                    # enable SSE with specific customer KMS key
                    headers["x-amz-server-side-encryption"] = "aws:kms"
                    headers[
                        "x-amz-server-side-encryption-aws-kms-key-id"
                    ] = lightly_s3_sse_kms_key

        # start requests session and make put request
        sess = session or requests
        if headers is not None:
            response = sess.put(signed_write_url, data=file, headers=headers)
        else:
            response = sess.put(signed_write_url, data=file)
        response.raise_for_status()
        return response



================================================
FILE: lightly/api/api_workflow_collaboration.py
================================================
from typing import List

from lightly.openapi_generated.swagger_client.models import (
    SharedAccessConfigCreateRequest,
    SharedAccessConfigData,
    SharedAccessType,
)


class _CollaborationMixin:
    def share_dataset_only_with(self, dataset_id: str, user_emails: List[str]) -> None:
        """Shares a dataset with a list of users.

        This method overwrites the list of users that have had access to the dataset
        before. If you want to add someone new to the list, make sure you first fetch
        the list of users with access and include them in the `user_emails`
        parameter.

        Args:
            dataset_id:
                ID of the dataset to be shared.
            user_emails:
                List of email addresses of users who will get access to the dataset.

        Examples:
          >>> # share a dataset with a user
          >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
          >>> client.share_dataset_only_with(dataset_id="MY_DATASET_ID", user_emails=["user@something.com"])
          >>>
          >>> # share dataset with a user while keep sharing it with previous users
          >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
          >>> user_emails = client.get_shared_users(dataset_id="MY_DATASET_ID")
          >>> user_emails.append("additional_user2@something.com")
          >>> client.share_dataset_only_with(dataset_id="MY_DATASET_ID", user_emails=user_emails)
          >>>
          >>> # revoke access to all users
          >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
          >>> client.share_dataset_only_with(dataset_id="MY_DATASET_ID", user_emails=[])
        """
        body = SharedAccessConfigCreateRequest(
            access_type=SharedAccessType.WRITE, users=user_emails, creator=self._creator
        )
        self._collaboration_api.create_or_update_shared_access_config_by_dataset_id(
            shared_access_config_create_request=body, dataset_id=dataset_id
        )

    def get_shared_users(self, dataset_id: str) -> List[str]:
        """Fetches a list of users that have access to the dataset.

        Args:
            dataset_id:
                Dataset ID.

        Returns:
            List of email addresses of users that have write access to the dataset.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> client.get_shared_users(dataset_id="MY_DATASET_ID")
            >>> ["user@something.com"]
        """

        access_configs: List[
            SharedAccessConfigData
        ] = self._collaboration_api.get_shared_access_configs_by_dataset_id(
            dataset_id=dataset_id
        )
        user_emails = []

        # iterate through configs and find first WRITE config
        # we use the same hard rule in the frontend to communicate with the API
        # as we currently only support WRITE access
        for access_config in access_configs:
            if access_config.access_type == SharedAccessType.WRITE:
                user_emails.extend(access_config.users)
                break

        return user_emails



================================================
FILE: lightly/api/api_workflow_compute_worker.py
================================================
import copy
import dataclasses
import difflib
import json
import time
from functools import partial
from typing import Any, Callable, Dict, Iterator, List, Optional, Type, TypeVar, Union

from lightly.api import retry_utils, utils
from lightly.openapi_generated.swagger_client.api_client import ApiClient
from lightly.openapi_generated.swagger_client.models import (
    AutoTask,
    AutoTaskTiling,
    CreateDockerWorkerRegistryEntryRequest,
    DockerRunData,
    DockerRunScheduledCreateRequest,
    DockerRunScheduledData,
    DockerRunScheduledPriority,
    DockerRunScheduledState,
    DockerRunState,
    DockerWorkerConfigOmniVXCreateRequest,
    DockerWorkerConfigV3Lightly,
    DockerWorkerConfigV4,
    DockerWorkerConfigV4Docker,
    DockerWorkerRegistryEntryData,
    DockerWorkerType,
    SelectionConfigV4,
    SelectionConfigV4Entry,
    SelectionConfigV4EntryInput,
    SelectionConfigV4EntryStrategy,
    TagData,
)
from lightly.openapi_generated.swagger_client.rest import ApiException

STATE_SCHEDULED_ID_NOT_FOUND = "CANCELED_OR_NOT_EXISTING"


class InvalidConfigurationError(RuntimeError):
    pass


@dataclasses.dataclass
class ComputeWorkerRunInfo:
    """Information about a Lightly Worker run.

    Attributes:
        state:
            The state of the Lightly Worker run.
        message:
            The last message of the Lightly Worker run.
    """

    state: Union[
        DockerRunState, DockerRunScheduledState.OPEN, STATE_SCHEDULED_ID_NOT_FOUND
    ]
    message: str

    def in_end_state(self) -> bool:
        """Checks whether the Lightly Worker run has ended."""
        return self.state in [
            DockerRunState.COMPLETED,
            DockerRunState.ABORTED,
            DockerRunState.FAILED,
            DockerRunState.CRASHED,
            STATE_SCHEDULED_ID_NOT_FOUND,
        ]

    def ended_successfully(self) -> bool:
        """Checkes whether the Lightly Worker run ended successfully or failed.

        Returns:
            A boolean value indicating if the Lightly Worker run was successful.
            True if the run was successful.

        Raises:
            ValueError:
                If the Lightly Worker run is still in progress.
        """
        if not self.in_end_state():
            raise ValueError("Lightly Worker run is still in progress.")
        return self.state == DockerRunState.COMPLETED


class _ComputeWorkerMixin:
    def register_compute_worker(
        self, name: str = "Default", labels: Optional[List[str]] = None
    ) -> str:
        """Registers a new Lightly Worker.

        The ID of the registered worker will be returned. If a worker with the same
        name already exists, the ID of the existing worker is returned.

        Args:
            name:
                The name of the Lightly Worker.
            labels:
                The labels of the Lightly Worker.
                See our docs for more information regarding the labels parameter:
                https://docs.lightly.ai/docs/assign-scheduled-runs-to-specific-workers

        Returns:
            ID of the registered Lightly Worker.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> worker_id = client.register_compute_worker(name="my-worker", labels=["worker-label"])
            >>> worker_id
            '64709eac61e9ce68180a6529'
        """
        if labels is None:
            labels = []
        request = CreateDockerWorkerRegistryEntryRequest(
            name=name,
            worker_type=DockerWorkerType.FULL,
            labels=labels,
            creator=self._creator,
        )
        response = self._compute_worker_api.register_docker_worker(request)
        return response.id

    def get_compute_worker_ids(self) -> List[str]:
        """Fetches the IDs of all registered Lightly Workers.

        Returns:
            A list of worker IDs.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> worker_ids = client.get_compute_worker_ids()
            >>> worker_ids
            ['64709eac61e9ce68180a6529', '64709f8f61e9ce68180a652a']
        """
        entries = self._compute_worker_api.get_docker_worker_registry_entries()
        return [entry.id for entry in entries]

    def get_compute_workers(self) -> List[DockerWorkerRegistryEntryData]:
        """Fetches details of all registered Lightly Workers.

        Returns:
            A list of Lightly Worker details.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> workers = client.get_compute_workers()
            >>> workers
            [{'created_at': 1685102336056,
                'docker_version': '2.6.0',
                'id': '64709eac61e9ce68180a6529',
                'labels': [],
                ...
            }]
        """
        entries: list[
            DockerWorkerRegistryEntryData
        ] = self._compute_worker_api.get_docker_worker_registry_entries()
        return entries

    def delete_compute_worker(self, worker_id: str) -> None:
        """Removes a Lightly Worker.

        Args:
            worker_id:
                ID of the worker to be removed.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> worker_ids = client.get_compute_worker_ids()
            >>> worker_ids
            ['64709eac61e9ce68180a6529']
            >>> client.delete_compute_worker(worker_id="64709eac61e9ce68180a6529")
            >>> client.get_compute_worker_ids()
            []
        """
        self._compute_worker_api.delete_docker_worker_registry_entry_by_id(worker_id)

    def create_compute_worker_config(
        self,
        worker_config: Optional[Dict[str, Any]] = None,
        lightly_config: Optional[Dict[str, Any]] = None,
        selection_config: Optional[Union[Dict[str, Any], SelectionConfigV4]] = None,
    ) -> str:
        """Creates a new configuration for a Lightly Worker run.

        See our docs for more information regarding the different configurations:
        https://docs.lightly.ai/docs/all-configuration-options

        Args:
            worker_config:
                Lightly Worker configuration.
            lightly_config:
                Lightly configuration.
            selection_config:
                Selection configuration.

        Returns:
            The ID of the created config.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> selection_config = {
            ...     "n_samples": 3,
            ...     "strategies": [
            ...         {
            ...             "input": {"type": "RANDOM", "random_seed": 42},
            ...             "strategy": {"type": "WEIGHTS"},
            ...         }
            ...     ],
            ... }
            >>> config_id = client.create_compute_worker_config(
            ...     selection_config=selection_config,
            ... )

        :meta private:  # Skip docstring generation
        """
        if isinstance(selection_config, dict):
            selection = selection_config_from_dict(cfg=selection_config)
        else:
            selection = selection_config

        if worker_config is not None:
            worker_config_cc = _config_to_camel_case(cfg=worker_config)
            deserialize_worker_config = _get_deserialize(
                api_client=self.api_client,
                klass=DockerWorkerConfigV4Docker,
            )
            docker = deserialize_worker_config(worker_config_cc)
            _validate_config(cfg=worker_config, obj=docker)
        else:
            docker = None

        if lightly_config is not None:
            lightly_config_cc = _config_to_camel_case(cfg=lightly_config)
            deserialize_lightly_config = _get_deserialize(
                api_client=self.api_client,
                klass=DockerWorkerConfigV3Lightly,
            )
            lightly = deserialize_lightly_config(lightly_config_cc)
            _validate_config(cfg=lightly_config, obj=lightly)
        else:
            lightly = None

        config = DockerWorkerConfigV4(
            worker_type=DockerWorkerType.FULL,
            docker=docker,
            lightly=lightly,
            selection=selection,
        )
        request = DockerWorkerConfigOmniVXCreateRequest.from_dict(
            {
                "version": "V4",
                "config": config.to_dict(by_alias=True),
                "creator": self._creator,
            }
        )
        response = self._compute_worker_api.create_docker_worker_config_vx(request)
        return response.id

    def schedule_compute_worker_run(
        self,
        worker_config: Optional[Dict[str, Any]] = None,
        lightly_config: Optional[Dict[str, Any]] = None,
        selection_config: Optional[Union[Dict[str, Any], SelectionConfigV4]] = None,
        priority: str = DockerRunScheduledPriority.MID,
        runs_on: Optional[List[str]] = None,
    ) -> str:
        """Schedules a run with the given configurations.

        See our docs for more information regarding the different configurations:
        https://docs.lightly.ai/docs/all-configuration-options

        Args:
            worker_config:
                Lightly Worker configuration.
            lightly_config:
                Lightly configuration.
            selection_config:
                Selection configuration.
            runs_on:
                The required labels the Lightly Worker must have to take the job.
                See our docs for more information regarding the runs_on paramter:
                https://docs.lightly.ai/docs/assign-scheduled-runs-to-specific-workers

        Returns:
            The id of the scheduled run.

        Raises:
            ApiException:
                If the API call returns a status code other than 200.
                    400: Missing or invalid parameters
                    402: Insufficient plan
                    403: Not authorized for this resource or invalid token
                    404: Resource (dataset or config) not found
                    422: Missing or invalid file in datasource
            InvalidConfigError:
                If one of the configurations is invalid.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> selection_config = {...}
            >>> worker_labels = ["worker-label"]
            >>> run_id = client.schedule_compute_worker_run(
            ...     selection_config=selection_config, runs_on=worker_labels
            ... )
        """
        if runs_on is None:
            runs_on = []
        config_id = self.create_compute_worker_config(
            worker_config=worker_config,
            lightly_config=lightly_config,
            selection_config=selection_config,
        )
        request = DockerRunScheduledCreateRequest(
            config_id=config_id,
            priority=priority,
            runs_on=runs_on,
            creator=self._creator,
        )
        response = self._compute_worker_api.create_docker_run_scheduled_by_dataset_id(
            docker_run_scheduled_create_request=request,
            dataset_id=self.dataset_id,
        )
        return response.id

    def get_compute_worker_runs_iter(
        self,
        dataset_id: Optional[str] = None,
    ) -> Iterator[DockerRunData]:
        """Returns an iterator over all Lightly Worker runs for the user.

        Args:
            dataset_id:
                Target dataset ID. Optional. If set, only runs with the given dataset
                will be returned.

        Returns:
            Runs iterator.

        """
        if dataset_id is not None:
            return utils.paginate_endpoint(
                self._compute_worker_api.get_docker_runs_query_by_dataset_id,
                dataset_id=dataset_id,
            )
        else:
            return utils.paginate_endpoint(
                self._compute_worker_api.get_docker_runs,
            )

    def get_compute_worker_runs(
        self,
        dataset_id: Optional[str] = None,
    ) -> List[DockerRunData]:
        """Fetches all Lightly Worker runs for the user.

        Args:
            dataset_id:
                Target dataset ID. Optional. If set, only runs with the given dataset
                will be returned.

        Returns:
            Runs sorted by creation time from the oldest to the latest.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> client.get_compute_worker_runs()
            [{'artifacts': [...],
             'config_id': '6470a16461e9ce68180a6530',
             'created_at': 1679479418110,
             'dataset_id': '6470a36361e9ce68180a6531',
             'docker_version': '2.6.0',
             ...
             }]
        """
        runs: List[DockerRunData] = list(self.get_compute_worker_runs_iter(dataset_id))
        sorted_runs = sorted(runs, key=lambda run: run.created_at or -1)
        return sorted_runs

    def get_compute_worker_run(self, run_id: str) -> DockerRunData:
        """Fetches a Lightly Worker run.

        Args:
            run_id: Run ID.

        Returns:
            Details of the Lightly Worker run.

        Raises:
            ApiException:
                If no run with the given ID exists.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> client.get_compute_worker_run(run_id="6470a20461e9ce68180a6530")
            {'artifacts': [...],
             'config_id': '6470a16461e9ce68180a6530',
             'created_at': 1679479418110,
             'dataset_id': '6470a36361e9ce68180a6531',
             'docker_version': '2.6.0',
             ...
             }
        """
        return self._compute_worker_api.get_docker_run_by_id(run_id=run_id)

    def get_compute_worker_run_from_scheduled_run(
        self,
        scheduled_run_id: str,
    ) -> DockerRunData:
        """Fetches a Lightly Worker run given its scheduled run ID.

        Args:
            scheduled_run_id: Scheduled run ID.

        Returns:
            Details of the Lightly Worker run.

        Raises:
            ApiException:
                If no run with the given scheduled run ID exists or if the scheduled
                run is not yet picked up by a worker.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> client.get_compute_worker_run_from_scheduled_run(scheduled_run_id="646f338a8a5613b57d8b73a1")
            {'artifacts': [...],
             'config_id': '6470a16461e9ce68180a6530',
             'created_at': 1679479418110,
             'dataset_id': '6470a36361e9ce68180a6531',
             'docker_version': '2.6.0',
             ...
            }
        """
        return self._compute_worker_api.get_docker_run_by_scheduled_id(
            scheduled_id=scheduled_run_id
        )

    def get_scheduled_compute_worker_runs(
        self,
        state: Optional[str] = None,
    ) -> List[DockerRunScheduledData]:
        """Returns a list of scheduled Lightly Worker runs with the current dataset.

        Args:
            state:
                DockerRunScheduledState value. If specified, then only runs in the given
                state are returned. If omitted, then runs which have not yet finished
                (neither 'DONE' nor 'CANCELED') are returned. Valid states are 'OPEN',
                'LOCKED', 'DONE', and 'CANCELED'.

        Returns:
            A list of scheduled Lightly Worker runs.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> client.get_scheduled_compute_worker_runs(state="OPEN")
            [{'config_id': '646f34608a5613b57d8b73cc',
             'created_at': 1685009508254,
             'dataset_id': '6470a36361e9ce68180a6531',
             'id': '646f338a8a5613b57d8b73a1',
             'last_modified_at': 1685009542667,
             'owner': '643d050b8bcb91967ded65df',
             'priority': 'MID',
             'runs_on': ['worker-label'],
             'state': 'OPEN'}]
        """
        if state is not None:
            return self._compute_worker_api.get_docker_runs_scheduled_by_dataset_id(
                dataset_id=self.dataset_id,
                state=state,
            )
        return self._compute_worker_api.get_docker_runs_scheduled_by_dataset_id(
            dataset_id=self.dataset_id,
        )

    def _get_scheduled_run_by_id(self, scheduled_run_id: str) -> DockerRunScheduledData:
        """Returns the schedule run data given the id of the scheduled run.

        TODO (MALTE, 09/2022): Have a proper API endpoint for doing this.
        Args:
            scheduled_run_id:
                The ID with which the run was scheduled.

        Returns:
            Defails of the scheduled run.

        """
        try:
            run: DockerRunScheduledData = next(
                run
                for run in retry_utils.retry(
                    lambda: self._compute_worker_api.get_docker_runs_scheduled_by_dataset_id(
                        self.dataset_id
                    )
                )
                if run.id == scheduled_run_id
            )
            return run
        except StopIteration:
            raise ApiException(
                f"No scheduled run found for run with scheduled_run_id='{scheduled_run_id}'."
            )

    def get_compute_worker_run_info(
        self, scheduled_run_id: str
    ) -> ComputeWorkerRunInfo:
        """Returns information about the Lightly Worker run.

        Args:
            scheduled_run_id:
                ID of the scheduled run.

        Returns:
            Details of the Lightly Worker run.

        Examples:
            >>> # Scheduled a Lightly Worker run and get its state
            >>> scheduled_run_id = client.schedule_compute_worker_run(...)
            >>> run_info = client.get_compute_worker_run_info(scheduled_run_id)
            >>> print(run_info)

        """
        """
        Because we currently (09/2022) have different Database entries for a ScheduledRun and DockerRun,
        the logic is more complicated and covers three cases.
        """
        try:
            # Case 1: DockerRun exists.
            docker_run: DockerRunData = (
                self._compute_worker_api.get_docker_run_by_scheduled_id(
                    scheduled_id=scheduled_run_id
                )
            )
            info = ComputeWorkerRunInfo(
                state=docker_run.state, message=docker_run.message
            )
        except ApiException:
            try:
                # Case 2: DockerRun does NOT exist, but ScheduledRun exists.
                _ = self._get_scheduled_run_by_id(scheduled_run_id)
                info = ComputeWorkerRunInfo(
                    state=DockerRunScheduledState.OPEN,
                    message="Waiting for pickup by Lightly Worker. "
                    "Make sure to start a Lightly Worker connected to your "
                    "user token to process the job.",
                )
            except ApiException:
                # Case 3: NEITHER the DockerRun NOR the ScheduledRun exist.
                info = ComputeWorkerRunInfo(
                    state=STATE_SCHEDULED_ID_NOT_FOUND,
                    message=f"Could not find a job for the given run_id: '{scheduled_run_id}'. "
                    "The scheduled run does not exist or was canceled before "
                    "being picked up by a Lightly Worker.",
                )
        return info

    def compute_worker_run_info_generator(
        self, scheduled_run_id: str
    ) -> Iterator[ComputeWorkerRunInfo]:
        """Pulls information about a Lightly Worker run continuously.

        Polls the Lightly Worker status every 30s.
        If the status changed, an update pops up.
        If the Lightly Worker run finished, the generator stops.

        Args:
            scheduled_run_id:
                The id with which the run was scheduled.

        Returns:
            Generator of information about the Lightly Worker run status.

        Examples:
            >>> # Scheduled a Lightly Worker run and monitor its state
            >>> scheduled_run_id = client.schedule_compute_worker_run(...)
            >>> for run_info in client.compute_worker_run_info_generator(scheduled_run_id):
            >>>     print(f"Lightly Worker run is now in state='{run_info.state}' with message='{run_info.message}'")
            >>>

        """
        last_run_info = None
        while True:
            run_info = self.get_compute_worker_run_info(
                scheduled_run_id=scheduled_run_id
            )

            # Only yield new run_info
            if run_info != last_run_info:
                yield run_info

            # Break if the scheduled run is in one of the end states.
            if run_info.in_end_state():
                break

            # Wait before polling the state again
            time.sleep(30)  # Keep this at 30s or larger to prevent rate limiting.

            last_run_info = run_info

    def get_compute_worker_run_tags(self, run_id: str) -> List[TagData]:
        """Returns all tags from a run with the current dataset.

        Only returns tags for runs made with Lightly Worker version >=2.4.2.

        Args:
            run_id:
                Run ID from which to return tags.

        Returns:
            List of tags created by the run. The tags are ordered by creation date from
            newest to oldest.

        Examples:
            >>> # Get filenames from last run.
            >>>
            >>> from lightly.api import ApiWorkflowClient
            >>> client = ApiWorkflowClient(
            >>>     token="MY_LIGHTLY_TOKEN", dataset_id="MY_DATASET_ID"
            >>> )
            >>> tags = client.get_compute_worker_run_tags(run_id="MY_LAST_RUN_ID")
            >>> filenames = client.export_filenames_by_tag_name(tag_name=tags[0].name)

        """
        tags = self._compute_worker_api.get_docker_run_tags(run_id=run_id)
        tags_in_dataset = [tag for tag in tags if tag.dataset_id == self.dataset_id]
        return tags_in_dataset


def selection_config_from_dict(cfg: Dict[str, Any]) -> SelectionConfigV4:
    """Recursively converts selection config from dict to a SelectionConfigV4 instance."""
    new_cfg = copy.deepcopy(cfg)
    strategies = []
    for entry in new_cfg.get("strategies", []):
        strategies.append(selection_config_entry_from_dict(entry=entry))
    new_cfg["strategies"] = strategies
    auto_tasks = []
    for entry in new_cfg.get("auto_tasks", []):
        auto_tasks.append(auto_task_from_dict(entry=entry))
    new_cfg["auto_tasks"] = auto_tasks
    return SelectionConfigV4(**new_cfg)


def selection_config_entry_from_dict(entry: Dict[str, Any]) -> AutoTask:
    new_entry = copy.deepcopy(entry)
    new_entry["input"] = SelectionConfigV4EntryInput(**new_entry["input"])
    new_entry["strategy"] = SelectionConfigV4EntryStrategy(**new_entry["strategy"])
    return SelectionConfigV4Entry(**new_entry)


def auto_task_from_dict(entry: Dict[str, Any]) -> AutoTask:
    auto_task_type_to_class = {
        "TILING": AutoTaskTiling,
    }
    if entry["type"] not in auto_task_type_to_class:
        raise ValueError(
            f"AutoTask type '{entry['type']}' not supported. "
            f"Supported types are: {list(auto_task_type_to_class.keys())}"
        )
    auto_task_class = auto_task_type_to_class[entry["type"]]
    auto_task_instance = auto_task_class(**entry)
    return AutoTask(actual_instance=auto_task_instance)


_T = TypeVar("_T")


def _get_deserialize(
    api_client: ApiClient,
    klass: Type[_T],
) -> Callable[[Dict[str, Any]], _T]:
    """Returns the deserializer of the ApiClient class for class klass.

    TODO(Philipp, 02/23): We should replace this by our own deserializer which
    accepts snake case strings as input.

    The deserializer takes a dictionary and and returns an instance of klass.

    """
    deserialize = getattr(api_client, "_ApiClient__deserialize")
    return partial(deserialize, klass=klass)


def _config_to_camel_case(cfg: Dict[str, Any]) -> Dict[str, Any]:
    """Converts all keys in the cfg dictionary to camelCase."""
    cfg_camel_case = {}
    for key, value in cfg.items():
        key_camel_case = _snake_to_camel_case(key)
        if isinstance(value, dict):
            cfg_camel_case[key_camel_case] = _config_to_camel_case(value)
        else:
            cfg_camel_case[key_camel_case] = value
    return cfg_camel_case


def _snake_to_camel_case(snake: str) -> str:
    """Converts the snake_case input to camelCase."""
    components = snake.split("_")
    return components[0] + "".join(component.title() for component in components[1:])


def _validate_config(
    cfg: Optional[Dict[str, Any]],
    obj: Any,
) -> None:
    """Validates that all keys in cfg are legitimate configuration options.

    Recursively checks if the keys in the cfg dictionary match the attributes of
    the DockerWorkerConfigV2Docker/DockerWorkerConfigV2Lightly instances. If not,
    suggests a best match.

    Raises:
        InvalidConfigurationError: If obj is not a valid config.

    """

    if cfg is None:
        return

    for key, item in cfg.items():
        if not hasattr(obj, key):
            possible_options = list(obj.__fields__.keys())
            closest_match = difflib.get_close_matches(
                word=key, possibilities=possible_options, n=1, cutoff=0.0
            )[0]
            error_msg = (
                f"Option '{key}' does not exist! Did you mean '{closest_match}'?"
            )
            raise InvalidConfigurationError(error_msg)
        if isinstance(item, dict):
            _validate_config(item, getattr(obj, key))



================================================
FILE: lightly/api/api_workflow_datasets.py
================================================
import warnings
from itertools import chain
from typing import Iterator, List, Optional, Set

from lightly.api import utils
from lightly.openapi_generated.swagger_client.models import (
    CreateEntityResponse,
    DatasetCreateRequest,
    DatasetData,
    DatasetType,
)
from lightly.openapi_generated.swagger_client.rest import ApiException


class _DatasetsMixin:
    @property
    def dataset_type(self) -> str:
        """Returns the dataset type of the current dataset."""
        dataset = self._get_current_dataset()
        return dataset.type  #  type: ignore

    def _get_current_dataset(self) -> DatasetData:
        """Returns the dataset with id == self.dataset_id."""
        return self.get_dataset_by_id(dataset_id=self.dataset_id)

    def dataset_exists(self, dataset_id: str) -> bool:
        """Checks if a dataset exists.

        Args:
            dataset_id: Dataset ID.

        Returns:
            True if the dataset exists and False otherwise.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> client.create_dataset("your-dataset-name", dataset_type=DatasetType.IMAGES)
            >>> dataset_id = client.dataset_id
            >>> client.dataset_exists(dataset_id=dataset_id)
            True
        """
        try:
            self.get_dataset_by_id(dataset_id)
            return True
        except ApiException as exception:
            if exception.status == 404:  # Not Found
                return False
            raise exception

    def dataset_name_exists(
        self, dataset_name: str, shared: Optional[bool] = False
    ) -> bool:
        """Checks if a dataset with the given name exists.

        There can be multiple datasets with the same name accessible to the current
        user. This can happen if either:
        * A dataset has been explicitly shared with the user
        * The user has access to team datasets
        The `shared` flag controls whether these datasets are checked.

        Args:
            dataset_name:
                Name of the dataset.
            shared:
                * If False (default), checks only datasets owned by the user.
                * If True, checks datasets which have been shared with the user,
                including team datasets. Excludes user's own datasets.
                * If None, checks all datasets the users has access to.

        Returns:
            A boolean value indicating whether any dataset with the given name exists.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> client.create_dataset("your-dataset-name", dataset_type=DatasetType.IMAGES)
            >>> client.dataset_name_exists(dataset_name="your-dataset-name")
            True
        """
        return bool(self.get_datasets_by_name(dataset_name=dataset_name, shared=shared))

    def get_dataset_by_id(self, dataset_id: str) -> DatasetData:
        """Fetches a dataset by ID.

        Args:
            dataset_id: Dataset ID.

        Returns:
            The dataset with the given dataset id.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> client.create_dataset("your-dataset-name", dataset_type=DatasetType.IMAGES)
            >>> dataset_id = client.dataset_id
            >>> client.get_dataset_by_id(dataset_id=dataset_id)
            {'created_at': 1685009504596,
             'datasource_processed_until_timestamp': 1685009513,
             'datasources': ['646f346004d77b4e1424e67e', '646f346004d77b4e1424e695'],
             'id': '646f34608a5613b57d8b73c9',
             'img_type': 'full',
             'type': 'Images',
             ...}
        """
        dataset: DatasetData = self._datasets_api.get_dataset_by_id(dataset_id)
        return dataset

    def get_datasets_by_name(
        self,
        dataset_name: str,
        shared: Optional[bool] = False,
    ) -> List[DatasetData]:
        """Fetches datasets by name.

        There can be multiple datasets with the same name accessible to the current
        user. This can happen if either:
        * A dataset has been explicitly shared with the user
        * The user has access to team datasets
        The `shared` flag controls whether these datasets are returned.

        Args:
            dataset_name:
                Name of the target dataset.
            shared:
                * If False (default), returns only datasets owned by the user. In this
                case at most one dataset will be returned.
                * If True, returns datasets which have been shared with the user,
                including team datasets. Excludes user's own datasets. Can return
                multiple datasets.
                * If None, returns all datasets the users has access to. Can return
                multiple datasets.

        Returns:
            A list of datasets that match the name. If no datasets with the name exist,
            an empty list is returned.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> client.create_dataset("your-dataset-name", dataset_type=DatasetType.IMAGES)
            >>> client.get_datasets_by_name(dataset_name="your-dataset-name")
            [{'created_at': 1685009504596,
             'datasource_processed_until_timestamp': 1685009513,
             'datasources': ['646f346004d77b4e1424e67e', '646f346004d77b4e1424e695'],
             'id': '646f34608a5613b57d8b73c9',
             'img_type': 'full',
             'type': 'Images',
             ...}]
            >>>
            >>> # Non-existent dataset
            >>> client.get_datasets_by_name(dataset_name="random-name")
            []
        """
        datasets: List[DatasetData] = []
        if not shared or shared is None:
            datasets.extend(
                list(
                    utils.paginate_endpoint(
                        self._datasets_api.get_datasets_query_by_name,
                        dataset_name=dataset_name,
                        exact=True,
                        shared=False,
                    )
                )
            )
        if shared or shared is None:
            datasets.extend(
                list(
                    utils.paginate_endpoint(
                        self._datasets_api.get_datasets_query_by_name,
                        dataset_name=dataset_name,
                        exact=True,
                        shared=True,
                    )
                )
            )
            datasets.extend(
                list(
                    utils.paginate_endpoint(
                        self._datasets_api.get_datasets_query_by_name,
                        dataset_name=dataset_name,
                        exact=True,
                        get_assets_of_team=True,
                    )
                )
            )

        # De-duplicate datasets because results from shared=True and
        # those from get_assets_of_team=True might overlap
        dataset_ids: Set[str] = set()
        filtered_datasets: List[DatasetData] = []
        for dataset in datasets:
            if dataset.id not in dataset_ids:
                dataset_ids.add(dataset.id)
                filtered_datasets.append(dataset)

        return filtered_datasets

    def get_datasets_iter(
        self, shared: Optional[bool] = False
    ) -> Iterator[DatasetData]:
        """Returns an iterator over all datasets owned by the current user.

        There can be multiple datasets with the same name accessible to the current
        user. This can happen if either:
        * A dataset has been explicitly shared with the user
        * The user has access to team datasets
        The `shared` flag controls whether these datasets are returned.

        Args:
            shared:
                * If False (default), returns only datasets owned by the user. In this
                case at most one dataset will be returned.
                * If True, returns datasets which have been shared with the user,
                including team datasets. Excludes user's own datasets. Can return
                multiple datasets.
                * If None, returns all datasets the users has access to. Can return
                multiple datasets.

        Returns:
            An iterator over datasets owned by the current user.
        """
        dataset_iterable: Iterator[DatasetData] = (_ for _ in ())
        if not shared or shared is None:
            dataset_iterable = utils.paginate_endpoint(
                self._datasets_api.get_datasets,
                shared=False,
            )
        if shared or shared is None:
            dataset_iterable = chain(
                dataset_iterable,
                utils.paginate_endpoint(
                    self._datasets_api.get_datasets,
                    shared=True,
                ),
            )
            dataset_iterable = chain(
                dataset_iterable,
                utils.paginate_endpoint(
                    self._datasets_api.get_datasets,
                    get_assets_of_team=True,
                ),
            )

        # De-duplicate datasets because results from shared=True and
        # those from get_assets_of_team=True might overlap
        dataset_ids: Set[str] = set()
        for dataset in dataset_iterable:
            if dataset.id not in dataset_ids:
                dataset_ids.add(dataset.id)
                yield dataset

    def get_datasets(self, shared: Optional[bool] = False) -> List[DatasetData]:
        """Returns all datasets owned by the current user.

        There can be multiple datasets with the same name accessible to the current
        user. This can happen if either:
        * A dataset has been explicitly shared with the user
        * The user has access to team datasets
        The `shared` flag controls whether these datasets are returned.

        Args:
            shared:
                * If False (default), returns only datasets owned by the user. In this
                case at most one dataset will be returned.
                * If True, returns datasets which have been shared with the user,
                including team datasets. Excludes user's own datasets. Can return
                multiple datasets.
                * If None, returns all datasets the users has access to. Can return
                multiple datasets.

        Returns:
            A list of datasets owned by the current user.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> client.create_dataset("your-dataset-name", dataset_type=DatasetType.IMAGES)
            >>> client.get_datasets()
            [{'created_at': 1685009504596,
             'datasource_processed_until_timestamp': 1685009513,
             'datasources': ['646f346004d77b4e1424e67e', '646f346004d77b4e1424e695'],
             'id': '646f34608a5613b57d8b73c9',
             'img_type': 'full',
             'type': 'Images',
             ...}]
        """
        return list(self.get_datasets_iter(shared))

    def get_all_datasets(self) -> List[DatasetData]:
        """Returns all datasets the user has access to.

        DEPRECATED in favour of get_datasets(shared=None) and will be removed in the
        future.
        """
        warnings.warn(
            "get_all_datasets() is deprecated in favour of get_datasets(shared=None) "
            "and will be removed in the future.",
            DeprecationWarning,
        )
        owned_datasets = self.get_datasets(shared=None)
        return owned_datasets

    def set_dataset_id_by_name(
        self, dataset_name: str, shared: Optional[bool] = False
    ) -> None:
        """Sets the dataset ID in the API client given the name of the desired dataset.

        There can be multiple datasets with the same name accessible to the current
        user. This can happen if either:
        * A dataset has been explicitly shared with the user
        * The user has access to team datasets
        The `shared` flag controls whether these datasets are also checked. If multiple
        datasets with the given name are found, the API client uses the ID of the first
        dataset and prints a warning message.

        Args:
            dataset_name:
                The name of the target dataset.
            shared:
                * If False (default), checks only datasets owned by the user.
                * If True, returns datasets which have been shared with the user,
                including team datasets. Excludes user's own datasets. There can be
                multiple candidate datasets.
                * If None, returns all datasets the users has access to. There can be
                multiple candidate datasets.

        Raises:
            ValueError:
                If no dataset with the given name exists.

        Examples:
            >>> # A new session. Dataset "old-dataset" was created before.
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> client.set_dataset_id_by_name("old-dataset")
        """
        datasets = self.get_datasets_by_name(dataset_name=dataset_name, shared=shared)
        if not datasets:
            raise ValueError(
                f"A dataset with the name '{dataset_name}' does not exist on the "
                f"Lightly Platform. Please create it first."
            )
        self._dataset_id = datasets[0].id
        if len(datasets) > 1:
            msg = (
                f"Found {len(datasets)} datasets with the name '{dataset_name}'. Their "
                f"ids are {[dataset.id for dataset in datasets]}. "
                f"The dataset_id of the client was set to '{self._dataset_id}'. "
            )
            if shared or shared is None:
                msg += (
                    f"We noticed that you set shared={shared} which also retrieves "
                    f"datasets shared with you. Set shared=False to only consider "
                    "datasets you own."
                )
            warnings.warn(msg)

    def create_dataset(
        self,
        dataset_name: str,
        dataset_type: str = DatasetType.IMAGES,
    ) -> None:
        """Creates a dataset on the Lightly Platform.

        The dataset_id of the created dataset is stored in the client.dataset_id
        attribute and all further requests with the client will use the created dataset
        by default.

        Args:
            dataset_name:
                The name of the dataset to be created.
            dataset_type:
                The type of the dataset. We recommend to use the API provided constants
                `DatasetType.IMAGES` and `DatasetType.VIDEOS`.

        Raises:
            ValueError: If a dataset with dataset_name already exists.

        Examples:
            >>> from lightly.api import ApiWorkflowClient
            >>> from lightly.openapi_generated.swagger_client.models import DatasetType
            >>>
            >>> client = ApiWorkflowClient(token="YOUR_TOKEN")
            >>> client.create_dataset('your-dataset-name', dataset_type=DatasetType.IMAGES)
            >>>
            >>> # or to work with videos
            >>> client.create_dataset('your-dataset-name', dataset_type=DatasetType.VIDEOS)
            >>>
            >>> # retrieving dataset_id of the created dataset
            >>> dataset_id = client.dataset_id
            >>>
            >>> # future client requests use the created dataset by default
            >>> client.dataset_type
            'Videos'
        """

        if self.dataset_name_exists(dataset_name=dataset_name):
            raise ValueError(
                f"A dataset with the name '{dataset_name}' already exists! Please use "
                f"the `set_dataset_id_by_name()` method instead if you intend to reuse "
                f"an existing dataset."
            )
        self._create_dataset_without_check_existing(
            dataset_name=dataset_name,
            dataset_type=dataset_type,
        )

    def _create_dataset_without_check_existing(
        self, dataset_name: str, dataset_type: str
    ) -> None:
        """Creates a dataset on the Lightly Platform.

        No checking if a dataset with such a name already exists is performed.

        Args:
            dataset_name:
                The name of the dataset to be created.
            dataset_type:
                The type of the dataset. We recommend to use the API provided
                constants `DatasetType.IMAGES` and `DatasetType.VIDEOS`.

        """
        body = DatasetCreateRequest(
            name=dataset_name, type=dataset_type, creator=self._creator
        )
        response: CreateEntityResponse = self._datasets_api.create_dataset(
            dataset_create_request=body
        )
        self._dataset_id = response.id

    def create_new_dataset_with_unique_name(
        self,
        dataset_basename: str,
        dataset_type: str = DatasetType.IMAGES,
    ) -> None:
        """Creates a new dataset on the Lightly Platform.

        If a dataset with the specified name already exists,
        the name is suffixed by a counter value.

        Args:
            dataset_basename:
                The name of the dataset to be created.
            dataset_type:
                The type of the dataset. We recommend to use the API provided
                constants `DatasetType.IMAGES` and `DatasetType.VIDEOS`.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Create a dataset with a brand new name.
            >>> client.create_new_dataset_with_unique_name("new-dataset")
            >>> client.get_dataset_by_id(client.dataset_id)
            {'id': '6470abef4f0eb7e635c30954',
             'name': 'new-dataset',
             ...}
            >>>
            >>> # Create another dataset with the same name. This time, the
            >>> # new dataset should have a suffix `_1`.
            >>> client.create_new_dataset_with_unique_name("new-dataset")
            >>> client.get_dataset_by_id(client.dataset_id)
            {'id': '6470ac194f0eb7e635c30990',
             'name': 'new-dataset_1',
             ...}

        """
        if not self.dataset_name_exists(dataset_name=dataset_basename):
            self._create_dataset_without_check_existing(
                dataset_name=dataset_basename,
                dataset_type=dataset_type,
            )
        else:
            existing_datasets = list(
                utils.paginate_endpoint(
                    self._datasets_api.get_datasets_query_by_name,
                    dataset_name=dataset_basename,
                    exact=False,
                    shared=False,
                )
            )
            existing_dataset_names = {dataset.name for dataset in existing_datasets}
            counter = 1
            dataset_name = f"{dataset_basename}_{counter}"
            while dataset_name in existing_dataset_names:
                counter += 1
                dataset_name = f"{dataset_basename}_{counter}"
            self._create_dataset_without_check_existing(
                dataset_name=dataset_name,
                dataset_type=dataset_type,
            )

    def delete_dataset_by_id(self, dataset_id: str) -> None:
        """Deletes a dataset on the Lightly Platform.

        Args:
            dataset_id:
                The ID of the dataset to be deleted.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>> client.create_dataset("your-dataset-name", dataset_type=DatasetType.IMAGES)
            >>> dataset_id = client.dataset_id
            >>> client.dataset_exists(dataset_id=dataset_id)
            True
            >>>
            >>> # Delete the dataset
            >>> client.delete_dataset_by_id(dataset_id=dataset_id)
            >>> client.dataset_exists(dataset_id=dataset_id)
            False
        """
        self._datasets_api.delete_dataset_by_id(dataset_id=dataset_id)
        del self._dataset_id



================================================
FILE: lightly/api/api_workflow_datasource_listing.py
================================================
import time
import warnings
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Iterator, List, Optional, Set, Tuple, Union

import tqdm

from lightly.api.retry_utils import retry
from lightly.openapi_generated.swagger_client.models import (
    DatasourceConfig,
    DatasourceProcessedUntilTimestampRequest,
    DatasourceProcessedUntilTimestampResponse,
    DatasourceRawSamplesData,
)
from lightly.openapi_generated.swagger_client.models.datasource_raw_samples_data_row import (
    DatasourceRawSamplesDataRow,
)
from lightly.openapi_generated.swagger_client.models.divide_and_conquer_cursor_data import (
    DivideAndConquerCursorData,
)

DownloadFunction = Union[
    "DatasourcesApi.get_list_of_raw_samples_from_datasource_by_dataset_id",
    "DatasourcesApi.get_list_of_raw_samples_predictions_from_datasource_by_dataset_id",
    "DatasourcesApi.get_list_of_raw_samples_metadata_from_datasource_by_dataset_id",
]

DivideAndConquerFunction = Union[
    "DatasourcesApi.get_divide_and_conquer_list_of_raw_samples_from_datasource_by_dataset_id",
    "DatasourcesApi.get_divide_and_conquer_list_of_raw_samples_metadata_from_datasource_by_dataset_id",
    "DatasourcesApi.get_divide_and_conquer_list_of_raw_samples_predictions_from_datasource_by_dataset_id",
]


class _DatasourceListingMixin:
    def download_raw_samples(
        self,
        from_: int = 0,
        to: Optional[int] = None,
        relevant_filenames_file_name: Optional[str] = None,
        divide_and_conquer_shards: int = 1,
        use_redirected_read_url: bool = False,
        progress_bar: Optional[tqdm.tqdm] = None,
    ) -> List[Tuple[str, str]]:
        """Downloads filenames and read urls from the datasource.

        Only samples with timestamp between `from_` (inclusive) and `to` (inclusive)
        will be downloaded.

        Args:
            from_:
                Unix timestamp from which on samples are downloaded. Defaults to the
                very beginning (timestamp 0).
            to:
                Unix timestamp up to and including which samples are downloaded.
                Defaults to the current timestamp.
            relevant_filenames_file_name:
                Path to the relevant filenames text file in the cloud bucket.
                The path is relative to the datasource root. Optional.
            divide_and_conquer_shards:
                Number of shards to use for divide and conquer listing. Typically num_workers/cpu_count.
            use_redirected_read_url:
                Flag for redirected read urls. When this flag is true,
                RedirectedReadUrls are returned instead of ReadUrls, meaning that the
                returned URLs have unlimited access to the file.
                Defaults to False. When S3DelegatedAccess is configured, this flag has
                no effect because RedirectedReadUrls are always returned.
            progress_bar:
                Tqdm progress bar to show how many samples have already been
                retrieved.

        Returns:
            A list of (filename, url) tuples where each tuple represents a sample.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> client.download_raw_samples()
            [('image-1.png', 'https://......'), ('image-2.png', 'https://......')]

        :meta private:  # Skip docstring generation
        """
        return self._download_raw_files(
            download_function=self._datasources_api.get_list_of_raw_samples_from_datasource_by_dataset_id,
            dnc_function=self._datasources_api.get_divide_and_conquer_list_of_raw_samples_from_datasource_by_dataset_id,
            from_=from_,
            to=to,
            relevant_filenames_file_name=relevant_filenames_file_name,
            use_redirected_read_url=use_redirected_read_url,
            divide_and_conquer_shards=divide_and_conquer_shards,
            progress_bar=progress_bar,
        )

    def download_raw_predictions(
        self,
        task_name: str,
        from_: int = 0,
        to: Optional[int] = None,
        relevant_filenames_file_name: Optional[str] = None,
        run_id: Optional[str] = None,
        relevant_filenames_artifact_id: Optional[str] = None,
        divide_and_conquer_shards: int = 1,
        use_redirected_read_url: bool = False,
        progress_bar: Optional[tqdm.tqdm] = None,
    ) -> List[Tuple[str, str]]:
        """Downloads all prediction filenames and read urls from the datasource.

        See `download_raw_predictions_iter` for details.

        :meta private:  # Skip docstring generation
        """
        return list(
            self.download_raw_predictions_iter(
                task_name=task_name,
                from_=from_,
                to=to,
                relevant_filenames_file_name=relevant_filenames_file_name,
                run_id=run_id,
                relevant_filenames_artifact_id=relevant_filenames_artifact_id,
                divide_and_conquer_shards=divide_and_conquer_shards,
                use_redirected_read_url=use_redirected_read_url,
                progress_bar=progress_bar,
            )
        )

    def download_raw_predictions_iter(
        self,
        task_name: str,
        from_: int = 0,
        to: Optional[int] = None,
        relevant_filenames_file_name: Optional[str] = None,
        run_id: Optional[str] = None,
        relevant_filenames_artifact_id: Optional[str] = None,
        divide_and_conquer_shards: int = 1,
        use_redirected_read_url: bool = False,
        progress_bar: Optional[tqdm.tqdm] = None,
    ) -> Iterator[Tuple[str, str]]:
        """Downloads prediction filenames and read urls from the datasource.

        Only samples with timestamp between `from_` (inclusive) and `to` (inclusive)
        will be downloaded.

        Args:
            task_name:
                Name of the prediction task.
            from_:
                Unix timestamp from which on samples are downloaded. Defaults to the
                very beginning (timestamp 0).
            to:
                Unix timestamp up to and including which samples are downloaded.
                Defaults to the current timestamp.
            relevant_filenames_file_name:
                Path to the relevant filenames text file in the cloud bucket.
                The path is relative to the datasource root. Optional.
            run_id:
                Run ID. Optional. Should be given along with
                `relevant_filenames_artifact_id` to download relevant files only.
            relevant_filenames_artifact_id:
                ID of the relevant filename artifact. Optional. Should be given along
                with `run_id` to download relevant files only. Note that this is
                different from `relevant_filenames_file_name`.
            divide_and_conquer_shards:
                Number of shards to use for divide and conquer listing. Typically num_workers/cpu_count.
            use_redirected_read_url:
                Flag for redirected read urls. When this flag is true,
                RedirectedReadUrls are returned instead of ReadUrls, meaning that the
                returned URLs have unlimited access to the file.
                Defaults to False. When S3DelegatedAccess is configured, this flag has
                no effect because RedirectedReadUrls are always returned.
            progress_bar:
                Tqdm progress bar to show how many prediction files have already been
                retrieved.

        Returns:
            An iterator of (filename, url) tuples where each tuple represents a sample.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> task_name = "object-detection"
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> list(client.download_raw_predictions(task_name=task_name))
            [('.lightly/predictions/object-detection/image-1.json', 'https://......'),
             ('.lightly/predictions/object-detection/image-2.json', 'https://......')]

        :meta private:  # Skip docstring generation
        """
        return self._download_raw_files_divide_and_conquer_iter(
            download_function=self._datasources_api.get_list_of_raw_samples_predictions_from_datasource_by_dataset_id,
            dnc_function=self._datasources_api.get_divide_and_conquer_list_of_raw_samples_predictions_from_datasource_by_dataset_id,
            task_name=task_name,
            from_=from_,
            to=to,
            relevant_filenames_file_name=relevant_filenames_file_name,
            run_id=run_id,
            relevant_filenames_artifact_id=relevant_filenames_artifact_id,
            divide_and_conquer_shards=divide_and_conquer_shards,
            use_redirected_read_url=use_redirected_read_url,
            progress_bar=progress_bar,
        )

    def download_raw_metadata(
        self,
        from_: int = 0,
        to: Optional[int] = None,
        run_id: Optional[str] = None,
        relevant_filenames_artifact_id: Optional[str] = None,
        relevant_filenames_file_name: Optional[str] = None,
        divide_and_conquer_shards: int = 1,
        use_redirected_read_url: bool = False,
        progress_bar: Optional[tqdm.tqdm] = None,
    ) -> List[Tuple[str, str]]:
        """Downloads all metadata filenames and read urls from the datasource.

        See `download_raw_metadata_iter` for details.

        :meta private:  # Skip docstring generation
        """
        return list(
            self.download_raw_metadata_iter(
                from_=from_,
                to=to,
                run_id=run_id,
                relevant_filenames_artifact_id=relevant_filenames_artifact_id,
                relevant_filenames_file_name=relevant_filenames_file_name,
                divide_and_conquer_shards=divide_and_conquer_shards,
                use_redirected_read_url=use_redirected_read_url,
                progress_bar=progress_bar,
            )
        )

    def download_raw_metadata_iter(
        self,
        from_: int = 0,
        to: Optional[int] = None,
        run_id: Optional[str] = None,
        relevant_filenames_artifact_id: Optional[str] = None,
        relevant_filenames_file_name: Optional[str] = None,
        divide_and_conquer_shards: int = 1,
        use_redirected_read_url: bool = False,
        progress_bar: Optional[tqdm.tqdm] = None,
    ) -> Iterator[Tuple[str, str]]:
        """Downloads all metadata filenames and read urls from the datasource.

        Only samples with timestamp between `from_` (inclusive) and `to` (inclusive)
        will be downloaded.

        Args:
            from_:
                Unix timestamp from which on samples are downloaded. Defaults to the
                very beginning (timestamp 0).
            to:
                Unix timestamp up to and including which samples are downloaded.
                Defaults to the current timestamp.
            relevant_filenames_file_name:
                Path to the relevant filenames text file in the cloud bucket.
                The path is relative to the datasource root. Optional.
            run_id:
                Run ID. Optional. Should be given along with
                `relevant_filenames_artifact_id` to download relevant files only.
            relevant_filenames_artifact_id:
                ID of the relevant filename artifact. Optional. Should be given along
                with `run_id` to download relevant files only. Note that this is
                different from `relevant_filenames_file_name`.
            divide_and_conquer_shards:
                Number of shards to use for divide and conquer listing. Typically num_workers/cpu_count.
            use_redirected_read_url:
                Flag for redirected read urls. When this flag is true,
                RedirectedReadUrls are returned instead of ReadUrls, meaning that the
                returned URLs have unlimited access to the file.
                Defaults to False. When S3DelegatedAccess is configured, this flag has
                no effect because RedirectedReadUrls are always returned.
            progress_bar:
                Tqdm progress bar to show how many metadata files have already been
                retrieved.

        Returns:
            An iterator of (filename, url) tuples where each tuple represents a sample.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> list(client.download_raw_metadata_iter())
            [('.lightly/metadata/object-detection/image-1.json', 'https://......'),
             ('.lightly/metadata/object-detection/image-2.json', 'https://......')]

        :meta private:  # Skip docstring generation
        """
        return self._download_raw_files_divide_and_conquer_iter(
            download_function=self._datasources_api.get_list_of_raw_samples_metadata_from_datasource_by_dataset_id,
            dnc_function=self._datasources_api.get_divide_and_conquer_list_of_raw_samples_metadata_from_datasource_by_dataset_id,
            from_=from_,
            to=to,
            run_id=run_id,
            relevant_filenames_artifact_id=relevant_filenames_artifact_id,
            relevant_filenames_file_name=relevant_filenames_file_name,
            divide_and_conquer_shards=divide_and_conquer_shards,
            use_redirected_read_url=use_redirected_read_url,
            progress_bar=progress_bar,
        )

    def download_new_raw_samples(
        self,
        divide_and_conquer_shards: int = 1,
        use_redirected_read_url: bool = False,
    ) -> List[Tuple[str, str]]:
        """Downloads filenames and read urls of unprocessed samples from the datasource.

        All samples after the timestamp of `ApiWorkflowClient.get_processed_until_timestamp()` are
        fetched. After downloading the samples, the timestamp is updated to the current time.
        This function can be repeatedly called to retrieve new samples from the datasource.

        Args:
            divide_and_conquer_shards:
                Number of shards to use for divide and conquer listing. Typically num_workers/cpu_count.
            use_redirected_read_url:
                Flag for redirected read urls. When this flag is true,
                RedirectedReadUrls are returned instead of ReadUrls, meaning that the
                returned URLs have unlimited access to the file.
                Defaults to False. When S3DelegatedAccess is configured, this flag has
                no effect because RedirectedReadUrls are always returned.

        Returns:
            A list of (filename, url) tuples where each tuple represents a sample.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> client.download_new_raw_samples()
            [('image-3.png', 'https://......'), ('image-4.png', 'https://......')]
        """
        from_ = self.get_processed_until_timestamp()

        if from_ != 0:
            # We already processed samples at some point.
            # Add 1 because the samples with timestamp == from_
            # have already been processed
            from_ += 1

        to = int(time.time())
        data = self.download_raw_samples(
            from_=from_,
            to=to,
            relevant_filenames_file_name=None,
            divide_and_conquer_shards=divide_and_conquer_shards,
            use_redirected_read_url=use_redirected_read_url,
        )
        self.update_processed_until_timestamp(timestamp=to)
        return data

    def get_processed_until_timestamp(self) -> int:
        """Returns the timestamp until which samples have been processed.

        Returns:
            Unix timestamp of last processed sample.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> client.get_processed_until_timestamp()
            1684750513

        :meta private:  # Skip docstring generation
        """
        response: DatasourceProcessedUntilTimestampResponse = retry(
            fn=self._datasources_api.get_datasource_processed_until_timestamp_by_dataset_id,
            dataset_id=self.dataset_id,
        )
        timestamp = int(response.processed_until_timestamp)
        return timestamp

    def update_processed_until_timestamp(self, timestamp: int) -> None:
        """Sets the timestamp until which samples have been processed.

        Args:
            timestamp:
                Unix timestamp of last processed sample.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset.
            >>> # All samples are processed at this moment.
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> client.download_new_raw_samples()
            []
            >>>
            >>> # Set timestamp to an earlier moment to reprocess samples
            >>> client.update_processed_until_timestamp(1684749813)
            >>> client.download_new_raw_samples()
            [('image-3.png', 'https://......'), ('image-4.png', 'https://......')]
        """
        body = DatasourceProcessedUntilTimestampRequest(
            processed_until_timestamp=timestamp
        )
        retry(
            fn=self._datasources_api.update_datasource_processed_until_timestamp_by_dataset_id,
            dataset_id=self.dataset_id,
            datasource_processed_until_timestamp_request=body,
        )

    def get_datasource(self) -> DatasourceConfig:
        """Returns the datasource of the current dataset.

        Returns:
            Datasource data of the datasource of the current dataset.

        Raises:
            ApiException if no datasource was configured.

        """
        return self._datasources_api.get_datasource_by_dataset_id(self.dataset_id)

    def get_prediction_read_url(
        self,
        filename: str,
    ) -> str:
        """Returns a read-url for .lightly/predictions/{filename}.

        Args:
            filename:
                Filename for which to get the read-url.

        Returns:
            A read-url to the file. Note that a URL will be returned even if the file does not
            exist.

        :meta private:  # Skip docstring generation
        """
        return self._datasources_api.get_prediction_file_read_url_from_datasource_by_dataset_id(
            dataset_id=self.dataset_id,
            file_name=filename,
        )

    def get_metadata_read_url(
        self,
        filename: str,
    ) -> str:
        """Returns a read-url for .lightly/metadata/{filename}.

        Args:
            filename:
                Filename for which to get the read-url.

        Returns:
            A read-url to the file. Note that a URL will be returned even if the file does not
            exist.

        :meta private:  # Skip docstring generation
        """
        return self._datasources_api.get_metadata_file_read_url_from_datasource_by_dataset_id(
            dataset_id=self.dataset_id,
            file_name=filename,
        )

    def get_custom_embedding_read_url(
        self,
        filename: str,
    ) -> str:
        """Returns a read-url for .lightly/embeddings/{filename}.

        Args:
            filename:
                Filename for which to get the read-url.

        Returns:
            A read-url to the file. Note that a URL will be returned even if the file does not
            exist.

        :meta private:  # Skip docstring generation
        """
        return self._datasources_api.get_custom_embedding_file_read_url_from_datasource_by_dataset_id(
            dataset_id=self.dataset_id,
            file_name=filename,
        )

    def _get_divide_and_conquer_list_cursors(
        self,
        dnc_function: DivideAndConquerFunction,
        from_: int = 0,
        to: Optional[int] = None,
        relevant_filenames_file_name: Optional[str] = None,
        divide_and_conquer_shards: int = 1,
        **kwargs,
    ) -> List[str]:
        if to is None:
            to = int(time.time())

        divide_and_conquer_shards = max(1, divide_and_conquer_shards)

        relevant_filenames_kwargs = (
            {"relevant_filenames_file_name": relevant_filenames_file_name}
            if relevant_filenames_file_name
            else dict()
        )
        response: DivideAndConquerCursorData = retry(
            fn=dnc_function,
            dataset_id=self.dataset_id,
            var_from=from_,
            to=to,
            dnc_shards=divide_and_conquer_shards,
            **relevant_filenames_kwargs,
            **kwargs,
        )
        if not response.cursors or len(response.cursors) == 0:
            # this should never happen from the API side, but we handle it gracefully for the unlikely case
            raise ValueError("No cursors returned from the datasource.")
        return response.cursors

    def _download_raw_files(
        self,
        download_function: DownloadFunction,
        dnc_function: DivideAndConquerFunction,
        from_: int = 0,
        to: Optional[int] = None,
        relevant_filenames_file_name: Optional[str] = None,
        divide_and_conquer_shards: int = 1,
        use_redirected_read_url: bool = False,
        progress_bar: Optional[tqdm.tqdm] = None,
        **kwargs,
    ) -> List[Tuple[str, str]]:
        return list(
            self._download_raw_files_divide_and_conquer_iter(
                download_function=download_function,
                dnc_function=dnc_function,
                from_=from_,
                to=to,
                relevant_filenames_file_name=relevant_filenames_file_name,
                divide_and_conquer_shards=divide_and_conquer_shards,
                use_redirected_read_url=use_redirected_read_url,
                progress_bar=progress_bar,
                **kwargs,
            )
        )

    def _download_raw_files_divide_and_conquer_iter(
        self,
        download_function: DownloadFunction,
        dnc_function: DivideAndConquerFunction,
        from_: int = 0,
        to: Optional[int] = None,
        run_id: Optional[str] = None,
        relevant_filenames_artifact_id: Optional[str] = None,
        relevant_filenames_file_name: Optional[str] = None,
        divide_and_conquer_shards: int = 1,
        use_redirected_read_url: bool = False,
        progress_bar: Optional[tqdm.tqdm] = None,
        **kwargs,
    ) -> Iterator[Tuple[str, str]]:
        if run_id is not None and relevant_filenames_artifact_id is None:
            raise ValueError(
                "'relevant_filenames_artifact_id' should not be 'None' when 'run_id' "
                "is specified."
            )
        if run_id is None and relevant_filenames_artifact_id is not None:
            raise ValueError(
                "'run_id' should not be `None` when 'relevant_filenames_artifact_id' "
                "is specified."
            )
        relevant_filenames_kwargs = {}
        if run_id is not None and relevant_filenames_artifact_id is not None:
            relevant_filenames_kwargs["relevant_filenames_run_id"] = run_id
            relevant_filenames_kwargs[
                "relevant_filenames_artifact_id"
            ] = relevant_filenames_artifact_id

        cursors = self._get_divide_and_conquer_list_cursors(
            dnc_function=dnc_function,
            from_=from_,
            to=to,
            relevant_filenames_file_name=relevant_filenames_file_name,
            divide_and_conquer_shards=divide_and_conquer_shards,
            **relevant_filenames_kwargs,
            **kwargs,
        )

        def download_with_cursor(cursor):
            return list(
                self._download_raw_files_cursor_iter(
                    download_function=download_function,
                    cursor=cursor,
                    relevant_filenames_file_name=relevant_filenames_file_name,
                    use_redirected_read_url=use_redirected_read_url,
                    progress_bar=progress_bar,
                    **relevant_filenames_kwargs,
                    **kwargs,
                )
            )

        # download in parallel using threads
        with ThreadPoolExecutor(max_workers=len(cursors)) as executor:
            futures = [
                executor.submit(download_with_cursor, cursor) for cursor in cursors
            ]
            for future in as_completed(futures):
                yield from future.result()

    def _download_raw_files_cursor_iter(
        self,
        download_function: DownloadFunction,
        cursor: str,
        relevant_filenames_file_name: Optional[str] = None,
        use_redirected_read_url: bool = False,
        progress_bar: Optional[tqdm.tqdm] = None,
        **kwargs,
    ) -> Iterator[Tuple[str, str]]:
        relevant_filenames_kwargs = (
            {"relevant_filenames_file_name": relevant_filenames_file_name}
            if relevant_filenames_file_name
            else dict()
        )

        listed_filenames = set()

        def get_entries(
            response: DatasourceRawSamplesData,
        ) -> Iterator[Tuple[str, str]]:
            for entry in response.data:
                if _sample_unseen_and_valid(
                    sample=entry,
                    relevant_filenames_file_name=relevant_filenames_file_name,
                    listed_filenames=listed_filenames,
                ):
                    listed_filenames.add(entry.file_name)
                    yield entry.file_name, entry.read_url
                if progress_bar is not None:
                    progress_bar.update(1)

        active_cursor = cursor
        while active_cursor:
            response: DatasourceRawSamplesData = retry(
                fn=download_function,
                dataset_id=self.dataset_id,
                cursor=active_cursor,
                use_redirected_read_url=use_redirected_read_url,
                **relevant_filenames_kwargs,
                **kwargs,
            )
            yield from get_entries(response=response)

            active_cursor = response.cursor
            if not response.has_more:
                active_cursor = None


def _sample_unseen_and_valid(
    sample: DatasourceRawSamplesDataRow,
    relevant_filenames_file_name: Optional[str],
    listed_filenames: Set[str],
) -> bool:
    # Note: We want to remove these checks eventually. Absolute paths and relative paths
    # with dot notation should be handled either in the API or the Worker. Duplicate
    # filenames should be handled in the Worker as handling it in the API would require
    # too much memory.
    if sample.file_name.startswith("/"):
        warnings.warn(
            UserWarning(
                f"Absolute file paths like {sample.file_name} are not supported"
                f" in relevant filenames file {relevant_filenames_file_name} due to blob storage"
            )
        )
        return False
    elif sample.file_name.startswith(("./", "../")):
        warnings.warn(
            UserWarning(
                f"Using dot notation ('./', '../') like in {sample.file_name} is not supported"
                f" in relevant filenames file {relevant_filenames_file_name} due to blob storage"
            )
        )
        return False
    elif sample.file_name in listed_filenames:
        warnings.warn(
            UserWarning(
                f"Duplicate filename {sample.file_name} in relevant"
                f" filenames file {relevant_filenames_file_name}"
            )
        )
        return False
    else:
        return True



================================================
FILE: lightly/api/api_workflow_datasources.py
================================================
from typing import Dict, Optional, Union

from lightly.openapi_generated.swagger_client.models import (
    DatasourceConfig,
    DatasourcePurpose,
)


class _DatasourcesMixin:
    def get_datasource(self) -> DatasourceConfig:
        """Returns the datasource of the current dataset.

        Returns:
            Datasource data of the datasource of the current dataset.

        Raises:
            ApiException if no datasource was configured.

        """
        return self._datasources_api.get_datasource_by_dataset_id(self.dataset_id)

    def set_azure_config(
        self,
        container_name: str,
        account_name: str,
        sas_token: str,
        thumbnail_suffix: Optional[
            str
        ] = ".lightly/thumbnails/[filename]_thumb.[extension]",
        purpose: str = DatasourcePurpose.INPUT_OUTPUT,
    ) -> None:
        """Sets the Azure configuration for the datasource of the current dataset.

        See our docs for a detailed explanation on how to setup Lightly with
        Azure: https://docs.lightly.ai/docs/azure

        Args:
            container_name:
                Container name of the dataset, for example: "my-container/path/to/my/data".
            account_name:
                Azure account name.
            sas_token:
                Secure Access Signature token.
            thumbnail_suffix:
                Where to save thumbnails of the images in the dataset, for
                example ".lightly/thumbnails/[filename]_thumb.[extension]".
                Set to None to disable thumbnails and use the full images from the
                datasource instead.
            purpose:
                Datasource purpose, determines if datasource is read only (INPUT)
                or can be written to as well (LIGHTLY, INPUT_OUTPUT).

        """
        # TODO: Use DatasourceConfigAzure once we switch/update the api generator.
        self._datasources_api.update_datasource_by_dataset_id(
            datasource_config=DatasourceConfig.from_dict(
                {
                    "type": "AZURE",
                    "fullPath": container_name,
                    "thumbSuffix": thumbnail_suffix,
                    "accountName": account_name,
                    "accountKey": sas_token,
                    "purpose": purpose,
                }
            ),
            dataset_id=self.dataset_id,
        )

    def set_gcs_config(
        self,
        resource_path: str,
        project_id: str,
        credentials: str,
        thumbnail_suffix: Optional[
            str
        ] = ".lightly/thumbnails/[filename]_thumb.[extension]",
        purpose: str = DatasourcePurpose.INPUT_OUTPUT,
    ) -> None:
        """Sets the Google Cloud Storage configuration for the datasource of the
        current dataset.

        See our docs for a detailed explanation on how to setup Lightly with
        Google Cloud Storage: https://docs.lightly.ai/docs/google-cloud-storage

        Args:
            resource_path:
                GCS url of your dataset, for example: "gs://my_bucket/path/to/my/data"
            project_id:
                GCS project id.
            credentials:
                Content of the credentials JSON file stringified which you
                download from Google Cloud Platform.
            thumbnail_suffix:
                Where to save thumbnails of the images in the dataset, for
                example ".lightly/thumbnails/[filename]_thumb.[extension]".
                Set to None to disable thumbnails and use the full images from the
                datasource instead.
            purpose:
                Datasource purpose, determines if datasource is read only (INPUT)
                or can be written to as well (LIGHTLY, INPUT_OUTPUT).

        """
        # TODO: Use DatasourceConfigGCS once we switch/update the api generator.
        self._datasources_api.update_datasource_by_dataset_id(
            datasource_config=DatasourceConfig.from_dict(
                {
                    "type": "GCS",
                    "fullPath": resource_path,
                    "thumbSuffix": thumbnail_suffix,
                    "gcsProjectId": project_id,
                    "gcsCredentials": credentials,
                    "purpose": purpose,
                }
            ),
            dataset_id=self.dataset_id,
        )

    def set_local_config(
        self,
        relative_path: str = "",
        web_server_location: Optional[str] = "http://localhost:3456",
        thumbnail_suffix: Optional[
            str
        ] = ".lightly/thumbnails/[filename]_thumb.[extension]",
        purpose: str = DatasourcePurpose.INPUT_OUTPUT,
    ) -> None:
        """Sets the local configuration for the datasource of the current dataset.

        Find a detailed explanation on how to setup Lightly with a local file
        server in our docs: https://docs.lightly.ai/getting_started/dataset_creation/dataset_creation_local_server.html

        Args:
            relative_path:
                Relative path from the mount root, for example: "path/to/my/data".
            web_server_location:
                Location of your local file server. Defaults to "http://localhost:3456".
            thumbnail_suffix:
                Where to save thumbnails of the images in the dataset, for
                example ".lightly/thumbnails/[filename]_thumb.[extension]".
                Set to None to disable thumbnails and use the full images from the
                datasource instead.
            purpose:
                Datasource purpose, determines if datasource is read only (INPUT)
                or can be written to as well (LIGHTLY, INPUT_OUTPUT).

        """
        # TODO: Use DatasourceConfigLocal once we switch/update the api generator.
        self._datasources_api.update_datasource_by_dataset_id(
            datasource_config=DatasourceConfig.from_dict(
                {
                    "type": "LOCAL",
                    "webServerLocation": web_server_location,
                    "fullPath": relative_path,
                    "thumbSuffix": thumbnail_suffix,
                    "purpose": purpose,
                }
            ),
            dataset_id=self.dataset_id,
        )

    def set_s3_config(
        self,
        resource_path: str,
        region: str,
        access_key: str,
        secret_access_key: str,
        thumbnail_suffix: Optional[
            str
        ] = ".lightly/thumbnails/[filename]_thumb.[extension]",
        purpose: str = DatasourcePurpose.INPUT_OUTPUT,
    ) -> None:
        """Sets the S3 configuration for the datasource of the current dataset.

        See our docs for a detailed explanation on how to setup Lightly with
        AWS S3: https://docs.lightly.ai/docs/aws-s3

        Args:
            resource_path:
                S3 url of your dataset, for example "s3://my_bucket/path/to/my/data".
            region:
                S3 region where the dataset bucket is located, for example "eu-central-1".
            access_key:
                S3 access key.
            secret_access_key:
                Secret for the S3 access key.
            thumbnail_suffix:
                Where to save thumbnails of the images in the dataset, for
                example ".lightly/thumbnails/[filename]_thumb.[extension]".
                Set to None to disable thumbnails and use the full images from the
                datasource instead.
            purpose:
                Datasource purpose, determines if datasource is read only (INPUT)
                or can be written to as well (LIGHTLY, INPUT_OUTPUT).

        """
        # TODO: Use DatasourceConfigS3 once we switch/update the api generator.
        self._datasources_api.update_datasource_by_dataset_id(
            datasource_config=DatasourceConfig.from_dict(
                {
                    "type": "S3",
                    "fullPath": resource_path,
                    "thumbSuffix": thumbnail_suffix,
                    "s3Region": region,
                    "s3AccessKeyId": access_key,
                    "s3SecretAccessKey": secret_access_key,
                    "purpose": purpose,
                }
            ),
            dataset_id=self.dataset_id,
        )

    def set_s3_delegated_access_config(
        self,
        resource_path: str,
        region: str,
        role_arn: str,
        external_id: str,
        thumbnail_suffix: Optional[
            str
        ] = ".lightly/thumbnails/[filename]_thumb.[extension]",
        purpose: str = DatasourcePurpose.INPUT_OUTPUT,
    ) -> None:
        """Sets the S3 configuration for the datasource of the current dataset.

        See our docs for a detailed explanation on how to setup Lightly with
        AWS S3 and delegated access: https://docs.lightly.ai/docs/aws-s3#delegated-access

        Args:
            resource_path:
                S3 url of your dataset, for example "s3://my_bucket/path/to/my/data".
            region:
                S3 region where the dataset bucket is located, for example "eu-central-1".
            role_arn:
                Unique ARN identifier of the role.
            external_id:
                External ID of the role.
            thumbnail_suffix:
                Where to save thumbnails of the images in the dataset, for
                example ".lightly/thumbnails/[filename]_thumb.[extension]".
                Set to None to disable thumbnails and use the full images from the
                datasource instead.
            purpose:
                Datasource purpose, determines if datasource is read only (INPUT)
                or can be written to as well (LIGHTLY, INPUT_OUTPUT).

        """
        # TODO: Use DatasourceConfigS3 once we switch/update the api generator.
        self._datasources_api.update_datasource_by_dataset_id(
            datasource_config=DatasourceConfig.from_dict(
                {
                    "type": "S3DelegatedAccess",
                    "fullPath": resource_path,
                    "thumbSuffix": thumbnail_suffix,
                    "s3Region": region,
                    "s3ARN": role_arn,
                    "s3ExternalId": external_id,
                    "purpose": purpose,
                }
            ),
            dataset_id=self.dataset_id,
        )

    def set_obs_config(
        self,
        resource_path: str,
        obs_endpoint: str,
        obs_access_key_id: str,
        obs_secret_access_key: str,
        thumbnail_suffix: Optional[
            str
        ] = ".lightly/thumbnails/[filename]_thumb.[extension]",
        purpose: str = DatasourcePurpose.INPUT_OUTPUT,
    ) -> None:
        """Sets the Telekom OBS configuration for the datasource of the current dataset.

        Args:
            resource_path:
                OBS url of your dataset. For example, "obs://my_bucket/path/to/my/data".
            obs_endpoint:
                OBS endpoint.
            obs_access_key_id:
                OBS access key id.
            obs_secret_access_key:
                OBS secret access key.
            thumbnail_suffix:
                Where to save thumbnails of the images in the dataset, for
                example ".lightly/thumbnails/[filename]_thumb.[extension]".
                Set to None to disable thumbnails and use the full images from the
                datasource instead.
            purpose:
                Datasource purpose, determines if datasource is read only (INPUT)
                or can be written to as well (LIGHTLY, INPUT_OUTPUT).

        """
        # TODO: Use DatasourceConfigOBS once we switch/update the api generator.
        self._datasources_api.update_datasource_by_dataset_id(
            datasource_config=DatasourceConfig.from_dict(
                {
                    "type": "OBS",
                    "fullPath": resource_path,
                    "thumbSuffix": thumbnail_suffix,
                    "obsEndpoint": obs_endpoint,
                    "obsAccessKeyId": obs_access_key_id,
                    "obsSecretAccessKey": obs_secret_access_key,
                    "purpose": purpose,
                }
            ),
            dataset_id=self.dataset_id,
        )

    def list_datasource_permissions(
        self,
    ) -> Dict[str, Union[bool, Dict[str, str]]]:
        """Lists granted access permissions for the datasource set up with a dataset.

        Returns a string dictionary, with each permission mapped to a boolean value,
        see the example below. An additional ``errors`` key is present if any permission
        errors have been encountered. Permission errors are stored in a dictionary where
        permission names are keys and error messages are values.

        >>> from lightly.api import ApiWorkflowClient
        >>> client = ApiWorkflowClient(
        ...    token="MY_LIGHTLY_TOKEN", dataset_id="MY_DATASET_ID"
        ... )
        >>> client.list_datasource_permissions()
        {
            'can_read': True,
            'can_write': True,
            'can_list': False,
            'can_overwrite': True,
            'errors': {'can_list': 'error message'}
        }

        """
        return self._datasources_api.verify_datasource_by_dataset_id(
            dataset_id=self.dataset_id,
        ).to_dict()



================================================
FILE: lightly/api/api_workflow_download_dataset.py
================================================
import io
import os
import urllib.request
import warnings
from concurrent.futures.thread import ThreadPoolExecutor
from typing import Dict, List, Optional
from urllib.request import Request

import tqdm
from PIL import Image

from lightly.api import download, utils
from lightly.api.bitmask import BitMask
from lightly.openapi_generated.swagger_client.models import (
    DatasetEmbeddingData,
    ImageType,
)
from lightly.utils.hipify import bcolors


def _make_dir_and_save_image(output_dir: str, filename: str, img: Image):
    """Saves the images and creates necessary subdirectories."""
    path = os.path.join(output_dir, filename)

    head = os.path.split(path)[0]
    if not os.path.exists(head):
        os.makedirs(head)

    img.save(path)
    img.close()


def _get_image_from_read_url(read_url: str):
    """Makes a get request to the signed read url and returns the image."""
    request = Request(read_url, method="GET")
    with urllib.request.urlopen(request) as response:
        blob = response.read()
        img = Image.open(io.BytesIO(blob))
    return img


class _DownloadDatasetMixin:
    def download_dataset(
        self,
        output_dir: str,
        tag_name: str = "initial-tag",
        max_workers: int = 8,
        verbose: bool = True,
    ) -> None:
        """Downloads images from the web-app and stores them in output_dir.

        Args:
            output_dir:
                Where to store the downloaded images.
            tag_name:
                Name of the tag which should be downloaded.
            max_workers:
                Maximum number of workers downloading images in parallel.
            verbose:
                Whether or not to show the progress bar.

        Raises:
            ValueError:
                If the specified tag does not exist on the dataset.
            RuntimeError:
                If the connection to the server failed.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> client.download_dataset("/tmp/data")
            Downloading 3 images (with 3 workers):
            100%|██████████████████████████████████| 3/3 [00:01<00:00,  1.99imgs/s]
        """

        # check if images are available
        dataset = self._datasets_api.get_dataset_by_id(self.dataset_id)
        if dataset.img_type != ImageType.FULL:
            # only thumbnails or metadata available
            raise ValueError(
                f"Dataset with id {self.dataset_id} has no downloadable images!"
            )

        # check if tag exists
        available_tags = self.get_all_tags()
        try:
            tag = next(tag for tag in available_tags if tag.name == tag_name)
        except StopIteration:
            raise ValueError(
                f"Dataset with id {self.dataset_id} has no tag {tag_name}!"
            )

        # get sample ids
        sample_ids = list(
            utils.paginate_endpoint(
                self._mappings_api.get_sample_mappings_by_dataset_id,
                page_size=25000,
                dataset_id=self.dataset_id,
                field="_id",
            )
        )

        indices = BitMask.from_hex(tag.bit_mask_data).to_indices()
        sample_ids = [sample_ids[i] for i in indices]

        filenames_on_server = self.get_filenames()
        filenames = [filenames_on_server[i] for i in indices]

        downloadables = zip(sample_ids, filenames)

        # handle the case where len(sample_ids) < max_workers
        max_workers = min(len(sample_ids), max_workers)
        max_workers = max(max_workers, 1)

        if verbose:
            print(
                f"Downloading {bcolors.OKGREEN}{len(sample_ids)}{bcolors.ENDC} images (with {bcolors.OKGREEN}{max_workers}{bcolors.ENDC} workers):",
                flush=True,
            )
            pbar = tqdm.tqdm(unit="imgs", total=len(sample_ids))
            tqdm_lock = tqdm.tqdm.get_lock()

        # define lambda function for concurrent download
        def lambda_(i):
            sample_id, filename = i
            # try to download image
            try:
                read_url = self._samples_api.get_sample_image_read_url_by_id(
                    dataset_id=self.dataset_id,
                    sample_id=sample_id,
                    type="full",
                )
                img = _get_image_from_read_url(read_url)
                _make_dir_and_save_image(output_dir, filename, img)
                success = True
            except Exception as e:  # pylint: disable=broad-except
                warnings.warn(f"Downloading of image {filename} failed with error {e}")
                success = False

            # update the progress bar
            if verbose:
                tqdm_lock.acquire()
                pbar.update(1)
                tqdm_lock.release()
            # return whether the download was successful
            return success

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = list(executor.map(lambda_, downloadables, chunksize=1))

        if not all(results):
            msg = "Warning: Unsuccessful download! "
            msg += "Failed at image: {}".format(results.index(False))
            warnings.warn(msg)

    def get_all_embedding_data(self) -> List[DatasetEmbeddingData]:
        """Fetches embedding data of all embeddings for this dataset.

        Returns:
            A list of embedding data.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> client.get_all_embedding_data()
            [{'created_at': 1684750552181,
             'id': '646b40d88355e2f54c6d2235',
             'is2d': False,
             'is_processed': True,
             'name': 'default_20230522_10h15m50s'}]
        """
        return self._embeddings_api.get_embeddings_by_dataset_id(
            dataset_id=self.dataset_id
        )

    def get_embedding_data_by_name(self, name: str) -> DatasetEmbeddingData:
        """Fetches embedding data with the given name for this dataset.

        Args:
            name: Embedding name.

        Returns:
            Embedding data.

        Raises:
            ValueError:
                If no embedding with this name exists.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> client.get_embedding_data_by_name("embedding-data")
            [{'created_at': 1654756552401,
             'id': '646f346004d77b4e1424e67e',
             'is2d': False,
             'is_processed': True,
             'name': 'embedding-data'}]
        """
        for embedding_data in self.get_all_embedding_data():
            if embedding_data.name == name:
                return embedding_data
        raise ValueError(
            f"There are no embeddings with name '{name}' for dataset with id "
            f"'{self.dataset_id}'."
        )

    def download_embeddings_csv_by_id(
        self,
        embedding_id: str,
        output_path: str,
    ) -> None:
        """Downloads embeddings with the given embedding id from the dataset.

        Args:
            embedding_id: ID of the embedding data to be downloaded.
            output_path: Where the downloaded embedding data should be stored.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> client.download_embeddings_csv_by_id(
            ...     embedding_id="646f346004d77b4e1424e67e",
            ...     output_path="/tmp/embeddings.csv"
            ... )
            >>>
            >>> # File content:
            >>> # filenames,embedding_0,embedding_1,embedding_...,labels
            >>> # image-1.png,0.2124302,-0.26934767,...,0
        """
        read_url = self._embeddings_api.get_embeddings_csv_read_url_by_id(
            dataset_id=self.dataset_id, embedding_id=embedding_id
        )
        download.download_and_write_file(url=read_url, output_path=output_path)

    def download_embeddings_csv(self, output_path: str) -> None:
        """Downloads the latest embeddings from the dataset.

        Args:
            output_path: Where the downloaded embedding data should be stored.

        Raises:
            RuntimeError:
                If no embeddings could be found for the dataset.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> client.download_embeddings_csv(output_path="/tmp/embeddings.csv")
            >>>
            >>> # File content:
            >>> # filenames,embedding_0,embedding_1,embedding_...,labels
            >>> # image-1.png,0.2124302,-0.26934767,...,0
        """
        last_embedding = _get_latest_default_embedding_data(
            embeddings=self.get_all_embedding_data()
        )
        if last_embedding is None:
            raise RuntimeError(
                f"Could not find embeddings for dataset with id '{self.dataset_id}'."
            )
        self.download_embeddings_csv_by_id(
            embedding_id=last_embedding.id,
            output_path=output_path,
        )

    def export_label_studio_tasks_by_tag_id(
        self,
        tag_id: str,
    ) -> List[Dict]:
        """Exports samples in a format compatible with Label Studio.

        The format is documented here:
        https://labelstud.io/guide/tasks.html#Basic-Label-Studio-JSON-format

        Args:
            tag_id:
                Id of the tag which should exported.

        Returns:
            A list of dictionaries in a format compatible with Label Studio.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> client.export_label_studio_tasks_by_tag_id(tag_id="646f34608a5613b57d8b73cc")
            [{'id': 0, 'data': {'image': '...', ...}}]
        """
        label_studio_tasks = list(
            utils.paginate_endpoint(
                self._tags_api.export_tag_to_label_studio_tasks,
                page_size=2500,
                dataset_id=self.dataset_id,
                tag_id=tag_id,
            )
        )
        return label_studio_tasks


def _get_latest_default_embedding_data(
    embeddings: List[DatasetEmbeddingData],
) -> Optional[DatasetEmbeddingData]:
    """Returns the latest embedding data with a default name or None if no such
    default embedding exists.
    """
    default_embeddings = [e for e in embeddings if e.name.startswith("default")]
    if default_embeddings:
        last_embedding = sorted(default_embeddings, key=lambda e: e.created_at)[-1]
        return last_embedding
    else:
        return None



================================================
FILE: lightly/api/api_workflow_export.py
================================================
import warnings
from typing import Dict, List

from lightly.api import utils
from lightly.openapi_generated.swagger_client.models import (
    FileNameFormat,
    LabelBoxDataRow,
    LabelBoxV4DataRow,
    LabelStudioTask,
)


class _ExportDatasetMixin:
    def export_label_studio_tasks_by_tag_id(
        self,
        tag_id: str,
    ) -> List[Dict]:
        """Fetches samples in a format compatible with Label Studio.

        The format is documented here:
        https://labelstud.io/guide/tasks.html#Basic-Label-Studio-JSON-format

        More information:
        https://docs.lightly.ai/docs/labelstudio-integration

        Args:
            tag_id:
                ID of the tag which should exported.

        Returns:
            A list of dictionaries in a format compatible with Label Studio.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> client.export_label_studio_tasks_by_tag_id(tag_id="646f34608a5613b57d8b73cc")
            [{'id': 0, 'data': {'image': '...', ...}}]
        """
        label_studio_tasks: List[LabelStudioTask] = list(
            utils.paginate_endpoint(
                self._tags_api.export_tag_to_label_studio_tasks,
                page_size=2500,
                dataset_id=self.dataset_id,
                tag_id=tag_id,
            )
        )
        return [task.to_dict(by_alias=True) for task in label_studio_tasks]

    def export_label_studio_tasks_by_tag_name(
        self,
        tag_name: str,
    ) -> List[Dict]:
        """Fetches samples in a format compatible with Label Studio.

        The format is documented here:
        https://labelstud.io/guide/tasks.html#Basic-Label-Studio-JSON-format

        More information:
        https://docs.lightly.ai/docs/labelstudio-integration

        Args:
            tag_name:
                Name of the tag which should exported.

        Returns:
            A list of dictionaries in a format compatible with Label Studio.

        Examples:
            >>> # write json file which can be imported in Label Studio
            >>> tasks = client.export_label_studio_tasks_by_tag_name(
            >>>     'initial-tag'
            >>> )
            >>>
            >>> with open('my-label-studio-tasks.json', 'w') as f:
            >>>     json.dump(tasks, f)

        """
        tag = self.get_tag_by_name(tag_name)
        return self.export_label_studio_tasks_by_tag_id(tag.id)

    def export_label_box_data_rows_by_tag_id(
        self,
        tag_id: str,
    ) -> List[Dict]:
        """Fetches samples in a format compatible with Labelbox v3.

        The format is documented here: https://docs.labelbox.com/docs/images-json

        More information:
        https://docs.lightly.ai/docs/labelbox

        Args:
            tag_id:
                ID of the tag which should exported.

        Returns:
            A list of dictionaries in a format compatible with Labelbox v3.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> client.export_label_box_data_rows_by_tag_id(tag_id="646f34608a5613b57d8b73cc")
            [{'externalId': '2218961434_7916358f53_z.jpg', 'imageUrl': ...}]
        """
        warnings.warn(
            DeprecationWarning(
                "This method exports data in the deprecated Labelbox v3 format and "
                "will be removed in the future. Use export_label_box_v4_data_rows_by_tag_id "
                "to export data in the Labelbox v4 format instead."
            )
        )
        label_box_data_rows: List[LabelBoxDataRow] = list(
            utils.paginate_endpoint(
                self._tags_api.export_tag_to_label_box_data_rows,
                page_size=2500,
                dataset_id=self.dataset_id,
                tag_id=tag_id,
            )
        )
        return [row.to_dict(by_alias=True) for row in label_box_data_rows]

    def export_label_box_data_rows_by_tag_name(
        self,
        tag_name: str,
    ) -> List[Dict]:
        """Fetches samples in a format compatible with Labelbox v3.

        The format is documented here: https://docs.labelbox.com/docs/images-json

        More information:
        https://docs.lightly.ai/docs/labelbox

        Args:
            tag_name:
                Name of the tag which should exported.

        Returns:
            A list of dictionaries in a format compatible with Labelbox v3.

        Examples:
            >>> # write json file which can be imported in Label Studio
            >>> tasks = client.export_label_box_data_rows_by_tag_name(
            >>>     'initial-tag'
            >>> )
            >>>
            >>> with open('my-labelbox-rows.json', 'w') as f:
            >>>     json.dump(tasks, f)

        """
        warnings.warn(
            DeprecationWarning(
                "This method exports data in the deprecated Labelbox v3 format and "
                "will be removed in the future. Use export_label_box_v4_data_rows_by_tag_name "
                "to export data in the Labelbox v4 format instead."
            )
        )
        tag = self.get_tag_by_name(tag_name)
        return self.export_label_box_data_rows_by_tag_id(tag.id)

    def export_label_box_v4_data_rows_by_tag_id(
        self,
        tag_id: str,
    ) -> List[Dict]:
        """Fetches samples in a format compatible with Labelbox v4.

        The format is documented here: https://docs.labelbox.com/docs/images-json

        More information:
        https://docs.lightly.ai/docs/labelbox

        Args:
            tag_id:
                ID of the tag which should exported.
        Returns:
            A list of dictionaries in a format compatible with Labelbox v4.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> client.export_label_box_v4_data_rows_by_tag_id(tag_id="646f34608a5613b57d8b73cc")
            [{'row_data': '...', 'global_key': 'image-1.jpg', 'media_type': 'IMAGE'}
        """
        label_box_data_rows: List[LabelBoxV4DataRow] = list(
            utils.paginate_endpoint(
                self._tags_api.export_tag_to_label_box_v4_data_rows,
                page_size=2500,
                dataset_id=self.dataset_id,
                tag_id=tag_id,
            )
        )
        return [row.to_dict() for row in label_box_data_rows]

    def export_label_box_v4_data_rows_by_tag_name(
        self,
        tag_name: str,
    ) -> List[Dict]:
        """Fetches samples in a format compatible with Labelbox.

        The format is documented here: https://docs.labelbox.com/docs/images-json

        More information:
        https://docs.lightly.ai/docs/labelbox

        Args:
            tag_name:
                Name of the tag which should exported.
        Returns:
            A list of dictionaries in a format compatible with Labelbox.
        Examples:
            >>> # write json file which can be imported in Label Studio
            >>> tasks = client.export_label_box_v4_data_rows_by_tag_name(
            >>>     'initial-tag'
            >>> )
            >>>
            >>> with open('my-labelbox-rows.json', 'w') as f:
            >>>     json.dump(tasks, f)
        """
        tag = self.get_tag_by_name(tag_name)
        return self.export_label_box_v4_data_rows_by_tag_id(tag.id)

    def export_filenames_by_tag_id(
        self,
        tag_id: str,
    ) -> str:
        """Fetches samples filenames within a certain tag by tag ID.

        More information:
        https://docs.lightly.ai/docs/filenames-and-readurls

        Args:
            tag_id:
                ID of the tag which should exported.

        Returns:
            A list of filenames of samples within a certain tag.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> client.export_filenames_by_tag_id("646b40d6c06aae1b91294a9e")
            'image-1.jpg\nimage-2.jpg\nimage-3.jpg'
        """
        filenames = "\n".join(
            utils.paginate_endpoint(
                self._tags_api.export_tag_to_basic_filenames,
                dataset_id=self.dataset_id,
                tag_id=tag_id,
            )
        )
        return filenames

    def export_filenames_by_tag_name(
        self,
        tag_name: str,
    ) -> str:
        """Fetches samples filenames within a certain tag by tag name.

        More information:
        https://docs.lightly.ai/docs/filenames-and-readurls

        Args:
            tag_name:
                Name of the tag which should exported.

        Returns:
            A list of filenames of samples within a certain tag.

        Examples:
            >>> # write json file which can be imported in Label Studio
            >>> filenames = client.export_filenames_by_tag_name(
            >>>     'initial-tag'
            >>> )
            >>>
            >>> with open('filenames-of-initial-tag.txt', 'w') as f:
            >>>     f.write(filenames)

        """
        tag = self.get_tag_by_name(tag_name)
        return self.export_filenames_by_tag_id(tag.id)

    def export_filenames_and_read_urls_by_tag_id(
        self,
        tag_id: str,
    ) -> List[Dict[str, str]]:
        """Fetches filenames, read URLs, and datasource URLs from the given tag.

        More information:
        https://docs.lightly.ai/docs/filenames-and-readurls

        Args:
            tag_id:
                ID of the tag which should exported.

        Returns:
            A list of dictionaries with the keys "filename", "readUrl" and "datasourceUrl".
            An example:
            [
                {
                    "fileName": "sample1.jpg",
                    "readUrl": "s3://my_datasource/sample1.jpg?read_url_key=EAIFUIENDLFN",
                    "datasourceUrl": "s3://my_datasource/sample1.jpg",
                },
                {
                    "fileName": "sample2.jpg",
                    "readUrl": "s3://my_datasource/sample2.jpg?read_url_key=JSBFIEUHVSJ",
                    "datasourceUrl": "s3://my_datasource/sample2.jpg",
                },
            ]

        """
        filenames_string = "\n".join(
            utils.paginate_endpoint(
                self._tags_api.export_tag_to_basic_filenames,
                dataset_id=self.dataset_id,
                tag_id=tag_id,
                file_name_format=FileNameFormat.NAME,
            )
        )
        read_urls_string = "\n".join(
            utils.paginate_endpoint(
                self._tags_api.export_tag_to_basic_filenames,
                dataset_id=self.dataset_id,
                tag_id=tag_id,
                file_name_format=FileNameFormat.REDIRECTED_READ_URL,
            )
        )
        datasource_urls_string = "\n".join(
            utils.paginate_endpoint(
                self._tags_api.export_tag_to_basic_filenames,
                dataset_id=self.dataset_id,
                tag_id=tag_id,
                file_name_format=FileNameFormat.DATASOURCE_FULL,
            )
        )
        # The endpoint exportTagToBasicFilenames returns a plain string so we
        # have to split it by newlines in order to get the individual entries.
        # The order of the fileNames and readUrls and datasourceUrls is guaranteed to be the same
        # by the API so we can simply zip them.
        filenames = filenames_string.split("\n")
        read_urls = read_urls_string.split("\n")
        datasource_urls = datasource_urls_string.split("\n")
        return [
            {
                "fileName": filename,
                "readUrl": read_url,
                "datasourceUrl": datasource_url,
            }
            for filename, read_url, datasource_url in zip(
                filenames, read_urls, datasource_urls
            )
        ]

    def export_filenames_and_read_urls_by_tag_name(
        self,
        tag_name: str,
    ) -> List[Dict[str, str]]:
        """Fetches filenames, read URLs, and datasource URLs from the given tag name.

        More information:
        https://docs.lightly.ai/docs/filenames-and-readurls

        Args:
            tag_name:
                Name of the tag which should exported.

        Returns:
            A list of dictionaries with keys "filename", "readUrl" and "datasourceUrl".

        Examples:
            >>> # write json file which can be used to access the actual file contents.
            >>> mappings = client.export_filenames_and_read_urls_by_tag_name(
            >>>     'initial-tag'
            >>> )
            >>>
            >>> with open('my-samples.json', 'w') as f:
            >>>     json.dump(mappings, f)

        """
        tag = self.get_tag_by_name(tag_name)
        return self.export_filenames_and_read_urls_by_tag_id(tag.id)



================================================
FILE: lightly/api/api_workflow_predictions.py
================================================
from typing import Sequence

from lightly.openapi_generated.swagger_client.models import (
    PredictionSingleton,
    PredictionTaskSchema,
)


class _PredictionsMixin:
    def create_or_update_prediction_task_schema(
        self,
        schema: PredictionTaskSchema,
        prediction_version_id: int = -1,
    ) -> None:
        """Creates or updates the prediction task schema.

        Args:
            schema:
                The prediction task schema.
            prediction_version_id:
                A numerical ID (e.g., timestamp) to distinguish different predictions of different model versions.
                Use the same ID if you don't require versioning or if you wish to overwrite the previous schema.

        Example:
          >>> import time
          >>> from lightly.api import ApiWorkflowClient
          >>> from lightly.openapi_generated.swagger_client.models import (
          >>>     PredictionTaskSchema,
          >>>     TaskType,
          >>>     PredictionTaskSchemaCategory,
          >>> )
          >>>
          >>> client = ApiWorkflowClient(
          >>>     token="MY_LIGHTLY_TOKEN", dataset_id="MY_DATASET_ID"
          >>> )
          >>>
          >>> schema = PredictionTaskSchema(
          >>>     name="my-object-detection",
          >>>     type=TaskType.OBJECT_DETECTION,
          >>>     categories=[
          >>>         PredictionTaskSchemaCategory(id=0, name="dog"),
          >>>         PredictionTaskSchemaCategory(id=1, name="cat"),
          >>>     ],
          >>> )
          >>> client.create_or_update_prediction_task_schema(schema=schema)

        :meta private:  # Skip docstring generation
        """
        self._predictions_api.create_or_update_prediction_task_schema_by_dataset_id(
            prediction_task_schema=schema,
            dataset_id=self.dataset_id,
            prediction_uuid_timestamp=prediction_version_id,
        )

    def create_or_update_prediction(
        self,
        sample_id: str,
        prediction_singletons: Sequence[PredictionSingleton],
        prediction_version_id: int = -1,
    ) -> None:
        """Creates or updates predictions for one specific sample.

        Args:
            sample_id
                The ID of the sample.

            prediction_version_id:
                A numerical ID (e.g., timestamp) to distinguish different predictions of different model versions.
                Use the same id if you don't require versioning or if you wish to overwrite the previous schema.
                This ID must match the ID of a prediction task schema.

            prediction_singletons:
                Predictions to be uploaded for the designated sample.

        :meta private:  # Skip docstring generation
        """
        self._predictions_api.create_or_update_prediction_by_sample_id(
            prediction_singleton=prediction_singletons,
            dataset_id=self.dataset_id,
            sample_id=sample_id,
            prediction_uuid_timestamp=prediction_version_id,
        )



================================================
FILE: lightly/api/api_workflow_selection.py
================================================
from __future__ import annotations

import time
import warnings
from typing import TYPE_CHECKING, Dict, List, Optional, Union

import numpy as np

from lightly.active_learning.config.selection_config import SelectionConfig
from lightly.openapi_generated.swagger_client.models import (
    ActiveLearningScoreCreateRequest,
    JobState,
    JobStatusData,
    SamplingConfig,
    SamplingConfigStoppingCondition,
    SamplingCreateRequest,
    TagData,
)

if TYPE_CHECKING:
    from numpy.typing import NDArray


def _parse_active_learning_scores(scores: Union[np.ndarray, List]):
    """Makes list/np.array of active learning scores serializable."""
    # the api only accepts float64s
    if isinstance(scores, np.ndarray):
        scores = scores.astype(np.float64)

    # convert to list and return
    return list(scores)


class _SelectionMixin:
    def upload_scores(
        self, al_scores: Dict[str, NDArray[np.float64]], query_tag_id: str
    ) -> None:
        """Uploads active learning scores for a tag.

        Args:
            al_scores:
                Active learning scores. Must be a mapping between score names
                and score arrays. The length of each score array must match samples
                in the designated tag.
            query_tag_id: ID of the desired tag.

        :meta private:  # Skip docstring generation
        """
        # iterate over all available score types and upload them
        for score_type, score_values in al_scores.items():
            body = ActiveLearningScoreCreateRequest(
                score_type=score_type,
                scores=_parse_active_learning_scores(score_values),
            )
            self._scores_api.create_or_update_active_learning_score_by_tag_id(
                active_learning_score_create_request=body,
                dataset_id=self.dataset_id,
                tag_id=query_tag_id,
            )

    def selection(
        self,
        selection_config: SelectionConfig,
        preselected_tag_id: Optional[str] = None,
        query_tag_id: Optional[str] = None,
    ) -> TagData:
        """Performs a selection given the arguments.

        Args:
            selection_config:
                The configuration of the selection.
            preselected_tag_id:
                The tag defining the already chosen samples (e.g., already
                labelled ones). Optional.
            query_tag_id:
                ID of the tag where samples should be fetched. None resolves to
                `initial-tag`. Defaults to None.

        Returns:
            The newly created tag of the selection.

        Raises:
            RuntimeError:
                When a tag with the tag name specified in the selection config already exists.
                When `initial-tag` does not exist in the dataset.
                When the selection task fails.

        :meta private:  # Skip docstring generation
        """

        warnings.warn(
            DeprecationWarning(
                "ApiWorkflowClient.selection() is deprecated "
                "and will be removed in the future."
            ),
        )

        # make sure the tag name does not exist yet
        tags = self.get_all_tags()
        if selection_config.name in [tag.name for tag in tags]:
            raise RuntimeError(
                f"There already exists a tag with tag_name {selection_config.name}."
            )
        if len(tags) == 0:
            raise RuntimeError("There exists no initial-tag for this dataset.")

        # make sure we have an embedding id
        try:
            self.embedding_id
        except AttributeError:
            self.set_embedding_id_to_latest()

        # trigger the selection
        payload = self._create_selection_create_request(
            selection_config, preselected_tag_id, query_tag_id
        )
        payload.row_count = self.get_all_tags()[0].tot_size
        response = self._selection_api.trigger_sampling_by_id(
            sampling_create_request=payload,
            dataset_id=self.dataset_id,
            embedding_id=self.embedding_id,
        )
        job_id = response.job_id

        # poll the job status till the job is not running anymore
        exception_counter = 0  # TODO; remove after solving https://github.com/lightly-ai/lightly-core/issues/156
        job_status_data = None

        wait_time_till_next_poll = getattr(self, "wait_time_till_next_poll", 1)
        while (
            job_status_data is None
            or job_status_data.status == JobState.RUNNING
            or job_status_data.status == JobState.WAITING
            or job_status_data.status == JobState.UNKNOWN
        ):
            # sleep before polling again
            time.sleep(wait_time_till_next_poll)
            # try to read the sleep time until the next poll from the status data
            try:
                job_status_data: JobStatusData = self._jobs_api.get_job_status_by_id(
                    job_id=job_id
                )
                wait_time_till_next_poll = job_status_data.wait_time_till_next_poll
            except Exception as err:
                exception_counter += 1
                if exception_counter == 20:
                    print(
                        f"Selection job with job_id {job_id} could not be started because of error: {err}"
                    )
                    raise err

        if job_status_data.status == JobState.FAILED:
            raise RuntimeError(
                f"Selection job with job_id {job_id} failed with error {job_status_data.error}"
            )

        # get the new tag from the job status
        new_tag_id = job_status_data.result.data
        if new_tag_id is None:
            raise RuntimeError(f"TagId returned by job with job_id {job_id} is None.")
        new_tag_data = self._tags_api.get_tag_by_tag_id(
            dataset_id=self.dataset_id, tag_id=new_tag_id
        )

        return new_tag_data

    def _create_selection_create_request(
        self,
        selection_config: SelectionConfig,
        preselected_tag_id: Optional[str],
        query_tag_id: Optional[str],
    ) -> SamplingCreateRequest:
        """Creates a SamplingCreateRequest

        First, it checks how many samples are already labeled by
            getting the number of samples in the preselected_tag_id.
        Then the stopping_condition.n_samples
            is set to be the number of already labeled samples + the selection_config.batch_size.
        Last the SamplingCreateRequest is created with the necessary nested class instances.

        """

        sampling_config = SamplingConfig(
            stopping_condition=SamplingConfigStoppingCondition(
                n_samples=selection_config.n_samples,
                min_distance=selection_config.min_distance,
            )
        )
        sampling_create_request = SamplingCreateRequest(
            new_tag_name=selection_config.name,
            method=selection_config.method,
            config=sampling_config,
            preselected_tag_id=preselected_tag_id,
            query_tag_id=query_tag_id,
        )
        return sampling_create_request



================================================
FILE: lightly/api/api_workflow_tags.py
================================================
from typing import *

from lightly.api.bitmask import BitMask
from lightly.openapi_generated.swagger_client.models import (
    TagArithmeticsOperation,
    TagArithmeticsRequest,
    TagBitMaskResponse,
    TagCreateRequest,
    TagData,
)


class TagDoesNotExistError(ValueError):
    pass


class _TagsMixin:
    def get_all_tags(self) -> List[TagData]:
        """Gets all tags in the Lightly Platform from the current dataset.

        Returns:
            A list of tags.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> client.get_all_tags()
            [{'created_at': 1684750550014,
             'dataset_id': '646b40a18355e2f54c6d2200',
             'id': '646b40d6c06aae1b91294a9e',
             'last_modified_at': 1684750550014,
             'name': 'cool-tag',
             'preselected_tag_id': None,
             ...}]
        """
        return self._tags_api.get_tags_by_dataset_id(self.dataset_id)

    def get_tag_by_id(self, tag_id: str) -> TagData:
        """Gets a tag from the current dataset by tag ID.

        Args:
            tag_id:
                ID of the requested tag.

        Returns:
            Tag data for the requested tag.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> client.get_tag_by_id("646b40d6c06aae1b91294a9e")
            {'created_at': 1684750550014,
             'dataset_id': '646b40a18355e2f54c6d2200',
             'id': '646b40d6c06aae1b91294a9e',
             'last_modified_at': 1684750550014,
             'name': 'cool-tag',
             'preselected_tag_id': None,
             ...}
        """
        tag_data = self._tags_api.get_tag_by_tag_id(
            dataset_id=self.dataset_id, tag_id=tag_id
        )
        return tag_data

    def get_tag_by_name(self, tag_name: str) -> TagData:
        """Gets a tag from the current dataset by tag name.

        Args:
            tag_name:
                Name of the requested tag.

        Returns:
            Tag data for the requested tag.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> client.get_tag_by_name("cool-tag")
            {'created_at': 1684750550014,
             'dataset_id': '646b40a18355e2f54c6d2200',
             'id': '646b40d6c06aae1b91294a9e',
             'last_modified_at': 1684750550014,
             'name': 'cool-tag',
             'preselected_tag_id': None,
             ...}
        """
        tag_name_id_dict = {tag.name: tag.id for tag in self.get_all_tags()}
        tag_id = tag_name_id_dict.get(tag_name, None)
        if tag_id is None:
            raise TagDoesNotExistError(f"Your tag_name does not exist: {tag_name}.")
        return self.get_tag_by_id(tag_id)

    def get_filenames_in_tag(
        self,
        tag_data: TagData,
        filenames_on_server: List[str] = None,
        exclude_parent_tag: bool = False,
    ) -> List[str]:
        """Gets the filenames of samples under a tag.

        Args:
            tag_data:
                Information about the tag.
            filenames_on_server:
                List of all filenames on the server. If they are not given,
                they need to be downloaded, which is a time-consuming operation.
            exclude_parent_tag:
                Excludes the parent tag in the returned filenames.

        Returns:
            Filenames of all samples under the tag.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> tag = client.get_tag_by_name("cool-tag")
            >>> client.get_filenames_in_tag(tag_data=tag)
            ['image-1.png', 'image-2.png']

        :meta private:  # Skip docstring generation
        """

        if exclude_parent_tag:
            parent_tag_id = tag_data.prev_tag_id
            tag_arithmetics_request = TagArithmeticsRequest(
                tag_id1=tag_data.id,
                tag_id2=parent_tag_id,
                operation=TagArithmeticsOperation.DIFFERENCE,
            )
            bit_mask_response: TagBitMaskResponse = (
                self._tags_api.perform_tag_arithmetics_bitmask(
                    tag_arithmetics_request=tag_arithmetics_request,
                    dataset_id=self.dataset_id,
                )
            )
            bit_mask_data = bit_mask_response.bit_mask_data
        else:
            bit_mask_data = tag_data.bit_mask_data

        if not filenames_on_server:
            filenames_on_server = self.get_filenames()

        filenames_tag = BitMask.from_hex(bit_mask_data).masked_select_from_list(
            filenames_on_server
        )

        return filenames_tag

    def create_tag_from_filenames(
        self, fnames_new_tag: List[str], new_tag_name: str, parent_tag_id: str = None
    ) -> TagData:
        """Creates a new tag from a list of filenames.

        Args:
            fnames_new_tag:
                A list of filenames to be included in the new tag.
            new_tag_name:
                The name of the new tag.
            parent_tag_id:
                The tag defining where to sample from, default: None resolves to the initial-tag.

        Returns:
            The newly created tag.

        Raises:
            RuntimeError:
                When a tag with the desired tag name already exists.
                When `initial-tag` does not exist.
                When any of the given files does not exist.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> filenames = ['image-1.png', 'image-2.png']
            >>> client.create_tag_from_filenames(fnames_new_tag=filenames, new_tag_name='new-tag')
            {'id': '6470c4c1060894655c5a8ed5'}
        """

        # make sure the tag name does not exist yet
        tags = self.get_all_tags()
        if new_tag_name in [tag.name for tag in tags]:
            raise RuntimeError(
                f"There already exists a tag with tag_name {new_tag_name}."
            )
        if len(tags) == 0:
            raise RuntimeError("There exists no initial-tag for this dataset.")

        # fallback to initial tag if no parent tag is provided
        if parent_tag_id is None:
            parent_tag_id = next(tag.id for tag in tags if tag.name == "initial-tag")

        # get list of filenames from tag
        fnames_server = self.get_filenames()
        tot_size = len(fnames_server)

        # create new bitmask for the new tag
        bitmask = BitMask(0)
        fnames_new_tag = set(fnames_new_tag)
        for i, fname in enumerate(fnames_server):
            if fname in fnames_new_tag:
                bitmask.set_kth_bit(i)

        # quick sanity check
        num_selected_samples = len(bitmask.to_indices())
        if num_selected_samples != len(fnames_new_tag):
            raise RuntimeError(
                "An error occurred when creating the new subset! "
                f"Out of the {len(fnames_new_tag)} filenames you provided "
                f"to create a new tag, only {num_selected_samples} have been "
                "found on the server. "
                "Make sure you use the correct filenames. "
                f"Valid filename example from the dataset: {fnames_server[0]}"
            )

        # create new tag
        tag_data_dict = {
            "name": new_tag_name,
            "prevTagId": parent_tag_id,
            "bitMaskData": bitmask.to_hex(),
            "totSize": tot_size,
            "creator": self._creator,
        }

        new_tag = self._tags_api.create_tag_by_dataset_id(
            tag_create_request=TagCreateRequest.from_dict(tag_data_dict),
            dataset_id=self.dataset_id,
        )

        return new_tag

    def delete_tag_by_id(self, tag_id: str) -> None:
        """Deletes a tag from the current dataset.

        Args:
            tag_id:
                The id of the tag to be deleted.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> filenames = ['image-1.png', 'image-2.png']
            >>> tag_id = client.create_tag_from_filenames(fnames_new_tag=filenames, new_tag_name='new-tag')["id"]
            >>> client.delete_tag_by_id(tag_id=tag_id)
        """
        self._tags_api.delete_tag_by_tag_id(dataset_id=self.dataset_id, tag_id=tag_id)

    def delete_tag_by_name(self, tag_name: str) -> None:
        """Deletes a tag from the current dataset.

        Args:
            tag_name:
                The name of the tag to be deleted.

        Examples:
            >>> client = ApiWorkflowClient(token="MY_AWESOME_TOKEN")
            >>>
            >>> # Already created some Lightly Worker runs with this dataset
            >>> client.set_dataset_id_by_name("my-dataset")
            >>> filenames = ['image-1.png', 'image-2.png']
            >>> client.create_tag_from_filenames(fnames_new_tag=filenames, new_tag_name='new-tag')
            >>> client.delete_tag_by_name(tag_name="new-tag")
        """
        tag_data = self.get_tag_by_name(tag_name=tag_name)
        self.delete_tag_by_id(tag_data.id)



================================================
FILE: lightly/api/api_workflow_upload_embeddings.py
================================================
import csv
import io
import tempfile
import urllib.request
from datetime import datetime
from typing import List
from urllib.request import Request

from lightly.api import retry_utils
from lightly.openapi_generated.swagger_client.models import (
    DatasetEmbeddingData,
    DimensionalityReductionMethod,
    Trigger2dEmbeddingJobRequest,
    WriteCSVUrlData,
)
from lightly.utils import io as io_utils


class EmbeddingDoesNotExistError(ValueError):
    pass


class _UploadEmbeddingsMixin:
    def _get_csv_reader_from_read_url(self, read_url: str) -> None:
        """Makes a get request to the signed read url and returns the .csv file."""
        request = Request(read_url, method="GET")
        with urllib.request.urlopen(request) as response:
            buffer = io.StringIO(response.read().decode("utf-8"))
            reader = csv.reader(buffer)

        return reader

    def set_embedding_id_to_latest(self) -> None:
        """Sets the embedding ID in the API client to the latest embedding ID in the current dataset.

        :meta private:  # Skip docstring generation
        """
        embeddings_on_server: List[
            DatasetEmbeddingData
        ] = self._embeddings_api.get_embeddings_by_dataset_id(
            dataset_id=self.dataset_id
        )
        if len(embeddings_on_server) == 0:
            raise RuntimeError(
                f"There are no known embeddings for dataset_id {self.dataset_id}."
            )
        # return first entry as the API returns newest first
        self.embedding_id = embeddings_on_server[0].id

    def get_embedding_by_name(
        self, name: str, ignore_suffix: bool = True
    ) -> DatasetEmbeddingData:
        """Fetches an embedding in the current dataset by name.

        Args:
            name:
                The name of the desired embedding.
            ignore_suffix:
                If true, a suffix of the embedding name in the current dataset
                is ignored.

        Returns:
            The embedding data.

        Raises:
            EmbeddingDoesNotExistError:
                If the name does not match the name of an embedding
                on the server.

        """
        embeddings_on_server: List[
            DatasetEmbeddingData
        ] = self._embeddings_api.get_embeddings_by_dataset_id(
            dataset_id=self.dataset_id
        )
        try:
            if ignore_suffix:
                embedding = next(
                    embedding
                    for embedding in embeddings_on_server
                    if embedding.name.startswith(name)
                )
            else:
                embedding = next(
                    embedding
                    for embedding in embeddings_on_server
                    if embedding.name == name
                )
        except StopIteration:
            raise EmbeddingDoesNotExistError(
                f"Embedding with the specified name "
                f"does not exist on the server: {name}"
            )
        return embedding

    def upload_embeddings(self, path_to_embeddings_csv: str, name: str) -> None:
        """Uploads embeddings to the Lightly Platform.

        First checks that the specified embedding name is not on the server. If it is, the upload is aborted.
        Then creates a new csv file with the embeddings in the order specified on the server. Next uploads it
        to the Lightly Platform. The received embedding ID is stored in the API client.

        Args:
            path_to_embeddings_csv:
                The path to the .csv containing the embeddings, e.g. "path/to/embeddings.csv"
            name:
                The name of the embedding. If an embedding with such a name already exists on the server,
                the upload is aborted.

        :meta private:  # Skip docstring generation
        """
        io_utils.check_embeddings(
            path_to_embeddings_csv, remove_additional_columns=True
        )

        # Try to append the embeddings on the server, if they exist
        try:
            embedding = self.get_embedding_by_name(name, ignore_suffix=True)
            # -> append rows from server
            print("Appending embeddings from server.")
            self.append_embeddings(path_to_embeddings_csv, embedding.id)
            now = datetime.now().strftime("%Y%m%d_%Hh%Mm%Ss")
            name = f"{name}_{now}"
        except EmbeddingDoesNotExistError:
            pass

        # create a new csv with the filenames in the desired order
        rows_csv = self._order_csv_by_filenames(
            path_to_embeddings_csv=path_to_embeddings_csv
        )

        # get the URL to upload the csv to
        response: WriteCSVUrlData = (
            self._embeddings_api.get_embeddings_csv_write_url_by_id(
                self.dataset_id, name=name
            )
        )
        self.embedding_id = response.embedding_id
        signed_write_url = response.signed_write_url

        # save the csv rows in a temporary in-memory string file
        # using a csv writer and then read them as bytes
        with tempfile.SpooledTemporaryFile(mode="rw") as f:
            writer = csv.writer(f)
            writer.writerows(rows_csv)
            f.seek(0)
            embeddings_csv_as_bytes = f.read().encode("utf-8")

        # write the bytes to a temporary in-memory byte file
        with tempfile.SpooledTemporaryFile(mode="r+b") as f_bytes:
            f_bytes.write(embeddings_csv_as_bytes)
            f_bytes.seek(0)
            retry_utils.retry(
                self.upload_file_with_signed_url,
                file=f_bytes,
                signed_write_url=signed_write_url,
            )

        # trigger the 2d embeddings job
        for dimensionality_reduction_method in [
            DimensionalityReductionMethod.PCA,
            DimensionalityReductionMethod.TSNE,
            DimensionalityReductionMethod.UMAP,
        ]:
            body = Trigger2dEmbeddingJobRequest(
                dimensionality_reduction_method=dimensionality_reduction_method
            )
            self._embeddings_api.trigger2d_embeddings_job(
                trigger2d_embedding_job_request=body,
                dataset_id=self.dataset_id,
                embedding_id=self.embedding_id,
            )

    def append_embeddings(self, path_to_embeddings_csv: str, embedding_id: str) -> None:
        """Concatenates embeddings from the Lightly Platform to the local ones.

        Loads the embedding csv file with the corresponding embedding ID in the current dataset
        and appends all of its rows to the local embeddings file located at
        'path_to_embeddings_csv'.

        Args:
            path_to_embeddings_csv:
                The path to the csv containing the local embeddings.
            embedding_id:
                ID of the embedding summary of the embeddings on the Lightly Platform.

        Raises:
            RuntimeError:
                If the number of columns in the local embeddings file and that of the remote
                embeddings file mismatch.

        :meta private:  # Skip docstring generation
        """

        # read embedding from API
        embedding_read_url = self._embeddings_api.get_embeddings_csv_read_url_by_id(
            self.dataset_id, embedding_id
        )
        embedding_reader = self._get_csv_reader_from_read_url(embedding_read_url)
        rows = list(embedding_reader)
        header, online_rows = rows[0], rows[1:]

        # read local embedding
        with open(path_to_embeddings_csv, "r") as f:
            local_rows = list(csv.reader(f))

            if len(local_rows[0]) != len(header):
                raise RuntimeError(
                    "Column mismatch! Number of columns in local and remote"
                    f" embeddings files must match but are {len(local_rows[0])}"
                    f" and {len(header)} respectively."
                )

            local_rows = local_rows[1:]

        # combine online and local embeddings
        total_rows = [header]
        filename_to_local_row = {row[0]: row for row in local_rows}
        for row in online_rows:
            # pick local over online filename if it exists
            total_rows.append(filename_to_local_row.pop(row[0], row))
        # add all local rows which were not added yet
        total_rows.extend(list(filename_to_local_row.values()))

        # save embeddings again
        with open(path_to_embeddings_csv, "w") as f:
            writer = csv.writer(f)
            writer.writerows(total_rows)

    def _order_csv_by_filenames(self, path_to_embeddings_csv: str) -> List[str]:
        """Orders the rows in a csv according to the order specified on the server and saves it as a new file.

        Args:
            path_to_embeddings_csv:
                the path to the csv to order

        Returns:
            the filepath to the new csv

        """
        with open(path_to_embeddings_csv, "r") as f:
            data = csv.reader(f)

            rows = list(data)
            header_row = rows[0]
            rows_without_header = rows[1:]
            index_filenames = header_row.index("filenames")
            filenames = [row[index_filenames] for row in rows_without_header]

            filenames_on_server = self.get_filenames()

            if len(filenames) != len(filenames_on_server):
                raise ValueError(
                    f"There are {len(filenames)} rows in the embedding file, but "
                    f"{len(filenames_on_server)} filenames/samples on the server."
                )
            if set(filenames) != set(filenames_on_server):
                raise ValueError(
                    f"The filenames in the embedding file and "
                    f"the filenames on the server do not align"
                )

            rows_without_header_ordered = self._order_list_by_filenames(
                filenames, rows_without_header
            )

            rows_csv = [header_row]
            rows_csv += rows_without_header_ordered

        return rows_csv



================================================
FILE: lightly/api/api_workflow_upload_metadata.py
================================================
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Dict, List, Union

from requests import Response
from tqdm import tqdm

from lightly.api.utils import paginate_endpoint, retry_utils
from lightly.openapi_generated.swagger_client.models import (
    ConfigurationEntry,
    ConfigurationSetRequest,
    SampleDataModes,
    SamplePartialMode,
    SampleUpdateRequest,
)
from lightly.utils import hipify
from lightly.utils.io import COCO_ANNOTATION_KEYS


class InvalidCustomMetadataWarning(Warning):
    pass


def _assert_key_exists_in_custom_metadata(key: str, dictionary: Dict[str, Any]):
    """Raises a formatted KeyError if key is not a key of the dictionary."""
    if key not in dictionary.keys():
        raise KeyError(
            f"Key {key} not found in custom metadata.\n"
            f"Found keys: {dictionary.keys()}"
        )


class _UploadCustomMetadataMixin:
    """Mixin of helpers to allow upload of custom metadata."""

    def verify_custom_metadata_format(self, custom_metadata: Dict) -> None:
        """Verifies that the custom metadata is in the correct format.

        Args:
            custom_metadata:
                Dictionary of custom metadata, see upload_custom_metadata for
                the required format.

        Raises:
            KeyError:
                If "images" or "metadata" aren't a key of custom_metadata.

        """
        _assert_key_exists_in_custom_metadata(
            COCO_ANNOTATION_KEYS.images, custom_metadata
        )
        _assert_key_exists_in_custom_metadata(
            COCO_ANNOTATION_KEYS.custom_metadata, custom_metadata
        )

    def index_custom_metadata_by_filename(
        self, custom_metadata: Dict[str, Any]
    ) -> Dict[str, Union[Dict, None]]:
        """Creates an index to lookup custom metadata by filename.

        Args:
            custom_metadata:
                Dictionary of custom metadata, see upload_custom_metadata for
                the required format.

        Returns:
            A dictionary mapping from filenames to custom metadata.
            If there are no annotations for a filename, the custom metadata
            is None instead.

        :meta private:  # Skip docstring generation
        """

        # The mapping is filename -> image_id -> custom_metadata
        # This mapping is created in linear time.
        filename_to_image_id = {
            image_info[COCO_ANNOTATION_KEYS.images_filename]: image_info[
                COCO_ANNOTATION_KEYS.images_id
            ]
            for image_info in custom_metadata[COCO_ANNOTATION_KEYS.images]
        }
        image_id_to_custom_metadata = {
            metadata[COCO_ANNOTATION_KEYS.custom_metadata_image_id]: metadata
            for metadata in custom_metadata[COCO_ANNOTATION_KEYS.custom_metadata]
        }
        filename_to_metadata = {
            filename: image_id_to_custom_metadata.get(image_id, None)
            for (filename, image_id) in filename_to_image_id.items()
        }
        return filename_to_metadata

    def upload_custom_metadata(
        self,
        custom_metadata: Dict[str, Any],
        verbose: bool = False,
        max_workers: int = 8,
    ) -> None:
        """Uploads custom metadata to the Lightly Platform.

        The custom metadata is expected in a format similar to the COCO annotations:
        Under the key "images" there should be a list of dictionaries, each with
        a file_name and id. Under the key "metadata", the custom metadata is stored
        as a list of dictionaries, each with an image ID that corresponds to an image
        under the key "images".

        Example:
            >>> custom_metadata = {
            >>>     "images": [
            >>>         {
            >>>             "file_name": "image0.jpg",
            >>>             "id": 0,
            >>>         },
            >>>         {
            >>>             "file_name": "image1.jpg",
            >>>             "id": 1,
            >>>         }
            >>>     ],
            >>>     "metadata": [
            >>>         {
            >>>             "image_id": 0,
            >>>             "number_of_people": 3,
            >>>             "weather": {
            >>>                 "scenario": "cloudy",
            >>>                 "temperature": 20.3
            >>>             }
            >>>         },
            >>>         {
            >>>             "image_id": 1,
            >>>             "number_of_people": 1,
            >>>             "weather": {
            >>>                 "scenario": "rainy",
            >>>                 "temperature": 15.0
            >>>             }
            >>>         }
            >>>     ]
            >>> }

        Args:
            custom_metadata:
                Custom metadata as described above.
            verbose:
                If True, displays a progress bar during the upload.
            max_workers:
                Maximum number of concurrent threads during upload.

        :meta private:  # Skip docstring generation
        """

        self.verify_custom_metadata_format(custom_metadata)

        # For each metadata, we need the corresponding sample_id
        # on the server. The mapping is:
        # metadata -> image_id -> filename -> sample_id

        image_id_to_filename = {
            image_info[COCO_ANNOTATION_KEYS.images_id]: image_info[
                COCO_ANNOTATION_KEYS.images_filename
            ]
            for image_info in custom_metadata[COCO_ANNOTATION_KEYS.images]
        }

        samples: List[SampleDataModes] = list(
            paginate_endpoint(
                self._samples_api.get_samples_partial_by_dataset_id,
                page_size=25000,  # as this information is rather small, we can request a lot of samples at once
                dataset_id=self.dataset_id,
                mode=SamplePartialMode.FILENAMES,
            )
        )

        filename_to_sample_id = {sample.file_name: sample.id for sample in samples}

        upload_requests = []
        for metadata in custom_metadata[COCO_ANNOTATION_KEYS.custom_metadata]:
            image_id = metadata[COCO_ANNOTATION_KEYS.custom_metadata_image_id]
            filename = image_id_to_filename.get(image_id, None)
            if filename is None:
                hipify.print_as_warning(
                    "No image found for custom metadata annotation "
                    f"with image_id {image_id}. "
                    "This custom metadata annotation is skipped. ",
                    InvalidCustomMetadataWarning,
                )
                continue
            sample_id = filename_to_sample_id.get(filename, None)
            if sample_id is None:
                hipify.print_as_warning(
                    "You tried to upload custom metadata for a sample with "
                    f"filename {{{filename}}}, "
                    "but a sample with this filename "
                    "does not exist on the server. "
                    "This custom metadata annotation is skipped. ",
                    InvalidCustomMetadataWarning,
                )
                continue
            upload_request = (metadata, sample_id)
            upload_requests.append(upload_request)

        # retry upload if it times out
        def upload_sample_metadata(upload_request):
            metadata, sample_id = upload_request
            request = SampleUpdateRequest(custom_meta_data=metadata)
            return retry_utils.retry(
                self._samples_api.update_sample_by_id,
                sample_update_request=request,
                dataset_id=self.dataset_id,
                sample_id=sample_id,
            )

        # Upload in parallel with a limit on the number of concurrent requests
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # get iterator over results
            results = executor.map(upload_sample_metadata, upload_requests)
            if verbose:
                results = tqdm(results, unit="metadata", total=len(upload_requests))
            # iterate over results to make sure they are completed
            list(results)

    def create_custom_metadata_config(
        self, name: str, configs: List[ConfigurationEntry]
    ) -> Response:
        """Creates custom metadata config from a list of configurations.

        Args:
            name:
                The name of the custom metadata configuration.
            configs:
                List of metadata configuration entries.

        Returns:
            The API response.

        Examples:
            >>> from lightly.openapi_generated.swagger_codegen.models.configuration_entry import ConfigurationEntry
            >>> entry = ConfigurationEntry(
            >>>     name='Weather',
            >>>     path='weather',
            >>>     default_value='unknown',
            >>>     value_data_type='CATEGORICAL_STRING',
            >>> )
            >>>
            >>> client.create_custom_metadata_config(
            >>>     'My Custom Metadata',
            >>>     [entry],
            >>> )

        :meta private:  # Skip docstring generation
        """
        config_set_request = ConfigurationSetRequest(name=name, configs=configs)
        resp = self._metadata_configurations_api.create_meta_data_configuration(
            configuration_set_request=config_set_request,
            dataset_id=self.dataset_id,
        )
        return resp



================================================
FILE: lightly/api/bitmask.py
================================================
""" Module to work with Lightly BitMasks """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved
import copy
from typing import List


def _hex_to_int(hexstring: str) -> int:
    """Converts a hex string representation of an integer to an integer."""
    return int(hexstring, 16)


def _bin_to_int(binstring: str) -> int:
    """Converts a binary string representation of an integer to an integer."""
    return int(binstring, 2)


def _int_to_hex(x: int) -> str:
    """Converts an integer to a hex string representation."""
    return hex(x)


def _int_to_bin(x: int) -> str:
    """Converts an integer to a binary string representation."""
    return bin(x)


def _get_nonzero_bits(x: int) -> List[int]:
    """Returns a list of indices of nonzero bits in x."""
    offset = 0
    nonzero_bit_indices = []
    while x > 0:
        # if the number is odd, there is a nonzero bit at offset
        if x % 2 > 0:
            nonzero_bit_indices.append(offset)
        # increment the offset and divide the number x by two (rounding down)
        offset += 1
        x = x // 2
    return nonzero_bit_indices


def _invert(x: int, total_size: int) -> int:
    """Flips every bit of x as if x was an unsigned integer."""
    # use XOR of x and 0xFFFFFF to get the inverse
    return x ^ (2**total_size - 1)


def _union(x: int, y: int) -> int:
    """Uses bitwise OR to get the union of the two masks."""
    return x | y


def _intersection(x: int, y: int) -> int:
    """Uses bitwise AND to get the intersection of the two masks."""
    return x & y


def _get_kth_bit(x: int, k: int) -> int:
    """Returns the kth bit in the mask from the right."""
    mask = 1 << k
    return x & mask


def _set_kth_bit(x: int, k: int) -> int:
    """Sets the kth bit in the mask from the right."""
    mask = 1 << k
    return x | mask


def _unset_kth_bit(x: int, k: int) -> int:
    """Clears the kth bit in the mask from the right."""
    mask = ~(1 << k)
    return x & mask


class BitMask:
    """Utility class to represent and manipulate tags.
    Attributes:
        x:
            An integer representation of the binary mask.
    Examples:
        >>> # the following are equivalent
        >>> mask = BitMask(6)
        >>> mask = BitMask.from_hex('0x6')
        >>> mask = Bitmask.from_bin('0b0110')
        >>> # for a dataset with 10 images, assume the following tag
        >>> # 0001011001 where the 1st, 4th, 5th and 7th image are selected
        >>> # this tag would be stored as 0x59.
        >>> hexstring = '0x59'                    # what you receive from the api
        >>> mask = BitMask.from_hex(hexstring)  # create a bitmask from it
        >>> indices = mask.to_indices()         # get list of indices which are one
        >>> # indices is [0, 3, 4, 6]
    """

    def __init__(self, x):
        self.x = x

    @classmethod
    def from_hex(cls, hexstring: str):
        """Creates a bit mask object from a hexstring."""
        return cls(_hex_to_int(hexstring))

    @classmethod
    def from_bin(cls, binstring: str):
        """Creates a BitMask from a binary string."""
        return cls(_bin_to_int(binstring))

    @classmethod
    def from_length(cls, length: int):
        """Creates a all-true bitmask of a predefined length"""
        binstring = "0b" + "1" * length
        return cls.from_bin(binstring)

    def to_hex(self):
        """Creates a BitMask from a hex string."""
        return _int_to_hex(self.x)

    def to_bin(self):
        """Returns a binary string representing the bit mask."""
        return _int_to_bin(self.x)

    def to_indices(self) -> List[int]:
        """Returns the list of indices bits which are set to 1 from the right.
        Examples:
            >>> mask = BitMask('0b0101')
            >>> indices = mask.to_indices()
            >>> # indices is [0, 2]
        """
        return _get_nonzero_bits(self.x)

    def invert(self, total_size: int):
        """Sets every 0 to 1 and every 1 to 0 in the bitstring.

        Args:
            total_size:
                Total size of the tag.

        """
        self.x = _invert(self.x, total_size)

    def complement(self):
        """Same as invert but with the appropriate name."""
        self.invert()

    def union(self, other):
        """Calculates the union of two bit masks.
        Examples:
            >>> mask1 = BitMask.from_bin('0b0011')
            >>> mask2 = BitMask.from_bin('0b1100')
            >>> mask1.union(mask2)
            >>> # mask1.binstring is '0b1111'
        """
        self.x = _union(self.x, other.x)

    def intersection(self, other):
        """Calculates the intersection of two bit masks.
        Examples:
            >>> mask1 = BitMask.from_bin('0b0011')
            >>> mask2 = BitMask.from_bin('0b1100')
            >>> mask1.intersection(mask2)
            >>> # mask1.binstring is '0b0000'
        """
        self.x = _intersection(self.x, other.x)

    def difference(self, other):
        """Calculates the difference of two bit masks.
        Examples:
            >>> mask1 = BitMask.from_bin('0b0111')
            >>> mask2 = BitMask.from_bin('0b1100')
            >>> mask1.difference(mask2)
            >>> # mask1.binstring is '0b0011'
        """
        self.union(other)
        self.x = self.x - other.x

    def __sub__(self, other):
        ret = copy.deepcopy(self)
        ret.difference(other)
        return ret

    def __eq__(self, other):
        return self.to_bin() == other.to_bin()

    def masked_select_from_list(self, list_: List):
        """Returns a subset of a list depending on the bitmask.

        The bitmask is read from right to left, i.e. the least significant bit
        corresponds to index 0.

        Examples:
            >>> list_to_subset = [4, 7, 9, 1]
            >>> mask = BitMask.from_bin("0b0101")
            >>> masked_list = mask.masked_select_from_list(list_to_subset)
            >>> # masked_list = [4, 9]

        """
        indices = self.to_indices()
        return [list_[index] for index in indices]

    def get_kth_bit(self, k: int) -> bool:
        """Returns the boolean value of the kth bit from the right."""
        return _get_kth_bit(self.x, k) > 0

    def set_kth_bit(self, k: int):
        """Sets the kth bit from the right to '1'.
        Examples:
            >>> mask = BitMask('0b0000')
            >>> mask.set_kth_bit(2)
            >>> # mask.binstring is '0b0100'
        """
        self.x = _set_kth_bit(self.x, k)

    def unset_kth_bit(self, k: int):
        """Unsets the kth bit from the right to '0'.
        Examples:
            >>> mask = BitMask('0b1111')
            >>> mask.unset_kth_bit(2)
            >>> # mask.binstring is '0b1011'
        """
        self.x = _unset_kth_bit(self.x, k)



================================================
FILE: lightly/api/download.py
================================================
from __future__ import annotations

import concurrent.futures
import os
import pathlib
import shutil
import threading
import warnings
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Callable

import requests
import tqdm

from lightly.api import retry_utils


def download_and_write_file(
    url: str,
    output_path: str,
    session: requests.Session | None = None,
    retry_fn: Callable[..., Any] = retry_utils.retry,
    request_kwargs: dict[str, Any] | None = None,
) -> None:
    """Downloads a file from a url and saves it to disk

    Args:
        url:
            Url of the file to download.
        output_path:
            Where to store the file, including filename and extension.
        session:
            Session object to persist certain parameters across requests.
        retry_fn:
            Retry function that handles failed downloads.
        request_kwargs:
            Additional parameters passed to requests.get().
    """
    request_kwargs = request_kwargs or {}
    request_kwargs.setdefault("stream", True)
    request_kwargs.setdefault("timeout", 10)
    req = requests if session is None else session
    out_path = pathlib.Path(output_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with retry_fn(req.get, url=url, **request_kwargs) as response:  # type: ignore[attr-defined]
        response.raise_for_status()
        with open(out_path, "wb") as file:
            shutil.copyfileobj(response.raw, file)


def download_and_write_all_files(
    file_infos: list[tuple[str, str]],
    output_dir: str,
    max_workers: int | None = None,
    verbose: bool = False,
    retry_fn: Callable[..., Any] = retry_utils.retry,
    request_kwargs: dict[str, Any] | None = None,
) -> None:
    """Downloads all files and writes them to disk.

    Args:
        file_infos:
            List containing (filename, url) tuples.
        output_dir:
            Output directory where files will stored in.
        max_workers:
            Maximum number of workers. If `None` the number of workers is chosen
            based on the number of available cores.
        verbose:
            Shows progress bar if set to `True`.
        retry_fn:
            Retry function that handles failed downloads.
        request_kwargs:
            Additional parameters passed to requests.get().

    """

    def thread_download_and_write(
        file_info: tuple[str, str],
        output_dir: str,
        lock: threading.Lock,
        sessions: dict[int, requests.Session],
        retry_fn: Callable[..., Any],
        request_kwargs: dict[str, Any] | None = None,
    ) -> None:
        filename, url = file_info
        output_path = os.path.join(output_dir, filename)
        thread_id = threading.get_ident()

        lock.acquire()
        session = sessions.get(thread_id)
        if session is None:
            session = requests.Session()
            sessions[thread_id] = session
        lock.release()

        download_and_write_file(
            url=url,
            output_path=output_path,
            session=session,
            retry_fn=retry_fn,
            request_kwargs=request_kwargs,
        )

    # dict where every thread stores its requests.Session
    sessions: dict[int, requests.Session] = dict()
    # use lock because sessions dict is shared between threads
    lock = threading.Lock()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures_to_file_info = {
            executor.submit(
                thread_download_and_write,
                file_info=file_info,
                output_dir=output_dir,
                lock=lock,
                sessions=sessions,
                retry_fn=retry_fn,
                request_kwargs=request_kwargs,
            ): file_info
            for file_info in file_infos
        }
        futures = concurrent.futures.as_completed(futures_to_file_info)
        if verbose:
            futures = tqdm.tqdm(futures)
        for future in futures:
            filename, url = futures_to_file_info[future]
            try:
                future.result()
            except Exception as ex:
                warnings.warn(f"Could not download {filename} from {url}")



================================================
FILE: lightly/api/patch.py
================================================
import logging
from typing import Any, Dict, Type


def make_swagger_configuration_picklable(
    configuration_cls: Type,
) -> None:
    """Adds __getstate__ and __setstate__ methods to swagger configuration to make it
    picklable.

    This doesn't make all swagger classes picklable. Notably, the ApiClient and
    and RESTClientObject classes are not picklable. Use the picklable
    LightlySwaggerApiClient and LightlySwaggerRESTClientObject classes instead.
    """
    configuration_cls.__getstate__ = _Configuration__getstate__
    configuration_cls.__setstate__ = _Configuration__setstate__


def _Configuration__getstate__(self: Type) -> Dict[str, Any]:
    state = self.__dict__.copy()

    # Remove all loggers as they are not picklable. This removes the package_logger
    # and urllib3_logger. Note that we cannot remove the with:
    # `del state["logger"]["package_logger"]` as this would modify self.__dict__ due to
    # shallow copy.
    state["logger"] = {}

    # formatter, stream_handler and file_handler are not picklable
    state["logger_formatter"] = None
    state["logger_stream_handler"] = None
    state["logger_file_handler"] = None
    return state


def _Configuration__setstate__(self: Type, state: Dict[str, Any]) -> None:
    self.__dict__.update(state)
    # Recreate logger objects.
    self.logger["package_logger"] = logging.getLogger(
        "lightly.openapi_generated.swagger_client"
    )
    self.logger["urllib3_logger"] = logging.getLogger("urllib3")

    # Set logger_format and logger_file explicitly because they have setter decoraters
    # defined on the Configuration class. These decorates have side effects and create
    # self.__logger_format, self.logger_formatter, self.__logger_file,
    # self.logger_file_handler, and self.logger_stream_handler under the hood.
    #
    # Note that the setter decorates are not called by the self.__dict__.update(state)
    # at the beginning of the function.
    #
    # The attributes are set to the class mangled values stored in the state dict.
    self.logger_format = state[
        "_Configuration__logger_format"
    ]  # set to self.__logger_format
    self.logger_file = state["_Configuration__logger_file"]  # set to self.__logger_file

    # Set debug explicitly because it has a setter decorator with side effects.
    self.debug = state["_Configuration__debug"]



================================================
FILE: lightly/api/retry_utils.py
================================================
from __future__ import annotations

import json
import logging
import reprlib
import time
from dataclasses import dataclass, field
from json.decoder import JSONDecodeError
from typing import Callable, Optional, TypeVar

import requests
import urllib3.exceptions
from typing_extensions import ParamSpec, Protocol, runtime_checkable

from lightly.openapi_generated.swagger_client.models.api_error_code import ApiErrorCode

logger = logging.getLogger("lightly-worker")


@runtime_checkable
class APIExceptionLike(Protocol):
    """Protocol for API exceptions.

    Implement the relevant attributes of the openapi-generated APIException class.
    See https://github.com/lightly-ai/lightly/blob/3642a12510895c4e917bc05801be613aab86fde2/lightly/openapi_generated/swagger_client/exceptions.py#L103-L115

    We cannot directly use API exception classes, as there might be multiple of them.
    """

    status: int | None
    body: str | None


RETRY_HTTP_ERROR_CODES = [
    400,  # Bad Request. Jeremy: This error is typically caused by the client but could also be caused by the server.
    408,  # Timeout (e.g google storage returns this)
    409,  # Conflict
    429,  # Too Many Requests
    500,  # Internal Server Error
    502,  # Bad Gateway
    503,  # Service Unavailable
    504,  # Gateway Timeout
]

# The api client generated by swagger uses urllib3 under the hood. It does not
# specify a retry strategy for urllib3 and also only catches certain errors
# raised by urllib3 but not all of them! Therefore api calls can result in
# urllib3 errors which we should retry on. Urllib3 without retries catches,
# potentially converts, and finally  reraises all errors listed here:
# https://github.com/urllib3/urllib3/blob/972e9f02cd219892701fa0a1129dda7f81dac535/src/urllib3/connectionpool.py#L764-L773
# Because some errors are converted, we do not have to catch all errors in the
# list but only the ones that can be reraised by urrlib3.
RETRY_API_URLLIB3_ERROR = (
    urllib3.exceptions.TimeoutError,
    urllib3.exceptions.ProtocolError,  # urllib3 converts HTTPException and OSError to ProtocolError
    urllib3.exceptions.SSLError,  # urllib3 converts BaseSSLError and CertificateError to SSLError
    urllib3.exceptions.ProxyError,
)

# Requests uses urllib3 under the hood which means that sometimes urllib3 errors
# can be raised from requests code. Here we list all the urrlib3 errors that
# should be retried when calling requests. The retried errors are the same as
# the ones caught by requests internally: https://github.com/psf/requests/blob/da9996fe4dc63356e9467d0a5e10df3d89a8528e/requests/models.py#L815-L825
# Note that requests only catches these errors when the iter_content
# function on the response object is called. But we usually use
# response.raw directly without calling iter_content and have to catch
# errors ourselves.
RETRY_REQUESTS_URLLIB3_ERROR = (
    urllib3.exceptions.DecodeError,
    urllib3.exceptions.ReadTimeoutError,
    urllib3.exceptions.ProtocolError,
    urllib3.exceptions.SSLError,
)

# Requests errors that we don't want to retry on. See
# https://github.com/psf/requests/blob/main/requests/exceptions.py for possible errors.
NOT_RETRY_REQUESTS_ERROR = (
    requests.exceptions.URLRequired,
    requests.exceptions.MissingSchema,
    requests.exceptions.InvalidSchema,
    requests.exceptions.InvalidURL,
    requests.exceptions.InvalidHeader,
    requests.exceptions.RetryError,
    requests.exceptions.JSONDecodeError,
    requests.exceptions.InvalidJSONError,
)


class MaxRetryError(Exception):
    """Error raised when the maximum number of retries are reached."""

    pass


class DatasetNotFoundError(Exception):
    """Exception raised when a dataset is not found, possibly due to deletion."""

    pass


@dataclass
class RetryConfig:
    """Base configuration for retry behavior.

    Attributes:
        max_retries:
             Maximum number of retries.
         backoff_factor:
             A backoff factor to apply between attempts after the second try.
             Will sleep for {backoff factor} * (2 ** ({number of total retries} - 1))
             seconds after the second try. Will never be more than backoff_max.
             Behavior is identical to urllib3.util.Retry.
         backoff_max:
             Maximum time in seconds to wait between retries.
    """

    max_retries: int = 5
    backoff_factor: float = 1.0
    backoff_max: float = 120.0


@dataclass
class RetryOnApiConfig(RetryConfig):
    """Configuration for API retry behavior.

    Attributes:
        backoff_min_on_429: Minimum time to wait between retries after a 429 error.
        retry_http_error_codes: HTTP error codes to retry on.
        retry_api_error_codes: API error codes to retry on.
        retry_api_urllib3_error: urllib3 error types to retry on for API calls.
    """

    backoff_min_on_429: float = 10.0
    retry_http_error_codes: set[int] = field(
        default_factory=lambda: set(RETRY_HTTP_ERROR_CODES)
    )
    retry_api_error_codes: set[ApiErrorCode] = field(default_factory=set)
    retry_api_urllib3_error: tuple[type[Exception], ...] = field(
        default_factory=lambda: RETRY_API_URLLIB3_ERROR
    )


@dataclass
class RetryOnRequestsConfig(RetryOnApiConfig):
    """Configuration for requests retry behavior.

    Attributes:
        retry_requests_urllib3_error: urllib3 error types to retry on for requests calls.
        not_retry_requests_error: Request error types to not retry on.
    """

    retry_requests_urllib3_error: tuple[type[Exception], ...] = field(
        default_factory=lambda: RETRY_REQUESTS_URLLIB3_ERROR
    )
    not_retry_requests_error: tuple[type[requests.RequestException], ...] = field(
        default_factory=lambda: NOT_RETRY_REQUESTS_ERROR
    )


# argument and return types
ParamType = ParamSpec("ParamType")
ReturnType = TypeVar("ReturnType")


class Retry:
    """Base class to retry function calls when they fail with an exception.

    Subclasses must overwrite should_retry().

    Attributes:
        config: Configuration for retry behavior, including max retries, backoff settings,
            and exception types to retry on.
    """

    def __init__(self, config: RetryConfig):
        self.config = config

    def __call__(
        self,
        fn: Callable[ParamType, ReturnType],
        *args: ParamType.args,
        **kwargs: ParamType.kwargs,
    ) -> ReturnType:
        """Tries to execute fn with the given args and kwargs."""
        retries = 0
        while True:
            try:
                return fn(*args, **kwargs)
            except BaseException as ex:
                if retries >= self.config.max_retries:
                    raise MaxRetryError(
                        f"Calling '{fn.__name__}' failed after {retries} attempt(s). "
                        f"Args: {reprlib.repr(args)}; kwargs: {reprlib.repr(kwargs)}. "
                        f"Last error: {self.format_error(self._wrap_exception(ex))}"
                    ) from ex
                if not self.should_retry(ex):
                    raise self._wrap_exception(ex)
                # TODO(Guarin, 02/23): Disabled logging because it can be called from
                # a different process than the main process, resulting in a potential
                # deadlock. Enable logging again once we switch to using spawn or
                # forkserver to start processes. See LIG-2486.
                # logger.debug(
                #     f"Retrying: Retry {retries + 1} of {self.config.max_retries} after {repr(ex)}"
                # )
                backoff = self.calculate_backoff(retries, ex)
                time.sleep(backoff)
            retries += 1

    def format_error(self, exception: BaseException) -> str:
        """Writes errors in a readable format.

        Args:
            exception: Exception raised.

        Returns:
            Formatted error message.
        """
        error_details = str(exception) or "None"
        return f"{exception.__class__.__name__}. Details: {error_details}"

    def should_retry(self, ex: BaseException) -> bool:
        """
        Takes an exception as input and returns True
        if the failed call should be retried and False otherwise.
        If False is returned then the original exception is raised.
        """
        raise NotImplementedError()

    def calculate_backoff(self, retries: int, ex: BaseException) -> float:
        """Returns the time to sleep before the next try."""
        if retries <= 0:
            return 0
        backoff: float = self.config.backoff_factor * (2 ** (retries - 1))
        return max(0.0, min(backoff, self.config.backoff_max))

    def _wrap_exception(self, error: BaseException) -> BaseException:
        return error


class RetryOnApiError(Retry):
    """Class to retry api calls when they fail with an exception.

    Attributes:
        config: Configuration for retry behavior, including max retries, backoff settings,
            and exception types to retry on.
    """

    config: RetryOnApiConfig

    def __init__(
        self,
        config: RetryOnApiConfig,
    ):
        super().__init__(config=config)

    def should_retry(self, ex: BaseException) -> bool:
        is_urllib_error = isinstance(ex, self.config.retry_api_urllib3_error)
        is_http_error = False
        is_api_error = False
        if isinstance(ex, APIExceptionLike):
            is_http_error = ex.status in self.config.retry_http_error_codes
            is_api_error = (
                _get_error_code_from_api_exception(ex=ex)
                in self.config.retry_api_error_codes
            )
        return is_urllib_error or is_http_error or is_api_error

    def calculate_backoff(self, retries: int, ex: BaseException) -> float:
        adjusted_retries = retries + 1  # relieve api by not immediately retrying
        backoff = super().calculate_backoff(adjusted_retries, ex)
        if isinstance(ex, APIExceptionLike):
            if ex.status == 429:
                # Additional backoff for too many requests error code
                backoff = max(backoff, self.config.backoff_min_on_429)
        return backoff

    def _wrap_exception(self, error: BaseException) -> BaseException:
        """Wraps exceptions to provide more meaningful error messages.

        Args:
            error: The original exception.

        Returns:
            The wrapped exception or the original exception if no wrapping is needed.
        """
        if isinstance(error, APIExceptionLike):
            error_code = _get_error_code_from_api_exception(error)
            if error_code == ApiErrorCode.DATASET_UNKNOWN:
                return DatasetNotFoundError(
                    "Dataset has suddenly disappeared and cannot be found. Did you delete it?"
                )

        return error


class RetryOnRequestsError(RetryOnApiError):
    """Retries on requests errors and api errors.

    We retry on api errors because redirected read urls are handled by our api but the
    original request in the worker is made using requests.

    We retry on urllib3 errors because requests uses urllib3 under the hood.
    """

    config: RetryOnRequestsConfig

    def __init__(
        self,
        config: RetryOnRequestsConfig,
    ):
        super().__init__(config=config)

    def should_retry(self, ex: BaseException) -> bool:
        if super().should_retry(ex):
            # Api error.
            return True
        elif isinstance(ex, self.config.retry_requests_urllib3_error):
            # Urllib3 error.
            return True
        elif isinstance(ex, self.config.not_retry_requests_error):
            # Requests error that we don't retry.
            return False
        elif isinstance(ex, requests.exceptions.HTTPError):
            # Retry requests with a missing response.
            if ex.response is None:
                return True

            # Retry the same HTTP errors that we also retry for the API.
            # Requests raises an error for any HTTP status code in [400, 599], see:
            # https://github.com/psf/requests/blob/6e5b15d542a4e85945fd72066bb6cecbc3a82191/requests/models.py#L1010-L1018
            return ex.response.status_code in self.config.retry_http_error_codes
        elif isinstance(ex, requests.exceptions.RequestException):
            # Retry on any other requests error.
            return True
        return False


# Default retry for all api calls
retry = RetryOnApiError(config=RetryOnApiConfig())


"""Helper function for fully retrying requests calls with a timeout.

This function can be used in conjunction with requests Sessions that already
have a retry strategy. The retry mechanism in requests does not fully retry
the request but only retries the last action. This can be a problem when
downloading files using streaming as requests will only retry the last read()
call to read the next chunk of the stream which fails, for example, if the
connection was interrupted. This function avoids the problem by restarting
the full download if a read() call raises an error.

"""
retry_on_requests_error = RetryOnRequestsError(config=RetryOnRequestsConfig())


def no_retry(
    fn: Callable[ParamType, ReturnType],
    *args: ParamType.args,
    **kwargs: ParamType.kwargs,
) -> ReturnType:
    return fn(*args, **kwargs)


def _get_error_code_from_api_exception(
    ex: APIExceptionLike,
) -> Optional[str]:
    """Returns api error code from ApiException.

    This is the "code" part from the failed api response. For example, a request failed
    with:

        HTTP response body: {
            "code": "MALFORMED_REQUEST",
            "error": "sampleId must match the following: \"/^[a-f0-9]{24}$/\"",
            "requestId": "a805582f5069db7696b9b66604873b3c"
        }

    will return "MALFORMED_REQUEST".
    """
    # TODO(Guarin, 03/23): Add debug log messages if ex.body has unexpected format once
    # we use process with spawn method.

    # Api error code exists only if returned body is json string.
    if not isinstance(ex.body, str):
        return None
    try:
        body = json.loads(ex.body)
    except JSONDecodeError:
        # Invalid json string.
        return None
    # We expect the api to return a json dict with a "code" entry.
    if isinstance(body, dict):
        return body.get("code")
    else:
        return None



================================================
FILE: lightly/api/serve.py
================================================
from __future__ import annotations

from http.server import SimpleHTTPRequestHandler, ThreadingHTTPServer
from pathlib import Path
from typing import Sequence
from urllib import parse

from lightly.data import _helpers


def get_server(
    paths: Sequence[Path],
    host: str,
    port: int,
) -> ThreadingHTTPServer:
    """Returns an HTTP server that serves a local datasource.

    Args:
        paths:
            List of paths to serve.
        host:
            Host to serve the datasource on.
        port:
            Port to serve the datasource on.

    Examples:
        >>> from lightly.api import serve
        >>> from pathlib import Path
        >>> serve(
        >>>    paths=[Path("/input_mount), Path("/lightly_mount)],
        >>>    host="localhost",
        >>>    port=3456,
        >>> )

    """

    class _LocalDatasourceRequestHandler(SimpleHTTPRequestHandler):
        def translate_path(self, path: str) -> str:
            return _translate_path(path=path, directories=paths)

        def do_OPTIONS(self) -> None:
            self.send_response(204)
            self.send_header("Access-Control-Allow-Origin", "*")
            self.send_header("Access-Control-Allow-Methods", "GET, POST, OPTIONS")
            self.end_headers()

        def send_response_only(self, code: int, message: str | None = None) -> None:
            super().send_response_only(code, message)
            self.send_header(
                "Cache-Control", "no-store, must-revalidate, no-cache, max-age=-1"
            )
            self.send_header("Expires", "0")

    return ThreadingHTTPServer((host, port), _LocalDatasourceRequestHandler)


def validate_input_mount(input_mount: Path) -> None:
    """Validates that the input mount is a directory and contains files."""
    input_mount = input_mount.resolve()
    if not input_mount.exists():
        raise ValueError(
            f"Path for 'input_mount' argument '{input_mount}' does not exist."
        )
    if not input_mount.is_dir():
        raise ValueError(
            f"Path for 'input_mount' argument '{input_mount}' is not a directory."
        )
    if not _dir_contains_image_or_video(path=input_mount):
        raise ValueError(
            f"Path for 'input_mount' argument '{input_mount}' does not contain any "
            "images or videos. Please verify that this is the correct directory. See "
            "our docs on lightly-serve for more information: "
            "https://docs.lightly.ai/docs/local-storage#optional-after-run-view-local-data-in-lightly-platform"
        )


def validate_lightly_mount(lightly_mount: Path) -> None:
    lightly_mount = lightly_mount.resolve()
    """Validates that the Lightly mount is a directory."""
    if not lightly_mount.exists():
        raise ValueError(
            f"Path for 'lightly_mount' argument '{lightly_mount}' does not exist."
        )
    if not lightly_mount.is_dir():
        raise ValueError(
            f"Path for 'lightly_mount' argument '{lightly_mount}' is not a directory."
        )


def _dir_contains_image_or_video(path: Path) -> bool:
    extensions = set(_helpers.IMG_EXTENSIONS + _helpers.VIDEO_EXTENSIONS)
    return any(
        p for p in path.rglob("**/*") if p.is_file() and p.suffix.lower() in extensions
    )


def _translate_path(path: str, directories: Sequence[Path]) -> str:
    """Translates a relative path to a file in the local datasource.

    Tries to resolve the relative path to a file in the first directory
    and serves it if it exists. Otherwise, it tries to resolve the relative
    path to a file in the second directory and serves it if it exists, etc.

    Args:
        path:
            Relative path to a file in the local datasource.
        directories:
            List of directories to search for the file.


    Returns:
        Absolute path to the file in the local datasource or an empty string
        if the file doesn't exist.

    """
    path = parse.unquote(path)
    stripped_path = path.lstrip("/")
    for directory in directories:
        if (directory / stripped_path).exists():
            return str(directory / stripped_path)
    return ""  # Not found.



================================================
FILE: lightly/api/swagger_api_client.py
================================================
from typing import Any, Dict, Optional, Tuple, Union

from lightly.api.swagger_rest_client import LightlySwaggerRESTClientObject
from lightly.openapi_generated.swagger_client.api_client import ApiClient, Configuration

DEFAULT_API_TIMEOUT = 60 * 3  # seconds


class PatchApiClientMixin:
    """Mixin that makes an ApiClient object picklable."""

    def __getstate__(self) -> Dict[str, Any]:
        state = self.__dict__.copy()
        # Set _pool to None as ThreadPool is not picklable. It will be automatically
        # recreated once the pool is accessed after unpickling.
        state["_pool"] = None
        # Urllib3 response is not picklable. We can safely remove this as it only
        # serves as a cache.
        if "last_response" in state:
            del state["last_response"]
        return state


class LightlySwaggerApiClient(PatchApiClientMixin, ApiClient):
    """Subclass of ApiClient with patches to make the client picklable.

    Uses a LightlySwaggerRESTClientObject instead of RESTClientObject for additional
    patches. See LightlySwaggerRESTClientObject for details.


    Attributes:
        configuration:
            Configuration.
        timeout:
            Timeout in seconds. Is either a single total_timeout value or a
            (connect_timeout, read_timeout) tuple. No timeout is applied if the
            value is None.
            See https://urllib3.readthedocs.io/en/stable/reference/urllib3.util.html?highlight=timeout#urllib3.util.Timeout
            for details on the different values.
        header_name:
            A header to pass when making calls to the API.
        header_value:
            A header value to pass when making calls to the API.
        cookie:
            A cookie to include in the header when making calls to the API.
    """

    def __init__(
        self,
        configuration: Configuration,
        timeout: Union[None, int, Tuple[int, int]] = DEFAULT_API_TIMEOUT,
        header_name: Optional[str] = None,
        header_value: Optional[str] = None,
        cookie: Optional[str] = None,
    ):
        super().__init__(
            configuration=configuration,
            header_name=header_name,
            header_value=header_value,
            cookie=cookie,
        )
        self.rest_client = LightlySwaggerRESTClientObject(
            configuration=configuration, timeout=timeout
        )



================================================
FILE: lightly/api/swagger_rest_client.py
================================================
import json
from json import JSONDecodeError
from typing import Any, Dict, Optional, Tuple, Union

from lightly.openapi_generated.swagger_client.api_client import Configuration
from lightly.openapi_generated.swagger_client.exceptions import ApiException
from lightly.openapi_generated.swagger_client.rest import RESTClientObject


class PrettyPrintApiException(ApiException):
    def __init__(self, current_exception: ApiException):
        super().__init__(current_exception.status, current_exception.reason)
        self.body = current_exception.body
        self.headers = current_exception.headers

    def __str__(self) -> str:
        error_message = "\n"
        error_message += "#" * 100
        error_message += "\n"
        error_message += f"Error Code: {self.status}"
        error_message += "\n"
        error_message += f"Error Reason: {self.reason}"
        error_message += "\n"
        error_message += "\n"
        try:
            error_body_dict = json.loads(self.body)
        except JSONDecodeError:
            pass
        else:
            if "error" in error_body_dict:
                error_message += f"Error Message: {error_body_dict['error']}"

        error_message += "\n"

        error_message += "#" * 100

        # make the error message red
        error_message = f"\033[91m{error_message}\033[0m"

        return error_message


class PatchRESTClientObjectMixin:
    """Mixin that adds patches to a RESTClientObject.

    * Adds default timeout to all requests
    * Encodes list query parameters properly
    * Makes the client picklable

    Should only used in combination with RESTClientObject and must come before the
    RESTClientObject in the inheritance order. So this is ok:

        >>> class MyRESTClientObject(PatchRESTClientObjectMixin, RESTClientObject): pass

    while this doesn't work:

        >>> class MyRESTClientObject(RESTClientObject, PatchRESTClientObjectMixin): pass

    A wrong inheritance order will result in the super() calls no calling the correct
    parent classes.
    """

    def __init__(
        self,
        configuration: Configuration,
        timeout: Union[None, int, Tuple[int, int]],
        pools_size: int = 4,
        maxsize: Optional[int] = None,
    ):
        # Save args as attributes to make the class picklable.
        self.configuration = configuration
        self.timeout = timeout
        self.pools_size = pools_size
        self.maxsize = maxsize

        # Initialize RESTClientObject class
        super().__init__(
            configuration=configuration, pools_size=pools_size, maxsize=maxsize
        )

    def request(
        self,
        method,
        url,
        query_params=None,
        headers=None,
        body=None,
        post_params=None,
        _preload_content=True,
        _request_timeout=None,
    ):
        # Set default timeout. This is necessary because the openapi client does not
        # respect timeouts configured by urllib3. Instead it expects a timeout to be
        # passed with every request. See code here:
        # https://github.com/lightly-ai/lightly/blob/ffbd32fe82f76b37c8ac497640355314474bfc3b/lightly/openapi_generated/swagger_client/rest.py#L141-L148
        if _request_timeout is None:
            _request_timeout = self.timeout

        # Call RESTClientObject.request
        try:
            return super().request(
                method=method,
                url=url,
                query_params=query_params,
                headers=headers,
                body=body,
                post_params=post_params,
                _preload_content=_preload_content,
                _request_timeout=_request_timeout,
            )
        except ApiException as e:
            raise PrettyPrintApiException(e) from None

    def __getstate__(self) -> Dict[str, Any]:
        """__getstate__ method for pickling."""
        state = self.__dict__.copy()
        # Delete pool_manager as it cannot be pickled. Note that it is not possible to
        # unpickle and use a LightlySwaggerRESTClientObject without either instantiating
        # the pool_manager manually again or calling the init method on the rest client.
        del state["pool_manager"]
        return state

    def __setstate__(self, state: Dict[str, Any]) -> None:
        """__setstate__ method for pickling."""
        self.__dict__.update(state)
        # Calling init to recreate the pool_manager attribute.
        self.__init__(
            configuration=state["configuration"],
            timeout=state["timeout"],
            pools_size=state["pools_size"],
            maxsize=state["maxsize"],
        )


class LightlySwaggerRESTClientObject(PatchRESTClientObjectMixin, RESTClientObject):
    """Subclass of RESTClientObject which contains additional patches for the request
    method and making the client picklable.

    See PatchRESTClientObjectMixin for details.

    Attributes:
        configuration:
            Configuration.
        timeout:
            Timeout in seconds. Is either a single total_timeout value or a
            (connect_timeout, read_timeout) tuple. No timeout is applied if the
            value is None.
            See https://urllib3.readthedocs.io/en/stable/reference/urllib3.util.html?highlight=timeout#urllib3.util.Timeout
            for details on the different values.
        pools_size:
            Number of connection pools. Defaults to 4.
        maxsize:
            Maxsize is the number of requests to host that are allowed in parallel.
            Defaults to None.
    """

    pass



================================================
FILE: lightly/api/utils.py
================================================
""" Communication Utility """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

import io
import os
from enum import Enum
from typing import Iterator, List, Optional

# the following two lines are needed because
# PIL misidentifies certain jpeg images as MPOs
from PIL import JpegImagePlugin

JpegImagePlugin._getmp = lambda: None

from lightly.api import retry_utils
from lightly.openapi_generated.swagger_client.configuration import Configuration

MAXIMUM_FILENAME_LENGTH = 255


class Paginated(Iterator):
    def __init__(self, fn, page_size, *args, **kwargs):
        self.entries: List = []
        self.last_chunk_size = page_size
        self.offset = 0
        self.fn = fn
        self.page_size = page_size
        self.args = args
        self.kwargs = kwargs

    def __iter__(self):
        return self

    def __next__(self):
        if len(self.entries) == 0:
            # stop iteration if the last chunk was smaller than the page size
            if self.last_chunk_size < self.page_size:
                raise StopIteration
            chunk = retry_utils.retry(
                self.fn,
                page_offset=self.offset * self.page_size,
                page_size=self.page_size,
                *self.args,
                **self.kwargs,
            )
            if len(chunk) == 0:
                raise StopIteration
            self.offset += 1
            self.last_chunk_size = len(chunk)
            # Handle the case where the chunk is a string. In this case we want
            # to return the whole page as a single string instead of an interable
            # of characters.
            chunk = chunk if not isinstance(chunk, str) else [chunk]
            self.entries.extend(chunk)
        return self.entries.pop(0)


def paginate_endpoint(fn, page_size=5000, *args, **kwargs) -> Iterator:
    """Paginates an API endpoint

    Args:
        fn:
            The endpoint which will be paginated until there is not any more data
        page_size:
            The size of the pages to pull
    """
    return Paginated(fn, page_size, *args, **kwargs)


def getenv(key: str, default: str):
    """Return the value of the environment variable key if it exists,
    or default if it doesn’t.

    """
    try:
        return os.getenvb(key.encode(), default.encode()).decode()
    except Exception:
        pass
    try:
        return os.getenv(key, default)
    except Exception:
        pass
    return default


def PIL_to_bytes(img, ext: str = "png", quality: int = None):
    """Return the PIL image as byte stream. Useful to send image via requests."""
    bytes_io = io.BytesIO()
    if quality is not None:
        img.save(bytes_io, format=ext, quality=quality)
    else:
        subsampling = -1 if ext.lower() in ["jpg", "jpeg"] else 0
        img.save(bytes_io, format=ext, quality=100, subsampling=subsampling)
    bytes_io.seek(0)
    return bytes_io


def check_filename(basename):
    """Checks the length of the filename.

    Args:
        basename:
            Basename of the file.

    """
    return len(basename) <= MAXIMUM_FILENAME_LENGTH


def build_azure_signed_url_write_headers(
    content_length: str,
    x_ms_blob_type: str = "BlockBlob",
    accept: str = "*/*",
    accept_encoding: str = "*",
):
    """Builds the headers required for a SAS PUT to Azure blob storage.

    Args:
        content_length:
            Length of the content in bytes as string.
        x_ms_blob_type:
            Blob type (one of BlockBlob, PageBlob, AppendBlob)
        accept:
            Indicates which content types the client is able to understand.
        accept_encoding:
            Indicates the content encoding that the client can understand.

    Returns:
        Formatted header which should be passed to the PUT request.

    """
    headers = {
        "x-ms-blob-type": x_ms_blob_type,
        "Accept": accept,
        "Content-Length": content_length,
        "x-ms-original-content-length": content_length,
        "Accept-Encoding": accept_encoding,
    }
    return headers


class DatasourceType(Enum):
    S3 = "S3"
    GCS = "GCS"
    AZURE = "AZURE"
    LOCAL = "LOCAL"


def get_signed_url_destination(signed_url: str = "") -> DatasourceType:
    """
    Tries to figure out the of which cloud provider/datasource type a signed url comes from (S3, GCS, Azure)
    Args:
        signed_url:
            The signed url of a "bucket" provider
    Returns:
        DatasourceType
    """

    assert isinstance(signed_url, str)

    if "storage.googleapis.com/" in signed_url:
        return DatasourceType.GCS
    if ".amazonaws.com/" in signed_url and ".s3." in signed_url:
        return DatasourceType.S3
    if ".windows.net/" in signed_url:
        return DatasourceType.AZURE
    # default to local as it must be some special setup
    return DatasourceType.LOCAL


def get_lightly_server_location_from_env() -> str:
    return (
        getenv("LIGHTLY_SERVER_LOCATION", "https://api.lightly.ai").strip().rstrip("/")
    )


def get_api_client_configuration(
    token: Optional[str] = None,
    raise_if_no_token_specified: bool = True,
) -> Configuration:
    host = get_lightly_server_location_from_env()
    ssl_ca_cert = getenv("LIGHTLY_CA_CERTS", None)
    proxy = getenv("ALL_PROXY", getenv("HTTPS_PROXY", getenv("HTTP_PROXY", None)))

    if token is None:
        token = getenv("LIGHTLY_TOKEN", None)
    if token is None and raise_if_no_token_specified:
        raise ValueError(
            "Either provide a 'token' argument or export a LIGHTLY_TOKEN environment variable"
        )

    configuration = Configuration()
    configuration.api_key = {"ApiKeyAuth": token}
    configuration.ssl_ca_cert = ssl_ca_cert
    configuration.proxy = proxy
    configuration.host = host

    return configuration



================================================
FILE: lightly/cli/__init__.py
================================================
""" The lightly.cli module provides a console interface
    for training self-supervised models, embedding,
    and filtering datasets
"""

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

from lightly.cli.crop_cli import crop_cli
from lightly.cli.download_cli import download_cli
from lightly.cli.embed_cli import embed_cli
from lightly.cli.lightly_cli import lightly_cli
from lightly.cli.train_cli import train_cli



================================================
FILE: lightly/cli/_cli_simclr.py
================================================
""" SimCLR Model """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

import warnings

import torch
import torch.nn as nn

from lightly.models.modules import SimCLRProjectionHead


class _SimCLR(nn.Module):
    """Implementation of SimCLR used by the command-line interface.

    Provides backwards compatability with old checkpoints.
    """

    def __init__(self, backbone: nn.Module, num_ftrs: int = 32, out_dim: int = 128):
        super(_SimCLR, self).__init__()

        self.backbone = backbone
        self.projection_head = SimCLRProjectionHead(
            num_ftrs, num_ftrs, out_dim, batch_norm=False
        )

    def forward(self, x0: torch.Tensor, x1: torch.Tensor = None):
        """Embeds and projects the input images."""

        # forward pass of first input x0
        f0 = self.backbone(x0).flatten(start_dim=1)
        out0 = self.projection_head(f0)

        # return out0 if x1 is None
        if x1 is None:
            return out0

        # forward pass of second input x1
        f1 = self.backbone(x1).flatten(start_dim=1)
        out1 = self.projection_head(f1)

        # return both outputs
        return out0, out1



================================================
FILE: lightly/cli/_helpers.py
================================================
""" Command-Line Interface Helpers """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved
import os

import hydra
import torch
from hydra import utils
from torch import nn as nn

from lightly.cli._cli_simclr import _SimCLR
from lightly.embedding import SelfSupervisedEmbedding
from lightly.models import ZOO as model_zoo
from lightly.models import ResNetGenerator
from lightly.models.batchnorm import get_norm_layer
from lightly.utils.version_compare import version_compare


def cpu_count():
    """Returns the number of CPUs which are present in the system.

    This number is not equivalent to the number of available CPUs to the process.

    """
    return os.cpu_count()


def fix_input_path(path):
    """Fix broken relative paths."""
    if not os.path.isabs(path):
        path = utils.to_absolute_path(path)
    return path


def fix_hydra_arguments(config_path: str = "config", config_name: str = "config"):
    """Helper to make hydra arugments adaptive to installed hydra version

    Hydra introduced the `version_base` argument in version 1.2.0
    We use this helper to provide backwards compatibility to older hydra verisons.
    """

    hydra_args = {"config_path": config_path, "config_name": config_name}

    try:
        if version_compare(hydra.__version__, "1.2.0") >= 0:
            hydra_args["version_base"] = None
        elif version_compare(hydra.__version__, "1.1.2") > 0:
            hydra_args["version_base"] = "1.1"
    except ValueError:
        pass

    return hydra_args


def is_url(checkpoint):
    """Check whether the checkpoint is a url or not."""
    is_url = "https://storage.googleapis.com" in checkpoint
    return is_url


def get_ptmodel_from_config(model):
    """Get a pre-trained model from the lightly model zoo."""
    key = model["name"]
    key += "/simclr"
    key += "/d" + str(model["num_ftrs"])
    key += "/w" + str(float(model["width"]))

    if key in model_zoo.keys():
        return model_zoo[key], key
    else:
        return "", key


def load_state_dict_from_url(url, map_location=None):
    """Try to load the checkopint from the given url."""
    try:
        state_dict = torch.hub.load_state_dict_from_url(url, map_location=map_location)
        return state_dict
    except Exception:
        print("Not able to load state dict from %s" % (url))
        print("Retrying with http:// prefix")
    try:
        url = url.replace("https", "http")
        state_dict = torch.hub.load_state_dict_from_url(url, map_location=map_location)
        return state_dict
    except Exception:
        print("Not able to load state dict from %s" % (url))

    # in this case downloading the pre-trained model was not possible
    # notify the user and return
    return {"state_dict": None}


def _maybe_expand_batchnorm_weights(model_dict, state_dict, num_splits):
    """Expands the weights of the BatchNorm2d to the size of SplitBatchNorm."""
    running_mean = "running_mean"
    running_var = "running_var"

    for key, item in model_dict.items():
        # not batchnorm -> continue
        if not running_mean in key and not running_var in key:
            continue

        state = state_dict.get(key, None)
        # not in dict -> continue
        if state is None:
            continue
        # same shape -> continue
        if item.shape == state.shape:
            continue

        # found running mean or running var with different shapes
        state_dict[key] = state.repeat(num_splits)

    return state_dict


def _filter_state_dict(state_dict, remove_model_prefix_offset: int = 1):
    """Makes the state_dict compatible with the model.

    Prevents unexpected key error when loading PyTorch-Lightning checkpoints.
    Allows backwards compatability to checkpoints before v1.0.6.

    """

    prev_backbone = "features"
    curr_backbone = "backbone"

    new_state_dict = {}
    for key, item in state_dict.items():
        # remove the "model." prefix from the state dict key
        key_parts = key.split(".")[remove_model_prefix_offset:]
        # with v1.0.6 the backbone of the models will be renamed from
        # "features" to "backbone", ensure compatability with old ckpts
        key_parts = [k if k != prev_backbone else curr_backbone for k in key_parts]

        new_key = ".".join(key_parts)
        new_state_dict[new_key] = item

    return new_state_dict


def _fix_projection_head_keys(state_dict):
    """Makes the state_dict compatible with the refactored projection heads.

    TODO: Remove once the models are refactored and the old checkpoints were
    replaced! Relevant issue: https://github.com/lightly-ai/lightly/issues/379

    Prevents unexpected key error when loading old checkpoints.

    """

    projection_head_identifier = "projection_head"
    prediction_head_identifier = "prediction_head"
    projection_head_insert = "layers"

    new_state_dict = {}
    for key, item in state_dict.items():
        if (
            projection_head_identifier in key or prediction_head_identifier in key
        ) and projection_head_insert not in key:
            # insert layers if it's not part of the key yet
            key_parts = key.split(".")
            key_parts.insert(1, projection_head_insert)
            new_key = ".".join(key_parts)
        else:
            new_key = key

        new_state_dict[new_key] = item

    return new_state_dict


def load_from_state_dict(
    model,
    state_dict,
    strict: bool = True,
    apply_filter: bool = True,
    num_splits: int = 0,
):
    """Loads the model weights from the state dictionary."""

    # step 1: filter state dict
    if apply_filter:
        state_dict = _filter_state_dict(state_dict)

    state_dict = _fix_projection_head_keys(state_dict)

    # step 2: expand batchnorm weights
    state_dict = _maybe_expand_batchnorm_weights(
        model.state_dict(), state_dict, num_splits
    )

    # step 3: load from checkpoint
    model.load_state_dict(state_dict, strict=strict)


def get_model_from_config(cfg, is_cli_call: bool = False) -> SelfSupervisedEmbedding:
    checkpoint = cfg["checkpoint"]
    if torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")

    if not checkpoint:
        checkpoint, key = get_ptmodel_from_config(cfg["model"])
        if not checkpoint:
            msg = "Cannot download checkpoint for key {} ".format(key)
            msg += "because it does not exist!"
            raise RuntimeError(msg)
        state_dict = load_state_dict_from_url(checkpoint, map_location=device)[
            "state_dict"
        ]
    else:
        checkpoint = fix_input_path(checkpoint) if is_cli_call else checkpoint
        state_dict = torch.load(checkpoint, map_location=device)["state_dict"]

    # load model
    resnet = ResNetGenerator(cfg["model"]["name"], cfg["model"]["width"])
    last_conv_channels = list(resnet.children())[-1].in_features
    features = nn.Sequential(
        get_norm_layer(3, 0),
        *list(resnet.children())[:-1],
        nn.Conv2d(last_conv_channels, cfg["model"]["num_ftrs"], 1),
        nn.AdaptiveAvgPool2d(1),
    )

    model = _SimCLR(
        features, num_ftrs=cfg["model"]["num_ftrs"], out_dim=cfg["model"]["out_dim"]
    ).to(device)

    if state_dict is not None:
        load_from_state_dict(model, state_dict)

    encoder = SelfSupervisedEmbedding(model, None, None, None)
    return encoder



================================================
FILE: lightly/cli/crop_cli.py
================================================
# -*- coding: utf-8 -*-
"""**Lightly Train:** Train a self-supervised model from the command-line.

This module contains the entrypoint for the **lightly-crop**
command-line interface.
"""

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved
import os.path
from typing import List

import hydra
import yaml

from lightly.cli._helpers import fix_hydra_arguments, fix_input_path
from lightly.data import LightlyDataset
from lightly.utils.bounding_box import BoundingBox
from lightly.utils.cropping.crop_image_by_bounding_boxes import (
    crop_dataset_by_bounding_boxes_and_save,
)
from lightly.utils.cropping.read_yolo_label_file import read_yolo_label_file
from lightly.utils.hipify import bcolors


def _crop_cli(cfg, is_cli_call=True):
    input_dir = cfg["input_dir"]
    if input_dir and is_cli_call:
        input_dir = fix_input_path(input_dir)
    output_dir = cfg["output_dir"]
    if output_dir and is_cli_call:
        output_dir = fix_input_path(output_dir)
    label_dir = cfg["label_dir"]
    if label_dir and is_cli_call:
        label_dir = fix_input_path(label_dir)
    label_names_file = cfg["label_names_file"]
    if label_names_file and len(label_names_file) > 0:
        if is_cli_call:
            label_names_file = fix_input_path(label_names_file)
        with open(label_names_file, "r") as file:
            label_names_file_dict = yaml.full_load(file)
        class_names = label_names_file_dict["names"]
    else:
        class_names = None

    dataset = LightlyDataset(input_dir)

    class_indices_list_list: List[List[int]] = []
    bounding_boxes_list_list: List[List[BoundingBox]] = []

    # YOLO-Specific
    for filename_image in dataset.get_filenames():
        filepath_image_base, image_extension = os.path.splitext(filename_image)
        filepath_label = os.path.join(label_dir, filename_image).replace(
            image_extension, ".txt"
        )
        class_indices, bounding_boxes = read_yolo_label_file(
            filepath_label, float(cfg["crop_padding"])
        )
        class_indices_list_list.append(class_indices)
        bounding_boxes_list_list.append(bounding_boxes)

    cropped_images_list_list = crop_dataset_by_bounding_boxes_and_save(
        dataset,
        output_dir,
        bounding_boxes_list_list,
        class_indices_list_list,
        class_names,
    )

    print(f"Cropped images are stored at: {bcolors.OKBLUE}{output_dir}{bcolors.ENDC}")
    return cropped_images_list_list


@hydra.main(**fix_hydra_arguments(config_path="config", config_name="config"))
def crop_cli(cfg):
    """Crops images into one sub-image for each object.

    Args:
        cfg:
            The default configs are loaded from the config file.
            To overwrite them please see the section on the config file
            (.config.config.yaml).

    Command-Line Args:
        input_dir:
            Path to the input directory where images are stored.
        labels_dir:
            Path to the directory where the labels are stored. There must be one label file for each image.
            The label file must have the same name as the image file, but the extension .txt.
            For example, img_123.txt for img_123.jpg. The label file must be in YOLO format.
        output_dir:
            Path to the directory where the cropped images are stored. They are stored in one directory per input image.
        crop_padding: Optional
            The additonal padding about the bounding box. This makes the crops include the context of the object.
            The padding is relative and added to the width and height.
        label_names_file: Optional
            A yaml file including the names of the classes. If it is given, the filenames of the cropped images include
            the class names instead of the class id. This file is usually included when having a dataset in yolo format.
            Example contents of such a label_names_file.yaml: "names: ['class_name_a', 'class_name_b']"


    Examples:
        >>> # Crop images and set the crop to be 20% around the bounding box
        >>> lightly-crop input_dir=data/images label_dir=data/labels output_dir=data/cropped_images crop_padding=0.2

        >>> # Crop images and use the class names in the filename
        >>> lightly-crop input_dir=data/images label_dir=data/labels output_dir=data/cropped_images label_names_file=data/data.yaml

    """
    return _crop_cli(cfg)


def entry():
    crop_cli()



================================================
FILE: lightly/cli/download_cli.py
================================================
# -*- coding: utf-8 -*-
"""**Lightly Download:** Download images from the Lightly platform.

This module contains the entrypoint for the **lightly-download**
command-line interface.
"""

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

import os

import hydra

import lightly.data as data
from lightly.api.api_workflow_client import ApiWorkflowClient
from lightly.cli._helpers import cpu_count, fix_hydra_arguments, fix_input_path
from lightly.openapi_generated.swagger_client.models import Creator
from lightly.utils.hipify import bcolors, print_as_warning


def _download_cli(cfg, is_cli_call=True):
    tag_name = str(cfg["tag_name"])
    dataset_id = str(cfg["dataset_id"])
    token = str(cfg["token"])

    if not tag_name or not token or not dataset_id:
        print_as_warning(
            "Please specify all of the parameters tag_name, token and dataset_id"
        )
        print_as_warning("For help, try: lightly-download --help")
        return

    # set the number of workers if unset
    if cfg["loader"]["num_workers"] < 0:
        # set the number of workers to the number of CPUs available,
        # but minimum of 8
        num_workers = max(8, cpu_count())
        num_workers = min(32, num_workers)
        cfg["loader"]["num_workers"] = num_workers

    api_workflow_client = ApiWorkflowClient(
        token=token, dataset_id=dataset_id, creator=Creator.USER_PIP_LIGHTLY_MAGIC
    )

    # get tag id
    tag_data = api_workflow_client.get_tag_by_name(tag_name)
    filenames_tag = api_workflow_client.get_filenames_in_tag(
        tag_data,
        exclude_parent_tag=cfg["exclude_parent_tag"],
    )

    # store sample names in a .txt file
    filename = tag_name + ".txt"
    with open(filename, "w") as f:
        for item in filenames_tag:
            f.write("%s\n" % item)

    filepath = os.path.join(os.getcwd(), filename)
    msg = f'The list of samples in tag {cfg["tag_name"]} is stored at: {bcolors.OKBLUE}{filepath}{bcolors.ENDC}'
    print(msg, flush=True)

    if not cfg["input_dir"] and cfg["output_dir"]:
        # download full images from api
        output_dir = fix_input_path(cfg["output_dir"])
        api_workflow_client.download_dataset(
            output_dir, tag_name=tag_name, max_workers=cfg["loader"]["num_workers"]
        )

    elif cfg["input_dir"] and cfg["output_dir"]:
        input_dir = fix_input_path(cfg["input_dir"])
        output_dir = fix_input_path(cfg["output_dir"])
        print(
            f"Copying files from {input_dir} to {bcolors.OKBLUE}{output_dir}{bcolors.ENDC}."
        )

        # create a dataset from the input directory
        dataset = data.LightlyDataset(input_dir=input_dir)

        # dump the dataset in the output directory
        dataset.dump(output_dir, filenames_tag)


@hydra.main(**fix_hydra_arguments(config_path="config", config_name="config"))
def download_cli(cfg):
    """Download images from the Lightly platform.

    Args:
        cfg:
            The default configs are loaded from the config file.
            To overwrite them please see the section on the config file
            (.config.config.yaml).

    Command-Line Args:
        tag_name:
            Download all images from the requested tag. Use initial-tag
            to get all images from the dataset.
        token:
            User access token to the Lightly platform. If dataset_id
            and token are specified, the images and embeddings are
            uploaded to the platform.
        dataset_id:
            Identifier of the dataset on the Lightly platform. If
            dataset_id and token are specified, the images and
            embeddings are uploaded to the platform.
        input_dir:
            If input_dir and output_dir are specified, lightly will copy
            all images belonging to the tag from the input_dir to the
            output_dir.
        output_dir:
            If input_dir and output_dir are specified, lightly will copy
            all images belonging to the tag from the input_dir to the
            output_dir.

    Examples:
        >>> # download list of all files in the dataset from the Lightly platform
        >>> lightly-download token='123' dataset_id='XYZ'
        >>>
        >>> # download list of all files in tag 'my-tag' from the Lightly platform
        >>> lightly-download token='123' dataset_id='XYZ' tag_name='my-tag'
        >>>
        >>> # download all images in tag 'my-tag' from the Lightly platform
        >>> lightly-download token='123' dataset_id='XYZ' tag_name='my-tag' output_dir='my_data/'
        >>>
        >>> # copy all files in 'my-tag' to a new directory
        >>> lightly-download token='123' dataset_id='XYZ' tag_name='my-tag' input_dir='data/' output_dir='my_data/'


    """
    _download_cli(cfg)


def entry():
    download_cli()



================================================
FILE: lightly/cli/embed_cli.py
================================================
# -*- coding: utf-8 -*-
"""**Lightly Embed:** Embed images with one command.

This module contains the entrypoint for the **lightly-embed**
command-line interface.
"""

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

import os
from typing import List, Tuple, Union

import hydra
import numpy as np
import torch
import torchvision

from lightly.cli._helpers import (
    cpu_count,
    fix_hydra_arguments,
    fix_input_path,
    get_model_from_config,
)
from lightly.data import LightlyDataset
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.utils.hipify import bcolors
from lightly.utils.io import save_embeddings


def _embed_cli(
    cfg, is_cli_call=True
) -> Union[Tuple[np.ndarray, List[int], List[str]], str]:
    """See embed_cli() for usage documentation

    is_cli_call:
        If True:
            Saves the embeddings as file and returns the filepath.
        If False:
            Returns the embeddings, labels, filenames as tuple.
            Embeddings are of shape (n_samples, embedding_size)
            len(labels) = len(filenames) = n_samples
    """
    input_dir = cfg["input_dir"]
    if input_dir and is_cli_call:
        input_dir = fix_input_path(input_dir)

    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    if torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")

    transform = T.Compose(
        [
            T.Resize((cfg["collate"]["input_size"], cfg["collate"]["input_size"])),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )

    dataset = LightlyDataset(input_dir, transform=transform)

    # disable drop_last and shuffle
    cfg["loader"]["drop_last"] = False
    cfg["loader"]["shuffle"] = False
    cfg["loader"]["batch_size"] = min(cfg["loader"]["batch_size"], len(dataset))

    # determine the number of available cores
    if cfg["loader"]["num_workers"] < 0:
        cfg["loader"]["num_workers"] = cpu_count()

    dataloader = torch.utils.data.DataLoader(dataset, **cfg["loader"])

    encoder = get_model_from_config(cfg, is_cli_call)

    embeddings, labels, filenames = encoder.embed(dataloader, device=device)

    if is_cli_call:
        path = os.path.join(os.getcwd(), "embeddings.csv")
        save_embeddings(path, embeddings, labels, filenames)
        print(f"Embeddings are stored at {bcolors.OKBLUE}{path}{bcolors.ENDC}")
        os.environ[
            cfg["environment_variable_names"]["lightly_last_embedding_path"]
        ] = path
        return path

    return embeddings, labels, filenames


@hydra.main(**fix_hydra_arguments(config_path="config", config_name="config"))
def embed_cli(cfg) -> str:
    """Embed images from the command-line.

    Args:
        cfg:
            The default configs are loaded from the config file.
            To overwrite them please see the section on the config file
            (.config.config.yaml).

    Command-Line Args:
        input_dir:
            Path to the input directory where images are stored.
        checkpoint:
            Path to the checkpoint of a pretrained model. If left
            empty, a pretrained model by lightly is used.

    Returns:
        The path to the created embeddings file.

    Examples:
        >>> # embed images with default settings and a lightly model
        >>> lightly-embed input_dir=data/
        >>>
        >>> # embed images with default settings and a custom checkpoint
        >>> lightly-embed input_dir=data/ checkpoint=my_checkpoint.ckpt
        >>>
        >>> # embed images with custom settings
        >>> lightly-embed input_dir=data/ model.num_ftrs=32

    """
    return _embed_cli(cfg)


def entry():
    embed_cli()



================================================
FILE: lightly/cli/lightly_cli.py
================================================
# -*- coding: utf-8 -*-
"""**Lightly Magic:** Train and embed in one command.

This module contains the entrypoint for the **lightly-magic**
command-line interface.
"""

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

import hydra
from omegaconf import DictConfig

from lightly.cli._helpers import fix_hydra_arguments
from lightly.cli.embed_cli import _embed_cli
from lightly.cli.train_cli import _train_cli
from lightly.utils.hipify import print_as_warning


def _lightly_cli(cfg, is_cli_call=True):
    cfg["loader"]["shuffle"] = True
    cfg["loader"]["drop_last"] = True

    if cfg["trainer"]["max_epochs"] > 0:
        print("#" * 10 + " Starting to train an embedding model.")
        checkpoint = _train_cli(cfg, is_cli_call)
    else:
        checkpoint = ""

    cfg["loader"]["shuffle"] = False
    cfg["loader"]["drop_last"] = False
    cfg["checkpoint"] = checkpoint

    print("#" * 10 + " Starting to embed your dataset.")
    embeddings = _embed_cli(cfg, is_cli_call)
    cfg["embeddings"] = embeddings

    print("#" * 10 + " Finished")


@hydra.main(**fix_hydra_arguments(config_path="config", config_name="config"))
def lightly_cli(cfg):
    """Train a self-supervised model and use it to embed your dataset.

    Args:
        cfg:
            The default configs are loaded from the config file.
            To overwrite them please see the section on the config file
            (.config.config.yaml).

    Command-Line Args:
        input_dir:
            Path to the input directory where images are stored.

    Examples:
        >>> # train model and embed images with default settings
        >>> lightly-magic input_dir=data/
        >>>
        >>> # train model for 10 epochs and embed images
        >>> lightly-magic input_dir=data/ trainer.max_epochs=10


    """
    return _lightly_cli(cfg)


def entry():
    lightly_cli()



================================================
FILE: lightly/cli/serve_cli.py
================================================
import ssl
import sys
from pathlib import Path

import hydra

from lightly.api import serve
from lightly.api.serve import validate_input_mount, validate_lightly_mount
from lightly.cli._helpers import fix_hydra_arguments
from lightly.utils.hipify import bcolors


@hydra.main(**fix_hydra_arguments(config_path="config", config_name="lightly-serve"))
def lightly_serve(cfg):
    """Use lightly-serve to serve your data for interactive exploration.

    Command-Line Args:
        input_mount:
            Path to the input directory.
        lightly_mount:
            Path to the Lightly directory.
        host:
            Hostname for serving the data (defaults to localhost). If you want to expose it to the internet or your local network, use '0.0.0.0'.
            See our docs on lightly-serve for more information: https://docs.lightly.ai/docs/local-storage#view-the-local-data-securely-over-the-networkvpn
        port:
            Port for serving the data (defaults to 3456).
        ssl_key:
            Optional path to the ssl key file.
        ssl_cert:
            Optional path to the ssl cert file.

    Examples:
        >>> lightly-serve input_mount=data/ lightly_mount=lightly/ port=3456


    """
    if not cfg.input_mount:
        print(
            "Please provide a valid 'input_mount' argument. Use --help for more "
            "information."
        )
        sys.exit(1)

    if not cfg.lightly_mount:
        print(
            "Please provide a valid 'lightly_mount' argument. Use --help for more "
            "information."
        )
        sys.exit(1)

    input_mount = Path(cfg.input_mount)
    validate_input_mount(input_mount=input_mount)
    lightly_mount = Path(cfg.lightly_mount)
    validate_lightly_mount(lightly_mount=lightly_mount)

    httpd = serve.get_server(
        paths=[input_mount, lightly_mount],
        host=cfg.host,
        port=cfg.port,
    )

    # setup https/ssl if key or cert are provided
    if cfg.ssl_key or cfg.ssl_cert:
        httpd.socket = ssl.wrap_socket(
            httpd.socket,
            keyfile=Path(cfg.ssl_key) if cfg.ssl_key else None,
            certfile=Path(cfg.ssl_cert) if cfg.ssl_cert else None,
            server_side=True,
        )

    print(
        f"Starting server, listening at '{bcolors.OKBLUE}{httpd.server_name}:{httpd.server_port}{bcolors.ENDC}'"
    )
    print(
        f"Serving files in '{bcolors.OKBLUE}{cfg.input_mount}{bcolors.ENDC}' and '{bcolors.OKBLUE}{cfg.lightly_mount}{bcolors.ENDC}'"
    )
    print(
        f"Please follow our docs if you are facing any issues: https://docs.lightly.ai/docs/local-storage#optional-after-run-view-local-data-in-lightly-platform"
    )
    try:
        httpd.serve_forever()
    finally:
        httpd.server_close()


def entry() -> None:
    lightly_serve()



================================================
FILE: lightly/cli/train_cli.py
================================================
# -*- coding: utf-8 -*-
"""**Lightly SSL Train:** Train a self-supervised model from the command-line.

This module contains the entrypoint for the **lightly-ssl-train**
command-line interface.
"""

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved
import copy
import os
import warnings

import hydra
import torch
import torch.nn as nn
from omegaconf import OmegaConf

from lightly.cli._cli_simclr import _SimCLR
from lightly.cli._helpers import (
    cpu_count,
    fix_hydra_arguments,
    fix_input_path,
    get_ptmodel_from_config,
    is_url,
    load_from_state_dict,
    load_state_dict_from_url,
)
from lightly.data import ImageCollateFunction, LightlyDataset
from lightly.embedding import SelfSupervisedEmbedding
from lightly.loss import NTXentLoss
from lightly.models import ResNetGenerator
from lightly.models.batchnorm import get_norm_layer
from lightly.utils.hipify import bcolors


def _train_cli(cfg, is_cli_call=True):
    input_dir = cfg["input_dir"]
    if input_dir and is_cli_call:
        input_dir = fix_input_path(input_dir)

    if "seed" in cfg.keys():
        seed = cfg["seed"]
        torch.manual_seed(seed)
        torch.cuda.manual_seed(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    accelerator = "gpu" if torch.cuda.is_available() else "cpu"
    if accelerator == "gpu" and cfg["trainer"]["gpus"] > 1:
        devices = cfg["trainer"]["gpus"]
        strategy = "ddp"
    else:
        devices = 1
        strategy = None

    if cfg["loader"]["batch_size"] < 64:
        msg = "Training a self-supervised model with a small batch size: {}! "
        msg = msg.format(cfg["loader"]["batch_size"])
        msg += "Small batch size may harm embedding quality. "
        msg += "You can specify the batch size via the loader key-word: "
        msg += "loader.batch_size=BSZ"
        warnings.warn(msg)

    # determine the number of available cores
    if cfg["loader"]["num_workers"] < 0:
        cfg["loader"]["num_workers"] = cpu_count()

    state_dict = None
    checkpoint = cfg["checkpoint"]
    if cfg["pre_trained"] and not checkpoint:
        # if checkpoint wasn't specified explicitly and pre_trained is True
        # try to load the checkpoint from the model zoo
        checkpoint, key = get_ptmodel_from_config(cfg["model"])
        if not checkpoint:
            msg = "Cannot download checkpoint for key {} ".format(key)
            msg += "because it does not exist! "
            msg += "Model will be trained from scratch."
            warnings.warn(msg)
    elif checkpoint:
        checkpoint = fix_input_path(checkpoint) if is_cli_call else checkpoint

    if checkpoint:
        # load the PyTorch state dictionary
        if is_url(checkpoint):
            state_dict = load_state_dict_from_url(checkpoint, map_location="cpu")[
                "state_dict"
            ]
        else:
            state_dict = torch.load(checkpoint, map_location="cpu")["state_dict"]

    # load model
    resnet = ResNetGenerator(cfg["model"]["name"], cfg["model"]["width"])
    last_conv_channels = list(resnet.children())[-1].in_features
    features = nn.Sequential(
        get_norm_layer(3, 0),
        *list(resnet.children())[:-1],
        nn.Conv2d(last_conv_channels, cfg["model"]["num_ftrs"], 1),
        nn.AdaptiveAvgPool2d(1),
    )

    model = _SimCLR(
        features, num_ftrs=cfg["model"]["num_ftrs"], out_dim=cfg["model"]["out_dim"]
    )
    if state_dict is not None:
        load_from_state_dict(model, state_dict)

    criterion = NTXentLoss(**cfg["criterion"])
    optimizer = torch.optim.SGD(model.parameters(), **cfg["optimizer"])

    dataset = LightlyDataset(input_dir)

    cfg["loader"]["batch_size"] = min(cfg["loader"]["batch_size"], len(dataset))

    collate_fn = ImageCollateFunction(**cfg["collate"])
    dataloader = torch.utils.data.DataLoader(
        dataset, **cfg["loader"], collate_fn=collate_fn
    )

    encoder = SelfSupervisedEmbedding(model, criterion, optimizer, dataloader)

    # Create trainer config
    if isinstance(cfg, dict):
        trainer_kwargs = copy.deepcopy(cfg["trainer"])
    else:
        trainer_kwargs = OmegaConf.to_container(cfg["trainer"])
    if "gpus" in trainer_kwargs:
        # PyTorch Lightning >= 2.0 doesn't support the gpus trainer flag anymore.
        # We have to use accelerator and devices instead.
        del trainer_kwargs["gpus"]
    trainer_kwargs["accelerator"] = accelerator
    trainer_kwargs["devices"] = devices
    if strategy is not None:
        # Only add strategy if it is set because PyTorch Lightning used by default
        # strategy = None for version < 2.0 and strategy = "auto" for >= 2.0. None is
        # not supported for >= 2.0 and "auto" not supported for some versions < 2.0.
        trainer_kwargs["strategy"] = strategy
    trainer_config = OmegaConf.create(trainer_kwargs)

    encoder.train_embedding(
        trainer_config=trainer_config,
        checkpoint_callback_config=cfg["checkpoint_callback"],
        summary_callback_config=cfg["summary_callback"],
    )

    print(
        f"Best model is stored at: {bcolors.OKBLUE}{encoder.checkpoint}{bcolors.ENDC}"
    )
    os.environ[
        cfg["environment_variable_names"]["lightly_last_checkpoint_path"]
    ] = encoder.checkpoint
    return encoder.checkpoint


@hydra.main(**fix_hydra_arguments(config_path="config", config_name="config"))
def train_cli(cfg):
    """Train a self-supervised model from the command-line.

    Args:
        cfg:
            The default configs are loaded from the config file.
            To overwrite them please see the section on the config file
            (.config.config.yaml).

    Command-Line Args:
        input_dir:
            Path to the input directory where images are stored.

    Examples:
        >>> # train model with default settings
        >>> lightly-ssl-train input_dir=data/
        >>>
        >>> # train model with batches of size 128
        >>> lightly-ssl-train input_dir=data/ loader.batch_size=128
        >>>
        >>> # train model for 10 epochs
        >>> lightly-ssl-train input_dir=data/ trainer.max_epochs=10
        >>>
        >>> # print a full summary of the model
        >>> lightly-ssl-train input_dir=data/ trainer.weights_summary=full

    """
    return _train_cli(cfg)


def entry():
    train_cli()



================================================
FILE: lightly/cli/version_cli.py
================================================
# -*- coding: utf-8 -*-
"""**Lightly Version:** Show the version of the installed package.

Example:
    >>> # show the version of the installed package
    >>> lightly-version
"""

# Copyright (c) 2021. Lightly AG and its affiliates.
# All Rights Reserved

import hydra

import lightly
from lightly.cli._helpers import fix_hydra_arguments


def _version_cli():
    version = lightly.__version__
    print(f"lightly version {version}", flush=True)


@hydra.main(**fix_hydra_arguments(config_path="config", config_name="config"))
def version_cli(cfg):
    """Prints the version of the used lightly package to the terminal."""
    _version_cli()


def entry():
    version_cli()



================================================
FILE: lightly/cli/config/__init__.py
================================================
""" lightly.cli.config

    The lightly.cli.config module holds default configs for the command-line
    interface.
"""

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved



================================================
FILE: lightly/cli/config/get_config.py
================================================
from pathlib import Path

from omegaconf import DictConfig, OmegaConf


def get_lightly_config() -> DictConfig:
    config_path = Path(__file__).with_name("config.yaml")
    conf = OmegaConf.load(config_path)
    # TODO(Huan, 05.04.2023): remove this when hydra is completely dropped
    if conf.get("hydra"):
        # This config entry is only for hydra; not referenced in any logic
        del conf["hydra"]
    return conf



================================================
FILE: lightly/data/__init__.py
================================================
"""The lightly.data module provides a dataset wrapper and collate functions. """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

from lightly.data._video import (
    EmptyVideoError,
    NonIncreasingTimestampError,
    UnseekableTimestampError,
    VideoError,
)
from lightly.data.collate import (
    BaseCollateFunction,
    DINOCollateFunction,
    ImageCollateFunction,
    MAECollateFunction,
    MoCoCollateFunction,
    MSNCollateFunction,
    MultiCropCollateFunction,
    PIRLCollateFunction,
    SimCLRCollateFunction,
    SwaVCollateFunction,
    VICRegLCollateFunction,
    imagenet_normalize,
)
from lightly.data.dataset import LightlyDataset



================================================
FILE: lightly/data/_helpers.py
================================================
""" Helper Functions """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved
from __future__ import annotations

import os
from typing import Any, Callable, Dict, Optional, Tuple

from torchvision import datasets

from lightly.data._image import DatasetFolder

try:
    from lightly.data._video import VideoDataset

    VIDEO_DATASET_AVAILABLE = True
except Exception as e:
    VIDEO_DATASET_AVAILABLE = False
    VIDEO_DATASET_ERRORMSG = e


IMG_EXTENSIONS = (
    ".jpg",
    ".jpeg",
    ".png",
    ".ppm",
    ".bmp",
    ".pgm",
    ".tif",
    ".tiff",
    ".webp",
)

VIDEO_EXTENSIONS = (".mp4", ".mov", ".avi", ".mpg", ".hevc", ".m4v", ".webm", ".mpeg")


def _dir_contains_videos(root: str, extensions: tuple[str, ...]) -> bool:
    """Checks whether the directory contains video files.

    Args:
        root: Root directory path.
        extensions: Tuple of valid video file extensions.

    Returns:
        True if the root directory contains video files, False otherwise.
    """
    with os.scandir(root) as scan_dir:
        return any(f.name.lower().endswith(extensions) for f in scan_dir)


def _contains_videos(root: str, extensions: tuple[str, ...]) -> bool:
    """Checks whether the directory or any subdirectory contains video files.

    Args:
        root: Root directory path.
        extensions: Tuple of valid video file extensions.

    Returns:
        True if the root directory or any subdirectory contains video files, False otherwise.
    """
    for subdir, _, _ in os.walk(root):
        if _dir_contains_videos(subdir, extensions):
            return True
    return False


def _is_lightly_output_dir(dirname: str) -> bool:
    """Checks whether the directory is a lightly_output directory.

    Args:
        dirname: Directory name to check.

    Returns:
        True if the directory name is "lightly_outputs", False otherwise.
    """
    return "lightly_outputs" in dirname


def _contains_subdirs(root: str) -> bool:
    """Checks whether the directory contains subdirectories.

    Args:
        root: Root directory path.

    Returns:
        True if the root directory contains subdirectories (excluding "lightly_outputs"), False otherwise.
    """
    with os.scandir(root) as scan_dir:
        return any(not _is_lightly_output_dir(f.name) for f in scan_dir if f.is_dir())


def _load_dataset_from_folder(
    root: str,
    transform: Callable[[Any], Any],
    is_valid_file: Callable[[str], bool] | None,
    tqdm_args: dict[str, Any] | None,
    num_workers_video_frame_counting: int = 0,
) -> datasets.VisionDataset:
    """Initializes a dataset from a folder.

    This function determines the appropriate dataset type based on the contents of the root directory
    and returns the corresponding dataset object.

    Args:
        root: Root directory path.
        transform: Composed image transformations to be applied to the dataset.
        is_valid_file: Optional function to determine valid files.
        tqdm_args: Optional dictionary of arguments for tqdm progress bar.
        num_workers_video_frame_counting: Number of workers for video frame counting.

    Returns:
        A dataset object (VideoDataset, ImageFolder, or DatasetFolder) based on the directory contents.

    Raises:
        ValueError: If the specified dataset directory doesn't exist or if videos are present
                but VideoDataset is not available.
    """
    if not os.path.exists(root):
        raise ValueError(f"The input directory {root} does not exist!")

    contains_videos = _contains_videos(root, VIDEO_EXTENSIONS)
    if contains_videos and not VIDEO_DATASET_AVAILABLE:
        raise ValueError(
            f"The input directory {root} contains videos "
            "but the VideoDataset is not available. "
            "Make sure you have installed the right "
            "dependencies. The error from the imported "
            f"module was: {VIDEO_DATASET_ERRORMSG}"
        )

    if contains_videos:
        return VideoDataset(
            root,
            extensions=VIDEO_EXTENSIONS,
            transform=transform,
            is_valid_file=is_valid_file,
            tqdm_args=tqdm_args,
            num_workers=num_workers_video_frame_counting,
        )
    elif _contains_subdirs(root):
        return datasets.ImageFolder(
            root, transform=transform, is_valid_file=is_valid_file
        )
    else:
        return DatasetFolder(
            root,
            extensions=IMG_EXTENSIONS,
            transform=transform,
            is_valid_file=is_valid_file,
        )



================================================
FILE: lightly/data/_image.py
================================================
""" Image Dataset """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

import os
from typing import Any, Callable, List, Optional, Set, Tuple, Union

import torch
import torchvision.datasets as datasets
from typing_extensions import Protocol

from lightly.data._image_loaders import default_loader


class DatasetFolder(datasets.VisionDataset):  # type: ignore
    """Implements a dataset folder.

    DatasetFolder based on torchvisions implementation.
    (https://pytorch.org/docs/stable/torchvision/datasets.html#datasetfolder)

    Attributes:
        root:
            Root directory path
        loader:
            Function that loads file at path
        extensions:
            Tuple of allowed extensions
        transform:
            Function that takes a PIL image and returns transformed version
        target_transform:
            As transform but for targets
        is_valid_file:
            Used to check corrupt files

    Raises:
        RuntimeError: If no supported files are found in root.

    """

    def __init__(
        self,
        root: str,
        loader: Callable[[str], Any] = default_loader,
        extensions: Optional[Tuple[str, ...]] = None,
        transform: Optional[Callable[[Any], Any]] = None,
        target_transform: Optional[Callable[[Any], Any]] = None,
        is_valid_file: Optional[Callable[[str], bool]] = None,
    ):
        """Initialize a DatasetFolder dataset.

        Args:
            root:
                Path to the root directory containing image files.
            loader:
                A function to load an image from a file path. Defaults to default_loader.
            extensions:
                A tuple of allowed file extensions. If None, is_valid_file must be provided.
            transform:
                Optional transform to be applied to the input image.
            target_transform:
                Optional transform to be applied to the target.
            is_valid_file:
                Optional function to validate file paths. If None and extensions is None,
                raises a ValueError.
        """
        super().__init__(root, transform=transform, target_transform=target_transform)

        samples = _make_dataset(self.root, extensions, is_valid_file)
        if len(samples) == 0:
            msg = "Found 0 files in folder: {}\n".format(self.root)
            if extensions is not None:
                msg += "Supported extensions are: {}".format(",".join(extensions))
            raise RuntimeError(msg)

        self.loader = loader
        self.extensions = extensions

        self.samples = samples
        self.targets = [s[1] for s in samples]

    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:
        """Retrieve a sample from the dataset.

        Args:
            index:
                Index of the sample to retrieve.

        Returns:
            A tuple containing the image sample and its target (always 0 in this implementation).
        """
        path, target = self.samples[index]
        sample = self.loader(path)
        if self.transform is not None:
            sample = self.transform(sample)
        if self.target_transform is not None:
            target = self.target_transform(target)

        return sample, target

    def __len__(self) -> int:
        """Get the total number of samples in the dataset.

        Returns:
            Total count of samples in the dataset.
        """
        return len(self.samples)


def _make_dataset(
    directory: str,
    extensions: Optional[Tuple[str, ...]] = None,
    is_valid_file: Optional[Callable[[str], bool]] = None,
) -> List[Tuple[str, int]]:
    """Create a list of valid image files in the given directory.

    Args:
        directory:
            Root directory path containing image files (should not contain subdirectories).
        extensions:
            Tuple of valid file extensions. If None, is_valid_file must be used.
        is_valid_file:
            Optional function to validate file paths beyond extension checking.

    Returns:
        A list of tuples, where each tuple contains:
        - Full path to an image file
        - Target label (always 0 in this implementation)

    Raises:
        ValueError: If both extensions and is_valid_file are None.
    """
    if extensions is None:
        if is_valid_file is None:
            raise ValueError("Both extensions and is_valid_file cannot be None")
        _is_valid_file = is_valid_file
    else:

        def is_valid_file_extension(filepath: str) -> bool:
            return filepath.lower().endswith(extensions)

        if is_valid_file is None:
            _is_valid_file = is_valid_file_extension
        else:

            def _is_valid_file(filepath: str) -> bool:
                return is_valid_file_extension(filepath) and is_valid_file(filepath)

    instances: List[Tuple[str, int]] = []
    for f in os.scandir(directory):
        if not _is_valid_file(f.path):
            continue

        # convention: the label of all images is 0, based on the fact that
        # they are all in the same directory
        item = (f.path, 0)
        instances.append(item)

    return sorted(instances, key=lambda x: x[0])  # sort by path



================================================
FILE: lightly/data/_image_loaders.py
================================================
"""Module for handling image loading in torchvision-compatible format.

This module provides image loading functionality similar to torchvision's implementation
(see https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html)
"""

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

from PIL import Image


def pil_loader(path: str) -> Image.Image:
    """Loads an image using PIL.

    Args:
        path: Path to the image file.

    Returns:
        A PIL Image in RGB format.
    """
    # open path as file to avoid ResourceWarning
    # (https://github.com/python-pillow/Pillow/issues/835)
    with open(path, "rb") as f:
        img = Image.open(f)
        return img.convert("RGB")


def accimage_loader(path: str) -> Image.Image:
    """Loads an image using the accimage library for faster loading.

    Falls back to PIL loader if accimage fails to load the image.

    Args:
        path: Path to the image file.

    Returns:
        An image loaded either by accimage or PIL in case of failure.
    """
    try:
        import accimage

        return accimage.Image(path)
    except IOError:
        # Potentially a decoding problem, fall back to PIL.Image
        return pil_loader(path)


def default_loader(path: str) -> Image.Image:
    """Loads an image using the default backend specified in torchvision.

    Uses accimage if available and configured as the backend, otherwise falls back to PIL.

    Args:
        path: Path to the image file.

    Returns:
        An image loaded by either accimage or PIL depending on the backend.
    """
    from torchvision import get_image_backend

    if get_image_backend() == "accimage":
        return accimage_loader(path)
    else:
        return pil_loader(path)



================================================
FILE: lightly/data/_utils.py
================================================
"""Provides functionality to identify corrupt images in a directory.

This module helps users identify corrupt or unreadable image files within a specified
directory. It uses parallel processing to efficiently scan through large collections
of images.
"""

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved
from __future__ import annotations

import os

import tqdm.contrib.concurrent as concurrent
from PIL import Image, UnidentifiedImageError

from lightly.data import LightlyDataset


def check_images(data_dir: str) -> tuple[list[str], list[str]]:
    """Identifies corrupt and healthy images in the specified directory.

    The function attempts to open each image file in the directory to verify
    its integrity. It processes images in parallel for better performance.

    Args:
        data_dir: Directory path containing the image files to check.

    Returns:
        A tuple containing two lists:
            - List of filenames of healthy images that can be opened successfully
            - List of filenames of corrupt images that cannot be opened

    Example:
        >>> healthy, corrupt = check_images("path/to/images")
        >>> print(f"Found {len(corrupt)} corrupt images")
    """
    dataset = LightlyDataset(input_dir=data_dir)
    filenames = dataset.get_filenames()

    def _is_corrupt(filename: str) -> bool:
        """Checks if a single image file is corrupt.

        Args:
            filename: Name of the image file to check.

        Returns:
            True if the image is corrupt, False otherwise.
        """
        try:
            image = Image.open(os.path.join(data_dir, filename))
            image.load()
        except (IOError, UnidentifiedImageError):
            return True
        else:
            return False

    mapped = concurrent.thread_map(
        _is_corrupt, filenames, chunksize=min(32, len(filenames))
    )
    healthy_images = [f for f, is_corrupt in zip(filenames, mapped) if not is_corrupt]
    corrupt_images = [f for f, is_corrupt in zip(filenames, mapped) if is_corrupt]
    return healthy_images, corrupt_images



================================================
FILE: lightly/data/_video.py
================================================
""" Video Dataset """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

import os
import threading
import warnings
import weakref
from fractions import Fraction
from typing import Any, Dict, List, Tuple

import numpy as np
import torch
import torchvision
from PIL import Image
from torch.utils.data import DataLoader, Dataset
from torchvision import datasets, io
from tqdm import tqdm

try:
    import av

    AV_AVAILABLE = True
except ImportError:
    AV_AVAILABLE = False

if io._HAS_VIDEO_OPT:
    torchvision.set_video_backend("video_reader")


class VideoError(Exception):
    """Base exception class for errors during video loading."""

    pass


class EmptyVideoError(VideoError):
    """Exception raised when trying to load a frame from an empty video."""

    pass


class FrameShapeError(VideoError):
    """Exception raised when the loaded frame has an unexpected shape."""

    pass


class NonIncreasingTimestampError(VideoError):
    """Exception raised when trying to load a frame that has a timestamp
    equal or lower than the timestamps of previous frames in the video.
    """

    pass


class UnseekableTimestampError(VideoError):
    """Exception raised when trying to load a frame that has a timestamp which
    cannot be seeked to by the video loader.
    """

    pass


# @guarin 18.02.2022
# VideoLoader and VideoDataset multi-thread and multi-processing infos
# --------------------------------------------------------------------
# The VideoDataset class should be safe to use in multi-thread and
# multi-processing settings. For the multi-processing setting it is assumed that
# a pytorch DataLoader is used. Multi-threading should not be use with the
# torchvision pyav video packend as pyav seems to be limited to a single thread.
# You will not see any speedups when using it from multiple threads!
#
# The VideoLoader class is thread safe because it inherits from threading.local.
# When using it within a pytorch DataLoader a new instance should be created
# in each process when using the torchvision video_reader backend, otherwise
# decoder errors can happen when iterating multiple times over the dataloader.
# This is specific to the video_reader backend and does not happen with the pyav
# backend.
#
# In the VideoDataset class we avoid sharing VideoLoader instances between
# workers by tracking the worker accessing the dataset. VideoLoaders are reset
# if a new worker accesses the dataset. Note that changes to the dataset class
# by a worker are unique to that worker and not seen by other workers or the
# main process.


class VideoLoader(threading.local):
    """Implementation of VideoLoader.

    The VideoLoader is a wrapper around the torchvision video interface. With
    the VideoLoader you can read specific frames or the next frames of a video.
    It automatically switches to the `video_loader` backend if available. Reading
    sequential frames is significantly faster since it uses the VideoReader
    class from torchvision.

    The video loader automatically detects if you read out subsequent frames and
    will use the fast read method if possible.

    Attributes:
        path:
            Root directory path.
        timestamps:
            Function that loads file at path.
        backend:
            Tuple of allowed extensions.
        transform:
            Function that takes a PIL image and returns transformed version
        target_transform:
            As transform but for targets
        is_valid_file:
            Used to check corrupt files
        eps:
            Small value to account for floating point imprecisions.

    Examples:
        >>> from torchvision import io
        >>>
        >>> # get timestamps
        >>> ts, fps = io.read_video_timestamps('myvideo.mp4', pts_unit = 'sec')
        >>>
        >>> # create a VideoLoader
        >>> video_loader = VideoLoader('myvideo.mp4', ts)
        >>>
        >>> # get frame at specific timestamp
        >>> frame = video_loader.read_frame(ts[21])
        >>>
        >>> # get next frame
        >>> frame = video_loader.read_frame()
    """

    def __init__(
        self,
        path: str,
        timestamps: List[float],
        backend: str = "video_reader",
        eps: float = 1e-6,
    ):
        self.path = path
        self.timestamps = timestamps
        self.current_index = None
        self.pts_unit = "sec"
        self.backend = backend
        self.eps = eps

        has_video_reader = io._HAS_VIDEO_OPT and hasattr(io, "VideoReader")

        if has_video_reader and self.backend == "video_reader":
            self.reader = io.VideoReader(path=self.path)
        else:
            self.reader = None

    def read_frame(self, timestamp=None):
        """Reads the next frame or from timestamp.

        If no timestamp is provided this method just returns the next frame from
        the video. This is significantly (up to 10x) faster if the `video_loader`
        backend is available. If a timestamp is provided we first have to seek
        to the right position and then load the frame.

        Args:
            timestamp:
                Specific timestamp of frame in seconds or None (default: None)

        Returns:
            A PIL Image

        Raises:
            StopIteration:
                If end of video is reached and timestamp is None.
            ValueError:
                If provided timestamp is not in self.timestamps.
            VideoError:
                If the frame could not be loaded.

        """
        if not self.timestamps:
            raise EmptyVideoError(f"Cannot load frame from empty video {self.path}.")

        if timestamp is None:
            # Try to read next frame.
            if self.current_index is None:
                # Beginning of video.
                index = 0
                timestamp = self.timestamps[index]
            elif self.current_index >= len(self.timestamps):
                # Reached end of video.
                raise StopIteration()
            else:
                # Read next frame.
                index = self.current_index + 1
                timestamp = self.timestamps[index]
        elif (
            self.current_index is not None
            and self.current_index + 1 < len(self.timestamps)
            and timestamp == self.timestamps[self.current_index + 1]
        ):
            # Provided timestamp is timestamp of next frame.
            index = self.current_index + 1
        else:
            # Random timestamp, must find corresponding index.
            index = self.timestamps.index(timestamp)

        if self.reader:
            # Only seek if we cannot just call next(self.reader).
            if (
                self.current_index is None
                and index != 0
                or self.current_index is not None
                and index != self.current_index + 1
            ):
                self.reader.seek(timestamp)

            # Find next larger timestamp than the one we seek. Used to verify
            # that we did not seek too far in the video and that the correct
            # frame is returned.
            if index + 1 < len(self.timestamps):
                try:
                    next_timestamp = next(
                        ts for ts in self.timestamps[index + 1 :] if ts > timestamp
                    )
                except StopIteration:
                    # All timestamps of future frames are smaller.
                    next_timestamp = float("inf")
            else:
                # Want to load last frame in video.
                next_timestamp = float("inf")

            # Load the frame.
            try:
                while True:
                    frame_info = next(self.reader)
                    if frame_info["pts"] < timestamp - self.eps:
                        # Did not read far enough, let's continue reading more
                        # frames. This can happen due to decreasing timestamps.
                        frame_info = next(self.reader)
                    elif frame_info["pts"] >= next_timestamp:
                        # Accidentally read too far, let's seek back to the
                        # correct position and exit. This can happen due to
                        # imprecise seek.
                        self.reader.seek(timestamp)
                        frame_info = next(self.reader)
                        break
                    else:
                        break
            except StopIteration:
                # Accidentally reached the end of the video, let's seek back to
                # the correction position. This can happen due to imprecise seek.
                self.reader.seek(timestamp)
                try:
                    frame_info = next(self.reader)
                except StopIteration as ex:
                    # Seeking to this timestamp simply doesn't work.
                    raise UnseekableTimestampError(
                        f"Cannot seek to frame with timestamp {float(timestamp)} "
                        f"in {self.path}."
                    ) from ex

            if (
                frame_info["pts"] < timestamp - self.eps
                or frame_info["pts"] >= next_timestamp
            ):
                # We accidentally loaded the wrong frame. This should only
                # happen if self.reader.seek(timestamp) does not seek to the
                # correct timestamp. In this case there is nothing we can do to
                # load the correct frame and we alert the user that something
                # went wrong.
                warnings.warn(
                    f"Loaded wrong frame in {self.path}! Tried to load frame "
                    f"with index {index} and timestamp {float(timestamp)} but "
                    f'could only find frame with timestamp {frame_info["pts"]}.'
                )

            # Make sure we have the tensor in correct shape (we want H x W x C)
            frame = frame_info["data"].permute(1, 2, 0)
            self.current_index = index

        else:  # fallback on pyav
            frame, _, _ = io.read_video(
                self.path,
                start_pts=timestamp,
                end_pts=timestamp,
                pts_unit=self.pts_unit,
            )
            self.current_index = index

        if len(frame.shape) < 3:
            raise FrameShapeError(
                f"Loaded frame has unexpected shape {frame.shape}. "
                f"Frames are expected to have 3 dimensions: (H, W, C)."
            )

        # sometimes torchvision returns multiple frames for one timestamp (bug?)
        if len(frame.shape) > 3 and frame.shape[0] > 1:
            frame = frame[0]

        # make sure we return a H x W x C tensor and not (1 x H x W x C)
        if len(frame.shape) == 4:
            frame = frame.squeeze()

        # convert to PIL image
        image = Image.fromarray(frame.numpy())
        return image


class VideoDataset(datasets.VisionDataset):
    """Implementation of a video dataset.

    The VideoDataset allows random reads from a video file without extracting
    all frames beforehand. This is more storage efficient but is slower.

    Attributes:
        root:
            Root directory path.
        extensions:
            Tuple of allowed extensions.
        transform:
            Function that takes a PIL image and returns transformed version
        target_transform:
            As transform but for targets
        is_valid_file:
            Used to check corrupt files
        exception_on_non_increasing_timestamp:
            If True, a NonIncreasingTimestampError is raised when trying to load
            a frame that has a timestamp lower or equal to the timestamps of
            previous frames in the same video.

    """

    def __init__(
        self,
        root,
        extensions=None,
        transform=None,
        target_transform=None,
        is_valid_file=None,
        exception_on_non_increasing_timestamp=True,
        tqdm_args: Dict[str, Any] = None,
        num_workers: int = 0,
    ):
        super(VideoDataset, self).__init__(
            root, transform=transform, target_transform=target_transform
        )

        videos, video_timestamps, offsets, fps = _make_dataset(
            self.root,
            extensions,
            is_valid_file,
            tqdm_args=tqdm_args,
            num_workers=num_workers,
        )

        if len(videos) == 0:
            msg = "Found 0 videos in folder: {}\n".format(self.root)
            if extensions is not None:
                msg += "Supported extensions are: {}".format(",".join(extensions))
            raise RuntimeError(msg)

        self.extensions = extensions
        self.backend = torchvision.get_video_backend()
        self.exception_on_non_increasing_timestamp = (
            exception_on_non_increasing_timestamp
        )

        self.videos = videos
        self.video_timestamps = video_timestamps
        self._length = sum((len(ts) for ts in self.video_timestamps))
        # Boolean value for every timestamp in self.video_timestamps. If True
        # the timestamp of the frame is non-increasing compared to timestamps of
        # previous frames in the video.
        self.video_timestamps_is_non_increasing = [
            _find_non_increasing_timestamps(timestamps)
            for timestamps in video_timestamps
        ]

        # offsets[i] indicates the index of the first frame of the i-th video.
        # e.g. for two videos of length 10 and 20, the offsets will be [0, 10].
        self.offsets = offsets
        self.fps = fps

        # Current VideoLoader instance and the corresponding video index. We
        # only keep track of the last accessed video as this is a good trade-off
        # between speed and memory requirements.
        # See https://github.com/lightly-ai/lightly/pull/702 for details.
        self._video_loader = None
        self._video_index = None

        # Keep unique reference of dataloader worker. We need this to avoid
        # accidentaly sharing VideoLoader instances between workers.
        self._worker_ref = None

        # Lock to prevent multiple threads creating a new VideoLoader at the
        # same time.
        self._video_loader_lock = threading.Lock()

    def __getitem__(self, index):
        """Returns item at index.

        Finds the video of the frame at index with the help of the frame
        offsets. Then, loads the frame from the video, applies the transforms,
        and returns the frame along with the index of the video (as target).

        For example, if there are two videos with 10 and 20 frames respectively
        in the input directory:

        Requesting the 5th sample returns the 5th frame from the first video and
        the target indicates the index of the source video which is 0.
        >>> dataset[5]
        >>> > <PIL Image>, 0

        Requesting the 20th sample returns the 10th frame from the second video
        and the target indicates the index of the source video which is 1.
        >>> dataset[20]
        >>> > <PIL Image>, 1

        Args:
            index:
                Index of the sample to retrieve.

        Returns:
            A tuple (sample, target) where target indicates the video index.

        Raises:
            IndexError:
                If index is out of bounds.
            VideoError:
                If the frame at the given index could not be loaded.

        """
        if index < 0 or index >= self.__len__():
            raise IndexError(
                f"Index {index} is out of bounds for VideoDataset"
                f" of size {self.__len__()}."
            )

        # each sample belongs to a video, to load the sample at index, we need
        # to find the video to which the sample belongs and then read the frame
        # from this video on the disk.
        i = len(self.offsets) - 1
        while self.offsets[i] > index:
            i = i - 1

        timestamp_idx = index - self.offsets[i]

        if (
            self.exception_on_non_increasing_timestamp
            and self.video_timestamps_is_non_increasing[i][timestamp_idx]
        ):
            raise NonIncreasingTimestampError(
                f"Frame {timestamp_idx} of video {self.videos[i]} has "
                f"a timestamp that is equal or lower than timestamps of previous "
                f"frames in the video. Trying to load this frame might result "
                f"in the wrong frame being returned. Set the VideoDataset.exception_on_non_increasing_timestamp"
                f"attribute to False to allow unsafe frame loading."
            )

        # find and return the frame as PIL image
        frame_timestamp = self.video_timestamps[i][timestamp_idx]
        video_loader = self._get_video_loader(i)
        sample = video_loader.read_frame(frame_timestamp)

        target = i
        if self.transform is not None:
            sample = self.transform(sample)
        if self.target_transform is not None:
            target = self.target_transform(target)

        return sample, target

    def __len__(self):
        """Returns the number of samples (frames) in the dataset.

        This can be precomputed, because self.video_timestamps is only
        set in the __init__
        """
        return self._length

    def get_filename(self, index):
        """Returns a filename for the frame at index.

        The filename is created from the video filename, the frame number, and
        the video format. The frame number will be zero padded to make sure
        all filenames have the same length and can easily be sorted.
        E.g. when retrieving a sample from the video
        `my_video.mp4` at frame 153, the filename will be:

        >>> my_video-153-mp4.png

        Args:
            index:
                Index of the frame to retrieve.

        Returns:
            The filename of the frame as described above.

        """
        if index < 0 or index >= self.__len__():
            raise IndexError(
                f"Index {index} is out of bounds for VideoDataset"
                f" of size {self.__len__()}."
            )

        # each sample belongs to a video, to load the sample at index, we need
        # to find the video to which the sample belongs and then read the frame
        # from this video on the disk.
        i = len(self.offsets) - 1
        while self.offsets[i] > index:
            i = i - 1

        # get filename of the video file
        video = self.videos[i]
        video_name, video_format = self._video_name_format(video)

        # get frame number
        frame_number = index - self.offsets[i]

        n_frames = self._video_frame_count(i)
        zero_padding = len(str(n_frames))

        return self._format_filename(
            video_name=video_name,
            video_format=video_format,
            frame_number=frame_number,
            zero_padding=zero_padding,
        )

    def get_filenames(self) -> List[str]:
        """Returns a list filenames for all frames in the dataset."""
        filenames = []
        for i, video in enumerate(self.videos):
            video_name, video_format = self._video_name_format(video)
            n_frames = self._video_frame_count(i)

            zero_padding = len(str(n_frames))
            for frame_number in range(n_frames):
                filenames.append(
                    self._format_filename(
                        video_name=video_name,
                        frame_number=frame_number,
                        video_format=video_format,
                        zero_padding=zero_padding,
                    )
                )
        return filenames

    def _video_frame_count(self, video_index: int) -> int:
        """Returns the number of frames in the video with the given index."""
        if video_index < len(self.offsets) - 1:
            n_frames = self.offsets[video_index + 1] - self.offsets[video_index]
        else:
            n_frames = len(self) - self.offsets[video_index]
        return n_frames

    def _video_name_format(self, video_filename: str) -> Tuple[str, str]:
        """Extracts name and format from the filename of the video.

        Returns:
            A (video_name, video_format) tuple where video_name is the filename
            relative to self.root and video_format is the file extension, for
            example 'mp4'.

        """
        video_filename = os.path.relpath(video_filename, self.root)
        splits = video_filename.split(".")
        video_format = splits[-1]
        video_name = ".".join(splits[:-1])
        return video_name, video_format

    def _format_filename(
        self,
        video_name: str,
        frame_number: int,
        video_format: str,
        zero_padding: int = 8,
        extension: str = "png",
    ) -> str:
        return f"{video_name}-{frame_number:0{zero_padding}}-{video_format}.{extension}"

    def _get_video_loader(self, video_index: int) -> VideoLoader:
        """Returns a video loader unique to the current dataloader worker."""
        worker_info = torch.utils.data.get_worker_info()
        if worker_info is not None:
            # Use a weakref instead of worker_info.id as the worker id is reused
            # by different workers across epochs.
            worker_ref = weakref.ref(worker_info)
            if worker_ref != self._worker_ref:
                # This worker has never accessed the dataset before, we have to
                # reset the video loader.
                self._video_loader = None
                self._video_index = None
                self._worker_ref = worker_ref

        with self._video_loader_lock:
            if video_index != self._video_index:
                video = self.videos[video_index]
                timestamps = self.video_timestamps[video_index]
                self._video_loader = VideoLoader(
                    video, timestamps, backend=self.backend
                )
                self._video_index = video_index

            return self._video_loader


class _TimestampFpsFromVideosDataset(Dataset):
    def __init__(self, video_instances: List[str], pts_unit: str):
        self.video_instances = video_instances
        self.pts_unit = pts_unit

    def __len__(self):
        return len(self.video_instances)

    def __getitem__(self, index):
        instance = self.video_instances[index]
        ts, fps = io.read_video_timestamps(instance, pts_unit=self.pts_unit)
        return ts, fps


def _make_dataset(
    directory,
    extensions=None,
    is_valid_file=None,
    pts_unit="sec",
    tqdm_args=None,
    num_workers: int = 0,
):
    """Returns a list of all video files, timestamps, and offsets.

    Args:
        directory:
            Root directory path (should not contain subdirectories).
        extensions:
            Tuple of valid extensions.
        is_valid_file:
            Used to find valid files.
        pts_unit:
            Unit of the timestamps.
        tqdm_args:
            arguments to pass to tqdm
        num_workers:
            number of workers to use for multithreading

    Returns:
        A list of video files, timestamps, frame offsets, and fps.

    """

    if tqdm_args is None:
        tqdm_args = {}
    if extensions is None:
        if is_valid_file is None:
            ValueError("Both extensions and is_valid_file cannot be None")
        else:
            _is_valid_file = is_valid_file
    else:

        def is_valid_file_extension(filepath):
            return filepath.lower().endswith(extensions)

        if is_valid_file is None:
            _is_valid_file = is_valid_file_extension
        else:

            def _is_valid_file(filepath):
                return is_valid_file_extension(filepath) and is_valid_file(filepath)

    # find all video instances (no subdirectories)
    video_instances = []

    def on_error(error):
        raise error

    for root, _, files in os.walk(directory, onerror=on_error):
        for fname in files:
            # skip invalid files
            if not _is_valid_file(os.path.join(root, fname)):
                continue

            # keep track of valid files
            path = os.path.join(root, fname)
            video_instances.append(path)

    # define loader to get the timestamps
    num_workers = min(num_workers, len(video_instances))
    if len(video_instances) == 1:
        num_workers = 0
    loader = DataLoader(
        _TimestampFpsFromVideosDataset(video_instances, pts_unit=pts_unit),
        num_workers=num_workers,
        batch_size=None,
        shuffle=False,
    )

    # actually load the data
    tqdm_args = dict(tqdm_args)
    tqdm_args.setdefault("unit", " video")
    tqdm_args.setdefault("desc", "Counting frames in videos")
    timestamps_fpss = list(tqdm(loader, **tqdm_args))
    timestamps, fpss = zip(*timestamps_fpss)

    # get frame offsets
    frame_counts = [len(ts) for ts in timestamps]
    offsets = [0] + list(np.cumsum(frame_counts[:-1]))

    return video_instances, timestamps, offsets, fpss


def _find_non_increasing_timestamps(timestamps: List[Fraction]) -> List[bool]:
    """Finds all non-increasing timestamps.

    Arguments:
        timestamps:
            Video frame timestamps.

    Returns:
        A boolean for each input timestamp which is True if the timestamp is
        non-increasing and False otherwise.

    """
    if len(timestamps) == 0:
        return []
    is_non_increasing = np.zeros(
        shape=len(timestamps),
        dtype=bool,
    )
    max_timestamp = timestamps[0] - 1
    for i, timestamp in enumerate(timestamps):
        if timestamp > max_timestamp:
            max_timestamp = timestamp
        else:
            is_non_increasing[i] = True

    return list(is_non_increasing)



================================================
FILE: lightly/data/collate.py
================================================
""" Collate Functions """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

import math
from multiprocessing import Value
from typing import List, Optional, Tuple, Union
from warnings import warn

import torch
import torch.nn as nn
import torchvision
from PIL import Image

from lightly.transforms import GaussianBlur, Jigsaw, RandomSolarization
from lightly.transforms.random_crop_and_flip_with_grid import RandomResizedCropAndFlip
from lightly.transforms.rotation import random_rotation_transform
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.transforms.utils import IMAGENET_NORMALIZE

imagenet_normalize = IMAGENET_NORMALIZE
# Kept for backwards compatibility


class BaseCollateFunction(nn.Module):
    """Base class for other collate implementations.

    Takes a batch of images as input and transforms each image into two
    different augmentations with the help of random transforms. The images are
    then concatenated such that the output batch is exactly twice the length
    of the input batch.

    Attributes:
        transform:
            A set of torchvision transforms which are randomly applied to
            each image.

    """

    def __init__(self, transform: T.Compose):
        _deprecation_warning_collate_functions()
        super(BaseCollateFunction, self).__init__()
        self.transform = transform

    def forward(self, batch: List[Tuple[Image.Image, int, str]]):
        """Turns a batch of tuples into a tuple of batches.

        Args:
            batch:
                A batch of tuples of images, labels, and filenames which
                is automatically provided if the dataloader is built from
                a LightlyDataset.

        Returns:
            A tuple of images, labels, and filenames. The images consist of
            two batches corresponding to the two transformations of the
            input images.

        Examples:
            >>> # define a random transformation and the collate function
            >>> transform = ... # some random augmentations
            >>> collate_fn = BaseCollateFunction(transform)
            >>>
            >>> # input is a batch of tuples (here, batch_size = 1)
            >>> input = [(img, 0, 'my-image.png')]
            >>> output = collate_fn(input)
            >>>
            >>> # output consists of two random transforms of the images,
            >>> # the labels, and the filenames in the batch
            >>> (img_t0, img_t1), label, filename = output

        """
        batch_size = len(batch)

        # list of transformed images
        transforms = [
            self.transform(batch[i % batch_size][0]).unsqueeze_(0)
            for i in range(2 * batch_size)
        ]
        # list of labels
        labels = torch.LongTensor([item[1] for item in batch])
        # list of filenames
        fnames = [item[2] for item in batch]

        # tuple of transforms
        transforms = (
            torch.cat(transforms[:batch_size], 0),
            torch.cat(transforms[batch_size:], 0),
        )

        return transforms, labels, fnames


class ImageCollateFunction(BaseCollateFunction):
    """Implementation of a collate function for images.

    This is an implementation of the BaseCollateFunction with a concrete
    set of transforms.

    The set of transforms is inspired by the SimCLR paper as it has shown
    to produce powerful embeddings.

    Attributes:
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        vf_prob:
            Probability that vertical flip is applied.
        hf_prob:
            Probability that horizontal flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None,
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple,
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        input_size: int = 64,
        cj_prob: float = 0.8,
        cj_bright: float = 0.7,
        cj_contrast: float = 0.7,
        cj_sat: float = 0.7,
        cj_hue: float = 0.2,
        min_scale: float = 0.15,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.2, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: dict = imagenet_normalize,
    ):
        if isinstance(input_size, tuple):
            input_size_ = max(input_size)
        else:
            input_size_ = input_size

        color_jitter = T.ColorJitter(cj_bright, cj_contrast, cj_sat, cj_hue)

        transform = [
            T.RandomResizedCrop(size=input_size, scale=(min_scale, 1.0)),
            T.RandomHorizontalFlip(p=hf_prob),
            T.RandomVerticalFlip(p=vf_prob),
            random_rotation_transform(rr_prob=rr_prob, rr_degrees=rr_degrees),
            T.RandomApply([color_jitter], p=cj_prob),
            T.RandomGrayscale(p=random_gray_scale),
            GaussianBlur(kernel_size=kernel_size, sigmas=sigmas, prob=gaussian_blur),
            T.ToTensor(),
        ]

        if normalize:
            transform += [T.Normalize(mean=normalize["mean"], std=normalize["std"])]

        transform = T.Compose(transform)

        super(ImageCollateFunction, self).__init__(transform)


class MultiViewCollateFunction(nn.Module):
    """Generates multiple views for each image in the batch.

    Attributes:
        transforms:
            List of transformation functions. Each function is used to generate
            one view of the back.

    """

    def __init__(self, transforms: List[T.Compose]):
        _deprecation_warning_collate_functions()
        super().__init__()
        self.transforms = transforms

    def forward(self, batch: List[tuple]):
        """Turns a batch of tuples into a tuple of batches.

        Args:
            batch:
                The input batch.

        Returns:
            A (views, labels, fnames) tuple where views is a list of tensors
            with each tensor containing one view of the batch.

        """
        views = []
        for transform in self.transforms:
            view = torch.stack([transform(img) for img, _, _ in batch])
            views.append(view)
        # list of labels
        labels = torch.LongTensor([label for _, label, _ in batch])
        # list of filenames
        fnames = [fname for _, _, fname in batch]
        return views, labels, fnames


class SimCLRCollateFunction(ImageCollateFunction):
    """Implements the transformations for SimCLR.

    Attributes:
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        vf_prob:
            Probability that vertical flip is applied.
        hf_prob:
            Probability that horizontal flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None,
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple,
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    Examples:

        >>> # SimCLR for ImageNet
        >>> collate_fn = SimCLRCollateFunction()
        >>>
        >>> # SimCLR for CIFAR-10
        >>> collate_fn = SimCLRCollateFunction(
        >>>     input_size=32,
        >>>     gaussian_blur=0.,
        >>> )

    """

    def __init__(
        self,
        input_size: int = 224,
        cj_prob: float = 0.8,
        cj_strength: float = 0.5,
        min_scale: float = 0.08,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.2, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: dict = imagenet_normalize,
    ):
        super(SimCLRCollateFunction, self).__init__(
            input_size=input_size,
            cj_prob=cj_prob,
            cj_bright=cj_strength * 0.8,
            cj_contrast=cj_strength * 0.8,
            cj_sat=cj_strength * 0.8,
            cj_hue=cj_strength * 0.2,
            min_scale=min_scale,
            random_gray_scale=random_gray_scale,
            gaussian_blur=gaussian_blur,
            kernel_size=kernel_size,
            sigmas=sigmas,
            vf_prob=vf_prob,
            hf_prob=hf_prob,
            rr_prob=rr_prob,
            rr_degrees=rr_degrees,
            normalize=normalize,
        )


class MoCoCollateFunction(ImageCollateFunction):
    """Implements the transformations for MoCo v1.

    For MoCo v2, simply use the SimCLR settings.

    Attributes:
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        vf_prob:
            Probability that vertical flip is applied.
        hf_prob:
            Probability that horizontal flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None,
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple,
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    Examples:

        >>> # MoCo v1 for ImageNet
        >>> collate_fn = MoCoCollateFunction()
        >>>
        >>> # MoCo v1 for CIFAR-10
        >>> collate_fn = MoCoCollateFunction(
        >>>     input_size=32,
        >>> )

    """

    def __init__(
        self,
        input_size: int = 224,
        cj_prob: float = 0.8,
        cj_strength: float = 0.4,
        min_scale: float = 0.2,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 0.0,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.2, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: dict = imagenet_normalize,
    ):
        super(MoCoCollateFunction, self).__init__(
            input_size=input_size,
            cj_prob=cj_prob,
            cj_bright=cj_strength,
            cj_contrast=cj_strength,
            cj_sat=cj_strength,
            cj_hue=cj_strength,
            min_scale=min_scale,
            random_gray_scale=random_gray_scale,
            gaussian_blur=gaussian_blur,
            kernel_size=kernel_size,
            sigmas=sigmas,
            vf_prob=vf_prob,
            hf_prob=hf_prob,
            rr_prob=rr_prob,
            rr_degrees=rr_degrees,
            normalize=normalize,
        )


class MultiCropCollateFunction(MultiViewCollateFunction):
    """Implements the multi-crop transformations for SwaV.

    Attributes:
        crop_sizes:
            Size of the input image in pixels for each crop category.
        crop_counts:
            Number of crops for each crop category.
        crop_min_scales:
            Min scales for each crop category.
        crop_max_scales:
            Max_scales for each crop category.
        transforms:
            Transforms which are applied to all crops.

    """

    def __init__(
        self,
        crop_sizes: List[int],
        crop_counts: List[int],
        crop_min_scales: List[float],
        crop_max_scales: List[float],
        transforms: T.Compose,
    ):
        if len(crop_sizes) != len(crop_counts):
            raise ValueError(
                "Length of crop_sizes and crop_counts must be equal but are"
                f" {len(crop_sizes)} and {len(crop_counts)}."
            )
        if len(crop_sizes) != len(crop_min_scales):
            raise ValueError(
                "Length of crop_sizes and crop_min_scales must be equal but are"
                f" {len(crop_sizes)} and {len(crop_min_scales)}."
            )
        if len(crop_sizes) != len(crop_min_scales):
            raise ValueError(
                "Length of crop_sizes and crop_max_scales must be equal but are"
                f" {len(crop_sizes)} and {len(crop_min_scales)}."
            )

        crop_transforms = []
        for i in range(len(crop_sizes)):
            random_resized_crop = T.RandomResizedCrop(
                crop_sizes[i], scale=(crop_min_scales[i], crop_max_scales[i])
            )

            crop_transforms.extend(
                [
                    T.Compose(
                        [
                            random_resized_crop,
                            transforms,
                        ]
                    )
                ]
                * crop_counts[i]
            )
        super().__init__(crop_transforms)


class SwaVCollateFunction(MultiCropCollateFunction):
    """Implements the multi-crop transformations for SwaV.

    Attributes:
        crop_sizes:
            Size of the input image in pixels for each crop category.
        crop_counts:
            Number of crops for each crop category.
        crop_min_scales:
            Min scales for each crop category.
        crop_max_scales:
            Max_scales for each crop category.
        hf_prob:
            Probability that horizontal flip is applied.
        vf_prob:
            Probability that vertical flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None,
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple,
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    Examples:

        >>> # SwaV for Imagenet
        >>> collate_fn = SwaVCollateFunction()
        >>>
        >>> # SwaV w/ 2x160 and 4x96 crops
        >>> collate_fn = SwaVCollateFunction(
        >>>     crop_sizes=[160, 96],
        >>>     crop_counts=[2, 4],
        >>> )

    """

    def __init__(
        self,
        crop_sizes: List[int] = [224, 96],
        crop_counts: List[int] = [2, 6],
        crop_min_scales: List[float] = [0.14, 0.05],
        crop_max_scales: List[float] = [1.0, 0.14],
        hf_prob: float = 0.5,
        vf_prob: float = 0.0,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        cj_prob: float = 0.8,
        cj_strength: float = 0.8,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.2, 2),
        normalize: dict = imagenet_normalize,
    ):
        color_jitter = T.ColorJitter(
            cj_strength,
            cj_strength,
            cj_strength,
            cj_strength / 4.0,
        )

        transforms = T.Compose(
            [
                T.RandomHorizontalFlip(p=hf_prob),
                T.RandomVerticalFlip(p=vf_prob),
                random_rotation_transform(rr_prob=rr_prob, rr_degrees=rr_degrees),
                T.ColorJitter(),
                T.RandomApply([color_jitter], p=cj_prob),
                T.RandomGrayscale(p=random_gray_scale),
                GaussianBlur(
                    kernel_size=kernel_size, sigmas=sigmas, prob=gaussian_blur
                ),
                T.ToTensor(),
                T.Normalize(mean=normalize["mean"], std=normalize["std"]),
            ]
        )

        super(SwaVCollateFunction, self).__init__(
            crop_sizes=crop_sizes,
            crop_counts=crop_counts,
            crop_min_scales=crop_min_scales,
            crop_max_scales=crop_max_scales,
            transforms=transforms,
        )


class DINOCollateFunction(MultiViewCollateFunction):
    """Implements the global and local view augmentations for DINO [0].

    This class generates two global and a user defined number of local views
    for each image in a batch. The code is adapted from [1].

    - [0]: DINO, 2021, https://arxiv.org/abs/2104.14294
    - [1]: https://github.com/facebookresearch/dino

    Attributes:
        global_crop_size:
            Crop size of the global views.
        global_crop_scale:
            Tuple of min and max scales relative to global_crop_size.
        local_crop_size:
            Crop size of the local views.
        local_crop_scale:
            Tuple of min and max scales relative to local_crop_size.
        n_local_views:
            Number of generated local views.
        hf_prob:
            Probability that horizontal flip is applied.
        vf_prob:
            Probability that vertical flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None,
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple,
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        cj_prob:
            Probability that color jitter is applied.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Tuple of probabilities to apply gaussian blur on the different
            views. The input is ordered as follows:
            (global_view_0, global_view_1, local_views)
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        kernel_scale:
            Old argument. Value is deprecated in favor of sigmas. If set, the old behavior applies and `sigmas` is ignored.
            Used to scale the `kernel_size` of a factor of `kernel_scale`
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        solarization:
            Probability to apply solarization on the second global view.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        global_crop_size=224,
        global_crop_scale=(0.4, 1.0),
        local_crop_size=96,
        local_crop_scale=(0.05, 0.4),
        n_local_views=6,
        hf_prob=0.5,
        vf_prob=0,
        rr_prob=0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        cj_prob=0.8,
        cj_bright=0.4,
        cj_contrast=0.4,
        cj_sat=0.2,
        cj_hue=0.1,
        random_gray_scale=0.2,
        gaussian_blur=(1.0, 0.1, 0.5),
        kernel_size: Optional[float] = None,
        kernel_scale: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        solarization_prob=0.2,
        normalize=imagenet_normalize,
    ):
        flip_and_color_jitter = T.Compose(
            [
                T.RandomHorizontalFlip(p=hf_prob),
                T.RandomVerticalFlip(p=vf_prob),
                random_rotation_transform(rr_prob=rr_prob, rr_degrees=rr_degrees),
                T.RandomApply(
                    [
                        T.ColorJitter(
                            brightness=cj_bright,
                            contrast=cj_contrast,
                            saturation=cj_sat,
                            hue=cj_hue,
                        )
                    ],
                    p=cj_prob,
                ),
                T.RandomGrayscale(p=random_gray_scale),
            ]
        )
        normalize = T.Compose(
            [
                T.ToTensor(),
                T.Normalize(mean=normalize["mean"], std=normalize["std"]),
            ]
        )
        global_crop = T.RandomResizedCrop(
            global_crop_size,
            scale=global_crop_scale,
            interpolation=Image.BICUBIC,
        )

        # first global crop
        global_transform_0 = T.Compose(
            [
                global_crop,
                flip_and_color_jitter,
                GaussianBlur(
                    kernel_size=kernel_size,
                    scale=kernel_scale,
                    sigmas=sigmas,
                    prob=gaussian_blur[0],
                ),
                normalize,
            ]
        )

        # second global crop
        global_transform_1 = T.Compose(
            [
                global_crop,
                flip_and_color_jitter,
                GaussianBlur(
                    kernel_size=kernel_size,
                    scale=kernel_scale,
                    sigmas=sigmas,
                    prob=gaussian_blur[1],
                ),
                RandomSolarization(prob=solarization_prob),
                normalize,
            ]
        )

        # transformation for the local small crops
        local_transform = T.Compose(
            [
                T.RandomResizedCrop(
                    local_crop_size, scale=local_crop_scale, interpolation=Image.BICUBIC
                ),
                flip_and_color_jitter,
                GaussianBlur(
                    kernel_size=kernel_size,
                    scale=kernel_scale,
                    sigmas=sigmas,
                    prob=gaussian_blur[2],
                ),
                normalize,
            ]
        )
        local_transforms = [local_transform] * n_local_views

        transforms = [global_transform_0, global_transform_1]
        transforms.extend(local_transforms)
        super().__init__(transforms)


class MAECollateFunction(MultiViewCollateFunction):
    """Implements the view augmentation for MAE [0].

    - [0]: Masked Autoencoder, 2021, https://arxiv.org/abs/2111.06377

    Attributes:
        input_size:
            Size of the input image in pixels.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        input_size: Union[int, Tuple[int, int]] = 224,
        min_scale: float = 0.2,
        normalize: dict = imagenet_normalize,
    ):
        transforms = [
            T.RandomResizedCrop(
                input_size, scale=(min_scale, 1.0), interpolation=3
            ),  # 3 is bicubic
            T.RandomHorizontalFlip(),
            T.ToTensor(),
        ]

        if normalize:
            transforms.append(T.Normalize(mean=normalize["mean"], std=normalize["std"]))
        super().__init__([T.Compose(transforms)])

    def forward(self, batch: List[tuple]):
        views, labels, fnames = super().forward(batch)
        # Return only first view as MAE needs only a single view per image.
        return views[0], labels, fnames


class PIRLCollateFunction(nn.Module):
    """Implements the transformations for PIRL [0]. The jigsaw augmentation
    is applied during the forward pass.

    - [0] PIRL, 2019: https://arxiv.org/abs/1912.01991

    Attributes:
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        hf_prob:
            Probability that horizontal flip is applied.
        n_grid:
            Sqrt of the number of grids in the jigsaw image.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    Examples:

        >>> # PIRL for ImageNet
        >>> collate_fn = PIRLCollateFunction()
        >>>
        >>> # PIRL for CIFAR-10
        >>> collate_fn = PIRLCollateFunction(
        >>>     input_size=32,
        >>> )

    """

    def __init__(
        self,
        input_size: int = 64,
        cj_prob: float = 0.8,
        cj_bright: float = 0.4,
        cj_contrast: float = 0.4,
        cj_sat: float = 0.4,
        cj_hue: float = 0.4,
        min_scale: float = 0.08,
        random_gray_scale: float = 0.2,
        hf_prob: float = 0.5,
        n_grid: int = 3,
        normalize: dict = imagenet_normalize,
    ):
        _deprecation_warning_collate_functions()
        super(PIRLCollateFunction, self).__init__()

        if isinstance(input_size, tuple):
            input_size_ = max(input_size)
        else:
            input_size_ = input_size

        color_jitter = T.ColorJitter(cj_bright, cj_contrast, cj_sat, cj_hue)

        # Transform for transformed jigsaw image
        transform = [
            T.RandomHorizontalFlip(p=hf_prob),
            T.RandomApply([color_jitter], p=cj_prob),
            T.RandomGrayscale(p=random_gray_scale),
            T.ToTensor(),
        ]

        if normalize:
            transform += [T.Normalize(mean=normalize["mean"], std=normalize["std"])]

        # Cropping and normalisation for untransformed image
        self.no_augment = T.Compose(
            [
                T.RandomResizedCrop(size=input_size, scale=(min_scale, 1.0)),
                T.ToTensor(),
                T.Normalize(mean=normalize["mean"], std=normalize["std"]),
            ]
        )
        self.jigsaw = Jigsaw(
            n_grid=n_grid,
            img_size=input_size_,
            crop_size=int(input_size_ // n_grid),
            transform=T.Compose(transform),
        )

    def forward(self, batch: List[tuple]):
        """Overriding the BaseCollateFunction class's forward method because
        for PIRL we need only one augmented batch, as opposed to both, which the
        BaseCollateFunction creates."""
        batch_size = len(batch)

        # list of transformed images
        img_transforms = [
            self.jigsaw(batch[i][0]).unsqueeze_(0) for i in range(batch_size)
        ]
        img = [self.no_augment(batch[i][0]).unsqueeze_(0) for i in range(batch_size)]
        # list of labels
        labels = torch.LongTensor([item[1] for item in batch])
        # list of filenames
        fnames = [item[2] for item in batch]

        # tuple of transforms
        transforms = (torch.cat(img, 0), torch.cat(img_transforms, 0))

        return transforms, labels, fnames


class MSNCollateFunction(MultiViewCollateFunction):
    """Implements the transformations for MSN [0].

    Generates a set of random and focal views for each input image. The generated output
    is (views, target, filenames) where views is list with the following entries:
    [random_views_0, random_views_1, ..., focal_views_0, focal_views_1, ...].

    - [0]: Masked Siamese Networks, 2022: https://arxiv.org/abs/2204.07141

    Attributes:
        random_size:
            Size of the random image views in pixels.
        focal_size:
            Size of the focal image views in pixels.
        random_views:
            Number of random views to generate.
        focal_views:
            Number of focal views to generate.
        random_crop_scale:
            Minimum and maximum size of the randomized crops for the relative to random_size.
        focal_crop_scale:
            Minimum and maximum size of the randomized crops relative to focal_size.
        cj_prob:
            Probability that color jittering is applied.
        cj_strength:
            Strength of the color jitter.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        random_gray_scale:
            Probability of conversion to grayscale.
        hf_prob:
            Probability that horizontal flip is applied.
        vf_prob:
            Probability that vertical flip is applied.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.
    """

    def __init__(
        self,
        random_size: int = 224,
        focal_size: int = 96,
        random_views: int = 2,
        focal_views: int = 10,
        random_crop_scale: Tuple[float, float] = (0.3, 1.0),
        focal_crop_scale: Tuple[float, float] = (0.05, 0.3),
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.2, 2),
        random_gray_scale: float = 0.2,
        hf_prob: float = 0.5,
        vf_prob: float = 0.0,
        normalize: dict = imagenet_normalize,
    ) -> None:
        color_jitter = T.ColorJitter(
            brightness=0.8 * cj_strength,
            contrast=0.8 * cj_strength,
            saturation=0.8 * cj_strength,
            hue=0.2 * cj_strength,
        )
        transform = T.Compose(
            [
                T.RandomResizedCrop(size=random_size, scale=random_crop_scale),
                T.RandomHorizontalFlip(p=hf_prob),
                T.RandomVerticalFlip(p=vf_prob),
                T.RandomApply([color_jitter], p=cj_prob),
                T.RandomGrayscale(p=random_gray_scale),
                GaussianBlur(
                    kernel_size=kernel_size, sigmas=sigmas, prob=gaussian_blur
                ),
                T.ToTensor(),
                T.Normalize(mean=normalize["mean"], std=normalize["std"]),
            ]
        )
        focal_transform = T.Compose(
            [
                T.RandomResizedCrop(size=focal_size, scale=focal_crop_scale),
                T.RandomHorizontalFlip(p=hf_prob),
                T.RandomVerticalFlip(p=vf_prob),
                T.RandomApply([color_jitter], p=cj_prob),
                T.RandomGrayscale(p=random_gray_scale),
                GaussianBlur(
                    kernel_size=kernel_size, sigmas=sigmas, prob=gaussian_blur
                ),
                T.ToTensor(),
                T.Normalize(mean=normalize["mean"], std=normalize["std"]),
            ]
        )
        transforms = [transform] * random_views
        transforms += [focal_transform] * focal_views
        super().__init__(transforms=transforms)


class SMoGCollateFunction(MultiViewCollateFunction):
    """Implements the transformations for SMoG.

    Attributes:
        crop_sizes:
            Size of the input image in pixels for each crop category.
        crop_counts:
            Number of crops for each crop category.
        crop_min_scales:
            Min scales for each crop category.
        crop_max_scales:
            Max_scales for each crop category.
        gaussian_blur_probs:
            Probability of Gaussian blur for each crop category.
        gaussian_blur_kernel_sizes:
            Deprecated values in favour of sigmas.
        gaussian_blur_sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
        solarize_probs:
            Probability of solarization for each crop category.
        hf_prob:
            Probability that horizontal flip is applied.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter.
        random_gray_scale:
            Probability of conversion to grayscale.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        crop_sizes: List[int] = [224, 96],
        crop_counts: List[int] = [4, 4],
        crop_min_scales: List[float] = [0.2, 0.05],
        crop_max_scales: List[float] = [1.0, 0.2],
        gaussian_blur_probs: List[float] = [0.5, 0.1],
        gaussian_blur_kernel_sizes: Optional[List[float]] = [None, None],
        gaussian_blur_sigmas: Tuple[float, float] = (0.2, 2),
        solarize_probs: List[float] = [0.0, 0.2],
        hf_prob: float = 0.5,
        cj_prob: float = 1.0,
        cj_strength: float = 0.5,
        random_gray_scale: float = 0.2,
        normalize: dict = imagenet_normalize,
    ):
        transforms = []
        for i in range(len(crop_sizes)):
            random_resized_crop = T.RandomResizedCrop(
                crop_sizes[i], scale=(crop_min_scales[i], crop_max_scales[i])
            )

            color_jitter = T.ColorJitter(
                0.8 * cj_strength,
                0.8 * cj_strength,
                0.4 * cj_strength,
                0.2 * cj_strength,
            )

            transforms.extend(
                [
                    T.Compose(
                        [
                            random_resized_crop,
                            T.RandomHorizontalFlip(p=hf_prob),
                            T.RandomApply([color_jitter], p=cj_prob),
                            T.RandomGrayscale(p=random_gray_scale),
                            GaussianBlur(
                                kernel_size=gaussian_blur_kernel_sizes[i],
                                prob=gaussian_blur_probs[i],
                                sigmas=gaussian_blur_sigmas,
                            ),  # TODO
                            RandomSolarization(prob=solarize_probs[i]),
                            T.ToTensor(),
                            T.Normalize(mean=normalize["mean"], std=normalize["std"]),
                        ]
                    )
                ]
                * crop_counts[i]
            )

        super().__init__(transforms)


class VICRegCollateFunction(BaseCollateFunction):
    """Implementation of a collate function for images.

    This is an implementation of the BaseCollateFunction with a concrete
    set of transforms.

    The set of transforms is inspired by the SimCLR paper as it has shown
    to produce powerful embeddings.

    Attributes:
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        solarize_prob:
            Probability of solarization.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        vf_prob:
            Probability that vertical flip is applied.
        hf_prob:
            Probability that horizontal flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None,
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple,
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        input_size: int = 224,
        cj_prob: float = 0.8,
        cj_bright: float = 0.4,
        cj_contrast: float = 0.4,
        cj_sat: float = 0.2,
        cj_hue: float = 0.1,
        min_scale: float = 0.08,
        random_gray_scale: float = 0.2,
        solarize_prob: float = 0.1,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.2, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: dict = imagenet_normalize,
    ):
        if isinstance(input_size, tuple):
            input_size_ = max(input_size)
        else:
            input_size_ = input_size

        color_jitter = T.ColorJitter(cj_bright, cj_contrast, cj_sat, cj_hue)

        transform = [
            T.RandomResizedCrop(size=input_size, scale=(min_scale, 1.0)),
            T.RandomHorizontalFlip(p=hf_prob),
            T.RandomVerticalFlip(p=vf_prob),
            random_rotation_transform(rr_prob=rr_prob, rr_degrees=rr_degrees),
            T.RandomApply([color_jitter], p=cj_prob),
            T.RandomGrayscale(p=random_gray_scale),
            RandomSolarization(prob=solarize_prob),
            GaussianBlur(kernel_size=kernel_size, sigmas=sigmas, prob=gaussian_blur),
            T.ToTensor(),
        ]

        if normalize:
            transform += [T.Normalize(mean=normalize["mean"], std=normalize["std"])]

        transform = T.Compose(transform)

        super(VICRegCollateFunction, self).__init__(transform)


class VICRegLCollateFunction(nn.Module):
    """Transforms images for VICRegL.

    Attributes:
        global_crop_size:
            Size of the input image in pixels for the global crop category.
        local_crop_size:
            Size of the input image in pixels for the local crop category.
        global_crop_scale:
            Min and max scales for the global crop category.
        local_crop_scale:
            Min and max scales for the local crop category.
        global_grid_size:
            Grid size for the global crop category.
        local_grid_size:
            Grid size for the local crop category.
        global_gaussian_blur_prob:
            Probability of Gaussian blur for the global crop category.
        local_gaussian_blur_prob:
            Probability of Gaussian blur for the local crop category.
        global_gaussian_blur_kernel_size:
            Will be deprecated in favor of `global_gaussian_blur_sigmas` argument. If set, the old behavior applies and `global_gaussian_blur_sigmas` is ignored.
            Used to calculate sigma of gaussian blur with global_gaussian_blur_kernel_size * input_size. Applied to global crop category.
        local_gaussian_blur_kernel_size:
            Will be deprecated in favor of `local_gaussian_blur_sigmas` argument. If set, the old behavior applies and `local_gaussian_blur_sigmas` is ignored.
            Used to calculate sigma of gaussian blur with local_gaussian_blur_kernel_size * input_size. Applied to local crop category.
        global_gaussian_blur_sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `global_gaussian_blur_kernel_size` is set. Applied to global crop category.
        local_gaussian_blur_sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `local_gaussian_blur_kernel_size` is set. Applied to local crop category.
        global_solarize_prob:
            Probability of solarization for the global crop category.
        local_solarize_prob:
            Probability of solarization for the local crop category.
        hf_prob:
            Probability that horizontal flip is applied.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter.
        random_gray_scale:
            Probability of conversion to grayscale.
        normalize:
            Dictionary with mean and standard deviation for normalization.
    """

    def __init__(
        self,
        global_crop_size: int = 224,
        local_crop_size: int = 96,
        global_crop_scale: Tuple[int] = (0.2, 1.0),
        local_crop_scale: Tuple[int] = (0.05, 0.2),
        global_grid_size: int = 7,
        local_grid_size: int = 3,
        global_gaussian_blur_prob: float = 0.5,
        local_gaussian_blur_prob: float = 0.1,
        global_gaussian_blur_kernel_size: Optional[float] = None,
        local_gaussian_blur_kernel_size: Optional[float] = None,
        global_gaussian_blur_sigmas: Tuple[float, float] = (0.2, 2),
        local_gaussian_blur_sigmas: Tuple[float, float] = (0.2, 2),
        global_solarize_prob: float = 0.0,
        local_solarize_prob: float = 0.2,
        hf_prob: float = 0.5,
        cj_prob: float = 1.0,
        cj_strength: float = 0.5,
        random_gray_scale: float = 0.2,
        normalize: dict = imagenet_normalize,
    ):
        _deprecation_warning_collate_functions()
        super().__init__()
        self.global_crop_and_flip = RandomResizedCropAndFlip(
            crop_size=global_crop_size,
            crop_min_scale=global_crop_scale[0],
            crop_max_scale=global_crop_scale[1],
            hf_prob=hf_prob,
            grid_size=global_grid_size,
        )
        self.local_crop_and_flip = RandomResizedCropAndFlip(
            crop_size=local_crop_size,
            crop_min_scale=local_crop_scale[0],
            crop_max_scale=local_crop_scale[1],
            hf_prob=hf_prob,
            grid_size=local_grid_size,
        )

        color_jitter = T.ColorJitter(
            0.8 * cj_strength,
            0.8 * cj_strength,
            0.4 * cj_strength,
            0.2 * cj_strength,
        )
        self.global_transform = T.Compose(
            [
                T.RandomApply([color_jitter], p=cj_prob),
                T.RandomGrayscale(p=random_gray_scale),
                GaussianBlur(
                    kernel_size=global_gaussian_blur_kernel_size,
                    prob=global_gaussian_blur_prob,
                    sigmas=global_gaussian_blur_sigmas,
                ),
                RandomSolarization(prob=global_solarize_prob),
                T.ToTensor(),
                T.Normalize(mean=normalize["mean"], std=normalize["std"]),
            ]
        )

        self.local_transform = T.Compose(
            [
                T.RandomApply([color_jitter], p=cj_prob),
                T.RandomGrayscale(p=random_gray_scale),
                GaussianBlur(
                    kernel_size=local_gaussian_blur_kernel_size,
                    prob=local_gaussian_blur_prob,
                    sigmas=local_gaussian_blur_sigmas,
                ),
                RandomSolarization(prob=local_solarize_prob),
                T.ToTensor(),
                T.Normalize(mean=normalize["mean"], std=normalize["std"]),
            ]
        )

    def forward(
        self, batch: List[Tuple[Image.Image, int, str]]
    ) -> Tuple[
        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor],
        torch.Tensor,
        torch.Tensor,
    ]:
        """
        Applies transforms to images in the input batch.

        Args:
            batch:
                A list of tuples containing an image (as a PIL Image),
                a label (int), and a filename (str).

        Returns:
            A tuple of transformed images (as a 4-tuple of torch.Tensors containing view_global, view_local, grid_global, grid_local),
            labels (as torch.Tensor), and filenames (as torch.Tensor).

        """

        views_global = []
        views_local = []
        grids_global = []
        grids_local = []
        labels = []
        fnames = []

        for image, label, filename in batch:
            view_global, grid_global = self.global_crop_and_flip.forward(image)
            view_local, grid_local = self.local_crop_and_flip.forward(image)
            views_global.append(self.global_transform(view_global))
            views_local.append(self.local_transform(view_local))
            grids_global.append(grid_global)
            grids_local.append(grid_local)
            labels.append(torch.LongTensor(label))
            fnames.append(filename)

        views_global = torch.stack(views_global)
        views_local = torch.stack(views_local)
        grids_global = torch.stack(grids_global)
        grids_local = torch.stack(grids_local)

        return (views_global, views_local, grids_global, grids_local), labels, fnames


class IJEPAMaskCollator:
    """Collator for IJEPA model [0].

    Experimental: Support for I-JEPA is experimental, there might be breaking changes
    in the future.

    Code inspired by [1].

    - [0]: Joint-Embedding Predictive Architecture, 2023, https://arxiv.org/abs/2301.08243
    - [1]: https://github.com/facebookresearch/ijepa
    """

    def __init__(
        self,
        input_size=(224, 224),
        patch_size=16,
        enc_mask_scale=(0.2, 0.8),
        pred_mask_scale=(0.2, 0.8),
        aspect_ratio=(0.3, 3.0),
        nenc=1,
        npred=2,
        min_keep=4,
        allow_overlap=False,
    ):
        if not isinstance(input_size, tuple):
            input_size = (input_size,) * 2
        self.patch_size = patch_size
        self.height, self.width = (
            input_size[0] // patch_size,
            input_size[1] // patch_size,
        )
        self.enc_mask_scale = enc_mask_scale
        self.pred_mask_scale = pred_mask_scale
        self.aspect_ratio = aspect_ratio
        self.nenc = nenc
        self.npred = npred
        self.min_keep = min_keep  # minimum number of patches to keep
        self.allow_overlap = (
            allow_overlap  # whether to allow overlap b/w enc and pred masks
        )
        self._itr_counter = Value("i", -1)  # collator is shared across worker processes

    def step(self):
        i = self._itr_counter
        with i.get_lock():
            i.value += 1
            v = i.value
        return v

    def _sample_block_size(self, generator, scale, aspect_ratio_scale):
        _rand = torch.rand(1, generator=generator).item()
        # -- Sample block scale
        min_s, max_s = scale
        mask_scale = min_s + _rand * (max_s - min_s)
        max_keep = int(self.height * self.width * mask_scale)
        # -- Sample block aspect-ratio
        min_ar, max_ar = aspect_ratio_scale
        aspect_ratio = min_ar + _rand * (max_ar - min_ar)
        # -- Compute block height and width (given scale and aspect-ratio)
        h = int(round(math.sqrt(max_keep * aspect_ratio)))
        w = int(round(math.sqrt(max_keep / aspect_ratio)))
        while h >= self.height:
            h -= 1
        while w >= self.width:
            w -= 1

        return (h, w)

    def _sample_block_mask(self, b_size, acceptable_regions=None):
        h, w = b_size

        def constrain_mask(mask, tries=0):
            """Helper to restrict given mask to a set of acceptable regions"""
            N = max(int(len(acceptable_regions) - tries), 0)
            for k in range(N):
                mask *= acceptable_regions[k]

        # --
        # -- Loop to sample masks until we find a valid one
        tries = 0
        timeout = og_timeout = 20
        valid_mask = False
        while not valid_mask:
            # -- Sample block top-left corner
            top = torch.randint(0, self.height - h, (1,))
            left = torch.randint(0, self.width - w, (1,))
            mask = torch.zeros((self.height, self.width), dtype=torch.int32)
            mask[top : top + h, left : left + w] = 1
            # -- Constrain mask to a set of acceptable regions
            if acceptable_regions is not None:
                constrain_mask(mask, tries)
            mask = torch.nonzero(mask.flatten())
            # -- If mask too small try again
            valid_mask = len(mask) > self.min_keep
            if not valid_mask:
                timeout -= 1
                if timeout == 0:
                    tries += 1
                    timeout = og_timeout
        mask = mask.squeeze()
        # --
        mask_complement = torch.ones((self.height, self.width), dtype=torch.int32)
        mask_complement[top : top + h, left : left + w] = 0
        # --
        return mask, mask_complement

    def __call__(self, batch):
        """
        Create encoder and predictor masks when collating imgs into a batch
        # 1. sample enc block (size + location) using seed
        # 2. sample pred block (size) using seed
        # 3. sample several enc block locations for each image (w/o seed)
        # 4. sample several pred block locations for each image (w/o seed)
        # 5. return enc mask and pred mask
        """
        B = len(batch)

        collated_batch = torch.utils.data.default_collate(batch)

        seed = self.step()
        g = torch.Generator()
        g.manual_seed(seed)
        p_size = self._sample_block_size(
            generator=g,
            scale=self.pred_mask_scale,
            aspect_ratio_scale=self.aspect_ratio,
        )
        e_size = self._sample_block_size(
            generator=g, scale=self.enc_mask_scale, aspect_ratio_scale=(1.0, 1.0)
        )

        collated_masks_pred, collated_masks_enc = [], []
        min_keep_pred = self.height * self.width
        min_keep_enc = self.height * self.width
        for _ in range(B):
            masks_p, masks_C = [], []
            for _ in range(self.npred):
                mask, mask_C = self._sample_block_mask(p_size)
                masks_p.append(mask)
                masks_C.append(mask_C)
                min_keep_pred = min(min_keep_pred, len(mask))
            collated_masks_pred.append(masks_p)

            acceptable_regions = masks_C

            if self.allow_overlap:
                acceptable_regions = None

            masks_e = []
            for _ in range(self.nenc):
                mask, _ = self._sample_block_mask(
                    e_size, acceptable_regions=acceptable_regions
                )
                masks_e.append(mask)
                min_keep_enc = min(min_keep_enc, len(mask))
            collated_masks_enc.append(masks_e)

        collated_masks_pred = [
            [cm[:min_keep_pred] for cm in cm_list] for cm_list in collated_masks_pred
        ]
        collated_masks_pred = torch.utils.data.default_collate(collated_masks_pred)
        # --
        collated_masks_enc = [
            [cm[:min_keep_enc] for cm in cm_list] for cm_list in collated_masks_enc
        ]
        collated_masks_enc = torch.utils.data.default_collate(collated_masks_enc)

        return collated_batch, collated_masks_enc, collated_masks_pred


def _deprecation_warning_collate_functions() -> None:
    warn(
        "Collate functions are deprecated and will be removed in favor of transforms in v1.4.0.\n"
        "See https://docs.lightly.ai/self-supervised-learning/examples/models.html for examples.",
        category=DeprecationWarning,
    )



================================================
FILE: lightly/data/dataset.py
================================================
# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

import bisect
import os
import shutil
import tempfile
from typing import Any, Callable, Dict, List, Optional, Union

import torchvision.datasets as datasets
from PIL import Image
from torchvision import transforms
from torchvision.datasets.vision import StandardTransform, VisionDataset

from lightly.data._helpers import DatasetFolder, _load_dataset_from_folder
from lightly.data._video import VideoDataset


class LightlyDataset:
    """Provides a uniform data interface for the embedding models.

    Should be used for all models and functions in the lightly package.
    Returns a tuple (sample, target, fname) when accessed using __getitem__.

    The LightlyDataset supports different input sources. You can use it
    on a folder of images. You can also use it on a folder with subfolders
    with images (ImageNet style). If the input_dir has subfolders,
    each subfolder gets its own target label.
    You can also work with videos (requires pyav).
    If there are multiple videos in the input_dir each video gets a different
    target label assigned. If input_dir contains images and videos
    only the videos are used.

    Can also be used in combination with the `from_torch_dataset` method
    to load a dataset offered by torchvision (e.g. cifar10).

    Parameters:
        input_dir:
            Path to directory holding the images or videos to load.
        transform:
            Image transforms (as in torchvision).
        index_to_filename:
            Function which takes the dataset and index as input and returns
            the filename of the file at the index. If None, uses default.
        filenames:
            If not None, it filters the dataset in the input directory
            by the given filenames.

    Examples:
        >>> # load a dataset consisting of images from a local folder
        >>> # mydata/
        >>> # `- img1.png
        >>> # `- img2.png
        >>> # `- ...
        >>> import lightly.data as data
        >>> dataset = data.LightlyDataset(input_dir='path/to/mydata/')
        >>> sample, target, fname = dataset[0]
        >>>
        >>> # also works with subfolders
        >>> # mydata/
        >>> # `- subfolder1
        >>> #     `- img1.png
        >>> # `- subfolder2
        >>> # ...
        >>>
        >>> # also works with videos
        >>> # mydata/
        >>> # `- video1.mp4
        >>> # `- video2.mp4
        >>> # `- ...
    """

    def __init__(
        self,
        input_dir: Union[str, None],
        transform: transforms.Compose = None,
        index_to_filename: Callable[[datasets.VisionDataset, int], str] = None,
        filenames: List[str] = None,
        tqdm_args: Dict[str, Any] = None,
        num_workers_video_frame_counting: int = 0,
    ):
        # can pass input_dir=None to create an "empty" dataset
        self.input_dir = input_dir
        if filenames is not None:
            filepaths = [os.path.join(input_dir, filename) for filename in filenames]
            filepaths = set(filepaths)

            def is_valid_file(filepath: str):
                return filepath in filepaths

        else:
            is_valid_file = None

        if self.input_dir is not None:
            self.dataset = _load_dataset_from_folder(
                self.input_dir,
                transform,
                is_valid_file=is_valid_file,
                tqdm_args=tqdm_args,
                num_workers_video_frame_counting=num_workers_video_frame_counting,
            )
        elif transform is not None:
            raise ValueError(
                "transform must be None when input_dir is None but is " f"{transform}",
            )

        # initialize function to get filename of image
        self.index_to_filename = _get_filename_by_index
        if index_to_filename is not None:
            self.index_to_filename = index_to_filename

    @classmethod
    def from_torch_dataset(cls, dataset, transform=None, index_to_filename=None):
        """Builds a LightlyDataset from a PyTorch (or torchvision) dataset.

        Args:
            dataset:
                PyTorch/torchvision dataset.
            transform:
                Image transforms (as in torchvision).
            index_to_filename:
                Function which takes the dataset and index as input and returns
                the filename of the file at the index. If None, uses default.

        Returns:
            A LightlyDataset object.

        Examples:
            >>> # load cifar10 from torchvision
            >>> import torchvision
            >>> import lightly.data as data
            >>> base = torchvision.datasets.CIFAR10(root='./')
            >>> dataset = data.LightlyDataset.from_torch_dataset(base)

        """
        # create an "empty" dataset object
        dataset_obj = cls(
            None,
            index_to_filename=index_to_filename,
        )

        # populate it with the torch dataset
        if transform is not None:
            dataset.transform = transform
            # If dataset is a VisionDataset, we need to initialize transforms, too.
            if isinstance(dataset, VisionDataset):
                dataset.transforms = StandardTransform(
                    transform, dataset.target_transform
                )
        dataset_obj.dataset = dataset
        return dataset_obj

    def __getitem__(self, index: int):
        """Returns (sample, target, fname) of item at index.

        Args:
            index:
                Index of the queried item.

        Returns:
            The image, target, and filename of the item at index.

        """
        fname = self.index_to_filename(self.dataset, index)
        sample, target = self.dataset.__getitem__(index)

        return sample, target, fname

    def __len__(self):
        """Returns the length of the dataset."""
        return len(self.dataset)

    def __add__(self, other):
        """Adds another item to the dataset."""
        raise NotImplementedError()

    def get_filenames(self) -> List[str]:
        """Returns all filenames in the dataset."""
        if hasattr(self.dataset, "get_filenames"):
            return self.dataset.get_filenames()

        list_of_filenames = []
        for index in range(len(self)):
            fname = self.index_to_filename(self.dataset, index)
            list_of_filenames.append(fname)
        return list_of_filenames

    def dump(
        self,
        output_dir: str,
        filenames: Optional[List[str]] = None,
        format: Optional[str] = None,
    ):
        """Saves images in the dataset to the output directory.

        Will copy the images from the input directory to the output directory
        if possible. If not (e.g. for VideoDatasets), will load the images and
        then save them to the output directory with the specified format.

        Args:
            output_dir:
                Output directory where the image is stored.
            filenames:
                Filenames of the images to store. If None, stores all images.
            format:
                Image format. Can be any pillow image format (png, jpg, ...).
                By default we try to use the same format as the input data. If
                not possible (e.g. for videos) we dump the image
                as a png image to prevent compression artifacts.

        """

        if self.dataset.transform is not None:
            raise RuntimeError("Cannot dump dataset which applies transforms!")

        # create directory if it doesn't exist yet
        os.makedirs(output_dir, exist_ok=True)

        # dump all the files if no filenames were passed, otherwise dump only
        # the ones referenced in the list
        if filenames is None:
            indices = [i for i in range(self.__len__())]
            filenames = self.get_filenames()
        else:
            indices = []
            filenames = sorted(filenames)
            all_filenames = self.get_filenames()
            for index, filename in enumerate(all_filenames):
                filename_index = bisect.bisect_left(filenames, filename)
                # make sure the filename exists in filenames
                if (
                    filename_index < len(filenames)
                    and filenames[filename_index] == filename
                ):
                    indices.append(index)

        # dump images
        for i, filename in zip(indices, filenames):
            _dump_image(self.dataset, output_dir, filename, i, fmt=format)

    def get_filepath_from_filename(self, filename: str, image: Image = None):
        """Returns the filepath given the filename of the image

        There are three cases:
            - The dataset is a regular dataset with the images in the input dir.
            - The dataset is a video dataset, thus the images have to be saved in a
              temporary folder.
            - The dataset is a torch dataset, thus the images have to be saved in a
              temporary folder.

        Args:
            filename:
                The filename of the image
            image:
                The image corresponding to the filename

        Returns:
            The filename to the image, either the existing one (case 1) or a
            newly created jpg (case 2, 3)

        """

        has_input_dir = hasattr(self, "input_dir") and isinstance(self.input_dir, str)
        if has_input_dir:
            path_to_image = os.path.join(self.input_dir, filename)
            if os.path.isfile(path_to_image):
                # the file exists, return its filepath
                return path_to_image

        if image is None:
            raise ValueError(
                "The parameter image must not be None for"
                "VideoDatasets and TorchDatasets"
            )

        # the file doesn't exist, save it as a jpg and return filepath
        folder_path = tempfile.mkdtemp()
        filepath = os.path.join(folder_path, filename) + ".jpg"

        if os.path.dirname(filepath):
            os.makedirs(os.path.dirname(filepath), exist_ok=True)

        image.save(filepath)
        return filepath

    @property
    def transform(self):
        """Getter for the transform of the dataset."""
        return self.dataset.transform

    @transform.setter
    def transform(self, t):
        """Setter for the transform of the dataset."""
        self.dataset.transform = t


def _get_filename_by_index(dataset, index):
    """Default function which maps the index of an image to a filename."""
    if isinstance(dataset, datasets.ImageFolder):
        # filename is the path of the image relative to the dataset root
        full_path = dataset.imgs[index][0]
        return os.path.relpath(full_path, dataset.root)
    elif isinstance(dataset, DatasetFolder):
        # filename is the path of the image relative to the dataset root
        full_path = dataset.samples[index][0]
        return os.path.relpath(full_path, dataset.root)
    elif isinstance(dataset, VideoDataset):
        # filename is constructed by the video dataset
        return dataset.get_filename(index)
    else:
        # dummy to prevent crashes
        return str(index)


def _ensure_dir(path):
    """Makes sure that the directory at path exists."""
    dirname = os.path.dirname(path)
    os.makedirs(dirname, exist_ok=True)


def _copy_image(input_dir, output_dir, filename):
    """Copies an image from the input directory to the output directory."""
    source = os.path.join(input_dir, filename)
    target = os.path.join(output_dir, filename)
    _ensure_dir(target)
    shutil.copyfile(source, target)


def _save_image(image, output_dir, filename, fmt):
    """Saves an image in the output directory."""
    target = os.path.join(output_dir, filename)
    _ensure_dir(target)
    try:
        # try to save the image with the specified format or
        # derive the format from the filename (if format=None)
        image.save(target, format=fmt)
    except ValueError:
        # could not determine format from filename
        image.save(target, format="png")


def _dump_image(dataset, output_dir, filename, index, fmt):
    """Saves a single image to the output directory.

    Will copy the image from the input directory to the output directory
    if possible. If not (e.g. for VideoDatasets), will load the image and
    then save it to the output directory with the specified format.

    """

    if isinstance(dataset, datasets.ImageFolder):
        # can safely copy the image from the input to the output directory
        _copy_image(dataset.root, output_dir, filename)
    elif isinstance(dataset, DatasetFolder):
        # can safely copy the image from the input to the output directory
        _copy_image(dataset.root, output_dir, filename)
    else:
        # need to load the image and save it to the output directory
        image, _ = dataset[index]
        _save_image(image, output_dir, filename, fmt)



================================================
FILE: lightly/data/lightly_subset.py
================================================
from typing import Dict, List, Optional, Tuple, Union

from lightly.data.dataset import LightlyDataset


class LightlySubset(LightlyDataset):  # type: ignore
    def __init__(self, base_dataset: LightlyDataset, filenames_subset: List[str]):
        """Creates a subset of a LightlyDataset by filtering samples based on their filenames.

        Args:
            base_dataset:
                The original dataset from which a subset will be created.
            filenames_subset:
                List of filenames to be included in the subset.
        """
        self.base_dataset = base_dataset
        self.filenames_subset = filenames_subset

        # Create a dictionary mapping filenames to their indices in the base dataset
        dict_base_dataset_filename_index: Dict[str, int] = {}
        for index in range(len(base_dataset)):
            fname = base_dataset.index_to_filename(base_dataset.dataset, index)
            dict_base_dataset_filename_index[fname] = index

        self.mapping_subset_index_to_baseset_index = [
            dict_base_dataset_filename_index[filename] for filename in filenames_subset
        ]

    def __getitem__(self, index_subset: int) -> Tuple[object, object, str]:
        """Retrieves a specific sample from the subset by its index.

        Args:
            index_subset:
                The index of a sample with respect to the subset.
                Index 0 corresponds to the first filename in filenames_subset.

        Returns:
            A tuple containing:
            - The sample data
            - The sample's target/label
            - The sample's filename
        """
        index_baseset = self.mapping_subset_index_to_baseset_index[index_subset]
        sample, target, fname = self.base_dataset.__getitem__(index_baseset)
        return sample, target, fname

    def __len__(self) -> int:
        """Returns the number of samples in the subset.

        Returns:
            The total count of samples in this subset.
        """
        return len(self.filenames_subset)

    def get_filenames(self) -> List[str]:
        """Retrieves the list of filenames in this dataset subset.

        Returns:
            A list of filenames included in this subset.
        """
        return self.filenames_subset

    def index_to_filename(
        self, dataset: Optional[Union[LightlyDataset, None]], index_subset: int
    ) -> str:
        """Converts a subset index to its corresponding filename.

        Args:
            dataset:
                Unused parameter to match the parent class method signature.
            index_subset:
                The index of the sample within the subset.

        Returns:
            The filename of the sample at the specified subset index.
        """
        fname = self.filenames_subset[index_subset]
        return fname

    @property
    def input_dir(self) -> str:
        """Provides access to the input directory of the base dataset.

        Returns:
            The input directory path from the original dataset.
        """
        return str(self.base_dataset.input_dir)

    @property
    def dataset(self) -> LightlyDataset:  # type: ignore
        """Provides access to the underlying dataset of the base dataset.

        Returns:
            The original dataset object.
        """
        return self.base_dataset.dataset



================================================
FILE: lightly/data/multi_view_collate.py
================================================
from typing import List, Tuple
from warnings import warn

import torch
from torch import Tensor


class MultiViewCollate:
    """Collate function that combines views from multiple images into a batch.

    This collate function processes a batch of tuples, where each tuple contains
    multiple views of an image, a label, and a filename. It outputs these as
    separate grouped tensors for easy batch processing.

    Example:
        >>> transform = SimCLRTransform()
        >>> dataset = LightlyDataset(input_dir, transform=transform)
        >>> dataloader = DataLoader(dataset, batch_size=4, collate_fn=MultiViewCollate())
        >>> for views, targets, filenames in dataloader:
        >>>     view0, view1 = views  # each view is a tensor of shape (batch_size, channels, height, width)
    """

    def __call__(
        self, batch: List[Tuple[List[Tensor], int, str]]
    ) -> Tuple[List[Tensor], Tensor, List[str]]:
        """Turns a batch of (views, label, filename) tuples into a single
        (views, labels, filenames) tuple.

        Args:
            batch:
                The input batch as a list of (views, label, filename) tuples, one for
                each image in the batch. `views` is a list of N view tensors, each
                representing a transformed version of the original image. `label` and
                `filename` are the class label and filename for the corresponding image.

                Example:
                    >>> batch = [
                    >>>     ([img_0_view_0, ..., img_0_view_N], label_0, filename_0),   # image 0
                    >>>     ([img_1_view_0, ..., img_1_view_N], label_1, filename_1),   # image 1
                    >>>     ...
                    >>>     ([img_B_view_0, ..., img_B_view_N], label_B, filename_B),  # image B
                    >>> ]

        Returns:
            A tuple containing:
                - **views**: A list of tensors, where each tensor corresponds to one view
                  of every image in the batch. Tensors are concatenated along the batch
                  dimension.
                - **labels**: A tensor of shape `(batch_size,)` with `torch.long` dtype,
                  containing the labels for all images in the batch.
                - **filenames**: A list of strings containing filenames for all images
                  in the batch.

            Example:
                >>> output = (
                >>>     [
                >>>         Tensor([img_0_view_0, ..., img_B_view_0]),    # view 0
                >>>         Tensor([img_0_view_1, ..., img_B_view_1]),    # view 1
                >>>         ...
                >>>         Tensor([img_0_view_N, ..., img_B_view_N]),    # view N
                >>>     ],
                >>>     torch.tensor([label_0, ..., label_B], dtype=torch.long),
                >>>     [filename_0, ..., filename_B],
                >>> )

        Notes:
            If the input batch is empty, a warning is issued, and an empty tuple
            `([], [], [])` is returned.
        """
        labels: List[int] = []
        fnames: List[str] = []

        if len(batch) == 0:
            warn("MultiViewCollate received empty batch.")
            return [], torch.tensor(labels, dtype=torch.long), fnames

        views: List[List[Tensor]] = [[] for _ in range(len(batch[0][0]))]
        for img, label, fname in batch:
            for i, view in enumerate(img):
                views[i].append(view.unsqueeze(0))
            labels.append(label)
            fnames.append(fname)

        unsqueezed_views = [torch.cat(unsqueezed_view) for unsqueezed_view in views]

        return unsqueezed_views, torch.tensor(labels, dtype=torch.long), fnames



================================================
FILE: lightly/loss/__init__.py
================================================
"""The lightly.loss package provides loss functions for self-supervised learning. """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved
from lightly.loss.barlow_twins_loss import BarlowTwinsLoss
from lightly.loss.dcl_loss import DCLLoss, DCLWLoss
from lightly.loss.detcon_loss import DetConBLoss, DetConSLoss
from lightly.loss.dino_loss import DINOLoss
from lightly.loss.directclr_loss import DirectCLRLoss
from lightly.loss.emp_ssl_loss import EMPSSLLoss
from lightly.loss.ibot_loss import IBOTPatchLoss
from lightly.loss.koleo_loss import KoLeoLoss
from lightly.loss.macl_loss import MACLLoss
from lightly.loss.mmcr_loss import MMCRLoss
from lightly.loss.msn_loss import MSNLoss
from lightly.loss.negative_cosine_similarity import NegativeCosineSimilarity
from lightly.loss.ntx_ent_loss import NTXentLoss
from lightly.loss.pmsn_loss import PMSNCustomLoss, PMSNLoss
from lightly.loss.swav_loss import SwaVLoss
from lightly.loss.sym_neg_cos_sim_loss import SymNegCosineSimilarityLoss
from lightly.loss.tico_loss import TiCoLoss
from lightly.loss.vicreg_loss import VICRegLoss
from lightly.loss.vicregl_loss import VICRegLLoss
from lightly.loss.wmse_loss import Whitening2d, WMSELoss



================================================
FILE: lightly/loss/barlow_twins_loss.py
================================================
from typing import Tuple

import torch
import torch.distributed as dist
import torch.nn.functional as F
from torch import Tensor


class BarlowTwinsLoss(torch.nn.Module):
    """Implementation of the Barlow Twins Loss from Barlow Twins[0] paper.

    This code specifically implements the Figure Algorithm 1 from [0].
    [0] Zbontar,J. et.al, 2021, Barlow Twins... https://arxiv.org/abs/2103.03230

    Examples:
        >>> # initialize loss function
        >>> loss_fn = BarlowTwinsLoss()
        >>>
        >>> # generate two random transforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # feed through SimSiam model
        >>> out0, out1 = model(t0, t1)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(out0, out1)
    """

    def __init__(self, lambda_param: float = 5e-3, gather_distributed: bool = False):
        """Lambda param configuration with default value like in [0]

        Initializes the BarlowTwinsLoss with the specified parameters.

        Args:
            lambda_param:
                Parameter for importance of redundancy reduction term.
            gather_distributed:
                If True, the cross-correlation matrices from all GPUs are
                gathered and summed before the loss calculation.

        Raises:
            ValueError: If gather_distributed is True but torch.distributed is not available.
        """
        super(BarlowTwinsLoss, self).__init__()
        self.lambda_param = lambda_param
        self.gather_distributed = gather_distributed

        if gather_distributed and not dist.is_available():
            raise ValueError(
                "gather_distributed is True but torch.distributed is not available."
                "Please set gather_distributed=False or install a torch version with "
                "distributed support."
            )

    def forward(self, z_a: torch.Tensor, z_b: torch.Tensor) -> torch.Tensor:
        """Computes the Barlow Twins loss for the given projections.

        Args:
            z_a: Output projections of the first set of transformed images.
            z_b: Output projections of the second set of transformed images.

        Returns:
            Computed Barlow Twins Loss.
        """

        # Normalize repr. along the batch dimension
        z_a_norm, z_b_norm = _normalize(z_a), _normalize(z_b)

        N = z_a.size(0)

        # Compute the cross-correlation matrix
        c = z_a_norm.T @ z_b_norm
        c.div_(N)

        # Aggregate and normalize the cross-correlation matrix between multiple GPUs
        if self.gather_distributed and dist.is_initialized():
            world_size = dist.get_world_size()
            if world_size > 1:
                c = c / world_size
                dist.all_reduce(c)

        invariance_loss = torch.diagonal(c).add_(-1).pow_(2).sum()
        redundancy_reduction_loss = _off_diagonal(c).pow_(2).sum()
        loss: Tensor = invariance_loss + self.lambda_param * redundancy_reduction_loss

        return loss


def _normalize(z: torch.Tensor) -> torch.Tensor:
    """Helper function to create batches of mean 0 and std 1."""
    return F.batch_norm(
        z,
        running_mean=None,
        running_var=None,
        weight=None,
        bias=None,
        training=True,
    )


def _off_diagonal(x: Tensor) -> Tensor:
    """Returns a flattened view of the off-diagonal elements of a square matrix."""

    # Ensure the input is a square matrix
    n, m = x.shape
    assert n == m

    # Flatten the matrix and extract off-diagonal elements
    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()



================================================
FILE: lightly/loss/dcl_loss.py
================================================
from functools import partial
from typing import Callable, Optional

import torch
from torch import Tensor
from torch import distributed as torch_dist
from torch import nn

from lightly.utils import dist


def negative_mises_fisher_weights(
    out0: Tensor, out1: Tensor, sigma: float = 0.5
) -> Tensor:
    """Negative Mises-Fisher weighting function as presented in Decoupled Contrastive Learning [0].

    The implementation was inspired by [1].

    - [0] Chun-Hsiao Y. et. al., 2021, Decoupled Contrastive Learning https://arxiv.org/abs/2110.06848
    - [1] https://github.com/raminnakhli/Decoupled-Contrastive-Learning

    Args:
        out0:
            Output projections of the first set of transformed images.
            Shape: (batch_size, embedding_size)
        out1:
            Output projections of the second set of transformed images.
            Shape: (batch_size, embedding_size)
        sigma:
            Similarities are scaled by inverse sigma.
    Returns:
        A tensor with shape (batch_size,) where each entry is the weight for one
        of the input images.
    """
    similarity = torch.einsum("nm,nm->n", out0.detach(), out1.detach()) / sigma

    # Return negative Mises-Fisher weights
    return torch.tensor(2 - out0.shape[0] * nn.functional.softmax(similarity, dim=0))


class DCLLoss(nn.Module):
    """Implementation of the Decoupled Contrastive Learning Loss from Decoupled Contrastive Learning [0].

    This code implements Equation 6 in [0], including the sum over all images `i`
    and views `k`. The loss is reduced to a mean loss over the mini-batch.
    The implementation was inspired by [1].

    - [0] Chun-Hsiao Y. et. al., 2021, Decoupled Contrastive Learning https://arxiv.org/abs/2110.06848
    - [1] https://github.com/raminnakhli/Decoupled-Contrastive-Learning

    Attributes:
        temperature:
            Similarities are scaled by inverse temperature.
        weight_fn:
            Weighting function `w` from the paper. Scales the loss between the
            positive views (views from the same image). No weighting is performed
            if weight_fn is None. The function must take the two input tensors
            passed to the forward call as input and return a weight tensor. The
            returned weight tensor must have the same length as the input tensors.
        gather_distributed:
            If True, negatives from all GPUs are gathered before the
            loss calculation.

    Examples:
        >>> loss_fn = DCLLoss(temperature=0.07)
        >>>
        >>> # generate two random transforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # embed images using some model, for example SimCLR
        >>> out0 = model(t0)
        >>> out1 = model(t1)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(out0, out1)
        >>>
        >>> # you can also add a custom weighting function
        >>> weight_fn = lambda out0, out1: torch.sum((out0 - out1) ** 2, dim=1)
        >>> loss_fn = DCLLoss(weight_fn=weight_fn)
    """

    def __init__(
        self,
        temperature: float = 0.1,
        weight_fn: Optional[Callable[[Tensor, Tensor], Tensor]] = None,
        gather_distributed: bool = False,
    ):
        """Initialzes the DCLoss module.

        Args:
            temperature:
                Similarities are scaled by inverse temperature.
            weight_fn:
                 Weighting function `w` from the paper. Scales the loss between the
                positive views (views from the same image). No weighting is performed
                if weight_fn is None. The function must take the two input tensors
                passed to the forward call as input and return a weight tensor. The
                returned weight tensor must have the same length as the input tensors.
            gather_distributed:
                If True, negatives from all GPUs are gathered before the
                loss calculation.

        Raises:
            ValuesError: If gather_distributed is True but torch.distributed is not available.
        """
        super().__init__()
        self.temperature = temperature
        self.weight_fn = weight_fn
        self.gather_distributed = gather_distributed

        # Check if distributed gathering is enabled but torch.distributed is not available
        if gather_distributed and not torch_dist.is_available():
            raise ValueError(
                "gather_distributed is True but torch.distributed is not available. "
                "Please set gather_distributed=False or install a torch version with "
                "distributed support."
            )

    def forward(
        self,
        out0: Tensor,
        out1: Tensor,
    ) -> Tensor:
        """Forward pass of the DCL loss.

        Args:
            out0:
                Output projections of the first set of transformed images.
                Shape: (batch_size, embedding_size)
            out1:
                Output projections of the second set of transformed images.
                Shape: (batch_size, embedding_size)

        Returns:
            Mean loss over the mini-batch.
        """
        # Normalize the output to length 1
        out0 = nn.functional.normalize(out0, dim=1)
        out1 = nn.functional.normalize(out1, dim=1)

        if self.gather_distributed and dist.world_size() > 1:
            # Gather representations from other processes if necessary
            out0_all = torch.cat(dist.gather(out0), 0)
            out1_all = torch.cat(dist.gather(out1), 0)
        else:
            out0_all = out0
            out1_all = out1

        # Calculate symmetric loss
        loss0: Tensor = self._loss(out0, out1, out0_all, out1_all)
        loss1: Tensor = self._loss(out1, out0, out1_all, out0_all)

        # Return the mean loss over the mini-batch
        return 0.5 * (loss0 + loss1)

    def _loss(
        self, out0: Tensor, out1: Tensor, out0_all: Tensor, out1_all: Tensor
    ) -> Tensor:
        """Calculates DCL loss for out0 with respect to its positives in out1
        and the negatives in out1, out0_all, and out1_all.

        This code implements Equation 6 in [0], including the sum over all images `i`
        but with `k` fixed at 0.

        Args:
            out0:
                Output projections of the first set of transformed images.
                Shape: (batch_size, embedding_size)
            out1:
                Output projections of the second set of transformed images.
                Shape: (batch_size, embedding_size)
            out0_all:
                Output projections of the first set of transformed images from
                all distributed processes/gpus. Should be equal to out0 in an
                undistributed setting.
                Shape: (batch_size * world_size, embedding_size)
            out1_all:
                Output projections of the second set of transformed images from
                all distributed processes/GPUs. Should be equal to out1 in an
                undistributed setting.
                Shape: (batch_size * world_size, embedding_size)

        Returns:
            Mean loss over the mini-batch.
        """
        # Create diagonal mask that only selects similarities between
        # representations of the same images
        batch_size = out0.shape[0]
        if self.gather_distributed and dist.world_size() > 1:
            diag_mask = dist.eye_rank(batch_size, device=out0.device)
        else:
            diag_mask = torch.eye(batch_size, device=out0.device, dtype=torch.bool)

        # Calculate similarities (n = batch_size, m = batch_size * world_size)
        sim_00 = torch.einsum("nc,mc->nm", out0, out0_all) / self.temperature
        sim_01 = torch.einsum("nc,mc->nm", out0, out1_all) / self.temperature

        positive_loss = -sim_01[diag_mask]
        if self.weight_fn:
            positive_loss = positive_loss * self.weight_fn(out0, out1)

        # Remove simliarities between same views of the same image
        sim_00 = sim_00[~diag_mask].view(batch_size, -1)

        # Remove similarities between different views of the same images
        # This is the key difference compared to NTXentLoss
        sim_01 = sim_01[~diag_mask].view(batch_size, -1)

        all_negs = torch.cat([sim_00, sim_01], dim=1)
        negative_loss = torch.logsumexp(all_negs, dim=1)  # log(sum exp over *all* negs)
        return (positive_loss + negative_loss).mean()


class DCLWLoss(DCLLoss):
    """Implementation of the Weighted Decoupled Contrastive Learning Loss from
    Decoupled Contrastive Learning [0].

    This code implements Equation 6 in [0] with a negative Mises-Fisher
    weighting function. The loss returns the mean over all images `i` and
    views `k` in the mini-batch. The implementation was inspired by [1].

    - [0] Chun-Hsiao Y. et. al., 2021, Decoupled Contrastive Learning https://arxiv.org/abs/2110.06848
    - [1] https://github.com/raminnakhli/Decoupled-Contrastive-Learning

    Attributes:
        temperature:
            Similarities are scaled by inverse temperature.
        sigma:
            Similar to temperature but applies the inverse scaling in the
            weighting function.
        gather_distributed:
            If True, negatives from all GPUs are gathered before the
            loss calculation.

    Examples:
        >>> loss_fn = DCLWLoss(temperature=0.07)
        >>>
        >>> # generate two random transforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # embed images using some model, for example SimCLR
        >>> out0 = model(t0)
        >>> out1 = model(t1)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(out0, out1)
    """

    def __init__(
        self,
        temperature: float = 0.1,
        sigma: float = 0.5,
        gather_distributed: bool = False,
    ):
        """Initializes the DCLWLoss module.

        Args:
            temperature:
                Similarities are scaled by inverse temperature.
            sigma:
                Applies inverse scaling in the weighting function.
            gather_distributed:
                If True, negatives from all GPUs are gathered before the
                loss calculation.
        """
        super().__init__(
            temperature=temperature,
            weight_fn=partial(negative_mises_fisher_weights, sigma=sigma),
            gather_distributed=gather_distributed,
        )



================================================
FILE: lightly/loss/detcon_loss.py
================================================
import torch
import torch.nn.functional as F
from torch import Tensor
from torch import distributed as torch_dist
from torch.nn import Module

import lightly.utils.dist as lightly_dist


class DetConSLoss(Module):
    """Implementation of the DetConS loss. [2]_

    The inputs are two-fold:

    - Two latent representations of the same batch under different views, as generated\
        by SimCLR [3]_ and additional pooling over the regions of the segmentation.
    - Two integer masks that indicate the regions of the segmentation that were used\
        for pooling.

    For calculating the contrastive loss, regions under the same mask in the same image
    (under a different view) are considered as positives and everything else as 
    negatives. With :math:`v_m` and :math:`v_{m'}'` being the pooled feature maps under 
    mask :math:`m` and :math:`m'` respectively, and additionally scaled to a norm of 
    :math:`\\frac{1}{\\sqrt{\\tau}}`, the formula for the contrastive loss is

    .. math::
        \\mathcal{L} = \\sum_{m}\\sum_{m'} \\mathbb{1}_{m, m'} \\left[ - \\log\
            \\frac{\\exp(v_m \\cdot v_{m'}')}{\\exp(v_m \\cdot v_{m'}') +\
            \\sum_{n}\\exp (v_m \\cdot v_{m'}')} \\right]

    where :math:`\\mathbb{1}_{m, m'}` is 1 if the masks are the same and 0 otherwise.

    References:
        .. [2] DetCon https://arxiv.org/abs/2103.10957
        .. [3] SimCLR https://arxiv.org/abs/2002.05709

    Attributes:
        temperature:
            The temperature :math:`\\tau` in the contrastive loss.
        gather_distributed:
            If True, the similarity matrix is gathered across all GPUs before the loss
            is calculated. Else, the loss is calculated on each GPU separately.
    """

    def __init__(
        self, temperature: float = 0.1, gather_distributed: bool = True
    ) -> None:
        super().__init__()
        self.detconbloss = DetConBLoss(
            temperature=temperature, gather_distributed=gather_distributed
        )

    def forward(
        self, view0: Tensor, view1: Tensor, mask_view0: Tensor, mask_view1: Tensor
    ) -> Tensor:
        """Calculate the contrastive loss under the same mask in the same image.

        The tensor shapes and value ranges are given by variables :math:`B, M, D, N`,
        where :math:`B` is the batch size, :math:`M` is the sampled number of image
        masks / regions, :math:`D` is the embedding size and :math:`N` is the total
        number of masks.

        Args:
            view0: Mask-pooled output for the first view, a float tensor of shape
                :math:`(B, M, D)`.
            pred_view1: Mask-pooled output for the second view, a float tensor of shape
                :math:`(B, M, D)`.
            mask_view0: Indices corresponding to the sampled masks for the first view,
                an integer tensor of shape :math:`(B, M)` with (possibly repeated)
                indices in the range :math:`[0, N)`.
            mask_view1: Indices corresponding to the sampled masks for the second view,
                an integer tensor of shape (B, M) with (possibly repeated) indices in
                the range :math:`[0, N)`.

        Returns:
            A scalar float tensor containing the contrastive loss.
        """
        loss: Tensor = self.detconbloss(
            view0, view1, view0, view1, mask_view0, mask_view1
        )
        return loss


class DetConBLoss(Module):
    """Implementation of the DetConB loss. [0]_

    The inputs are three-fold:

    - Two latent representations of the same batch under different views, as generated\
        by BYOL's [1]_ prediction branch and additional pooling over the regions of\
        the segmentation.
    - Two latent representations of the same batch under different views, as generated\
        by BYOL's target branch and additional pooling over the regions of the\
        segmentation.
    - Two integer masks that indicate the regions of the segmentation that were used\
        for pooling.

    For calculating the contrastive loss, regions under the same mask in the same image
    (under a different view) are considered as positives and everything else as 
    negatives. With :math:`v_m` and :math:`v_{m'}'` being the pooled feature maps under 
    mask :math:`m` and :math:`m'` respectively, and additionally scaled to a norm of 
    :math:`\\frac{1}{\\sqrt{\\tau}}`, the formula for the contrastive loss is

    .. math::
        \\mathcal{L} = \\sum_{m}\\sum_{m'} \\mathbb{1}_{m, m'} \\left[ - \\log \
        \\frac{\\exp(v_m \\cdot v_{m'}')}{\\exp(v_m \\cdot v_{m'}') + \\sum_{n}\\exp \
        (v_m \\cdot v_{m'}')} \\right]

    where :math:`\\mathbb{1}_{m, m'}` is 1 if the masks are the same and 0 otherwise.
    Since :math:`v_m` and :math:`v_{m'}'` stem from different branches, the loss is
    symmetrized by also calculating the loss with the roles of the views reversed. [1]_

    References:
        .. [0] DetCon https://arxiv.org/abs/2103.10957
        .. [1] BYOL https://arxiv.org/abs/2006.07733

    Attributes:
        temperature:
            The temperature :math:`\\tau` in the contrastive loss.
        gather_distributed:
            If True, the similarity matrix is gathered across all GPUs before the loss
            is calculated. Else, the loss is calculated on each GPU separately.
    """

    def __init__(
        self, temperature: float = 0.1, gather_distributed: bool = True
    ) -> None:
        super().__init__()
        self.eps = 1e-8
        self.temperature = temperature
        self.gather_distributed = gather_distributed
        if abs(self.temperature) < self.eps:
            raise ValueError(f"Illegal temperature: abs({self.temperature}) < 1e-8")
        if self.gather_distributed and not torch_dist.is_available():
            raise ValueError(
                "gather_distributed is True but torch.distributed is not available. "
                "Please set gather_distributed=False or install a torch version with "
                "distributed support."
            )

    def forward(
        self,
        pred_view0: Tensor,
        pred_view1: Tensor,
        target_view0: Tensor,
        target_view1: Tensor,
        mask_view0: Tensor,
        mask_view1: Tensor,
    ) -> Tensor:
        """Calculate the contrastive loss under the same mask in the same image.

        The tensor shapes and value ranges are given by variables :math:`B, M, D, N`,
        where :math:`B` is the batch size, :math:`M` is the sampled number of image
        masks / regions, :math:`D` is the embedding size and :math:`N` is the total
        number of masks.

        Args:
            pred_view0: Mask-pooled output of the prediction branch for the first view,
                a float tensor of shape :math:`(B, M, D)`.
            pred_view1: Mask-pooled output of the prediction branch for the second view,
                a float tensor of shape :math:`(B, M, D)`.
            target_view0: Mask-pooled output of the target branch for the first view,
                a float tensor of shape :math:`(B, M, D)`.
            target_view1: Mask-pooled output of the target branch for the second view,
                a float tensor of shape :math:`(B, M, D)`.
            mask_view0: Indices corresponding to the sampled masks for the first view,
                an integer tensor of shape :math:`(B, M)` with (possibly repeated)
                indices in the range :math:`[0, N)`.
            mask_view1: Indices corresponding to the sampled masks for the second view,
                an integer tensor of shape (B, M) with (possibly repeated) indices in
                the range :math:`[0, N)`.

        Returns:
            A scalar float tensor containing the contrastive loss.
        """
        b, m, d = pred_view0.size()
        infinity_proxy = 1e9

        # gather distributed
        if self.gather_distributed and lightly_dist.world_size() > 1:
            target_view0_large = torch.cat(lightly_dist.gather(target_view0), dim=0)
            target_view1_large = torch.cat(lightly_dist.gather(target_view1), dim=0)
            replica_id = lightly_dist.rank()
            labels_idx = torch.arange(b, device=pred_view0.device) + replica_id * b
            enlarged_b = b * lightly_dist.world_size()
            labels_local = F.one_hot(labels_idx, num_classes=enlarged_b)
        else:
            target_view0_large = target_view0
            target_view1_large = target_view1
            labels_local = torch.eye(b, device=pred_view0.device)
            enlarged_b = b

        # normalize
        pred_view0 = F.normalize(pred_view0, p=2, dim=2)
        pred_view1 = F.normalize(pred_view1, p=2, dim=2)
        target_view0_large = F.normalize(target_view0_large, p=2, dim=2)
        target_view1_large = F.normalize(target_view1_large, p=2, dim=2)

        ### Expand Labels ###
        # labels_local at this point only points towards the diagonal of the batch, i.e.
        # indicates to compare between the same samples across views.
        labels_local = labels_local[:, None, :, None]  # (b, 1, b * world_size, 1)

        ### Calculate Similarity Matrices ###
        # tensors of shape (b, m, b * world_size, m), indicating similarities between regions across
        # views and samples in the batch
        logits_aa = (
            torch.einsum("abk,uvk->abuv", pred_view0, target_view0_large)
            / self.temperature
        )
        logits_bb = (
            torch.einsum("abk,uvk->abuv", pred_view1, target_view1_large)
            / self.temperature
        )
        logits_ab = (
            torch.einsum("abk,uvk->abuv", pred_view0, target_view1_large)
            / self.temperature
        )
        logits_ba = (
            torch.einsum("abk,uvk->abuv", pred_view1, target_view0_large)
            / self.temperature
        )

        ### Find Corresponding Regions Across Views ###
        same_mask_aa = _same_mask(mask_view0, mask_view0)
        same_mask_bb = _same_mask(mask_view1, mask_view1)
        same_mask_ab = _same_mask(mask_view0, mask_view1)
        same_mask_ba = _same_mask(mask_view1, mask_view0)

        ### Remove Similarities Between Corresponding Views But Different Regions ###
        # labels_local initially compared all features across views, but we only want to
        # compare the same regions across views.
        # (b, 1, b * world_size, 1) * (b, m, 1, m) -> (b, m, b * world_size, m)
        labels_aa = labels_local * same_mask_aa
        labels_bb = labels_local * same_mask_bb
        labels_ab = labels_local * same_mask_ab
        labels_ba = labels_local * same_mask_ba

        ### Remove Logits And Lables Between The Same View ###
        logits_aa = logits_aa - infinity_proxy * labels_aa
        logits_bb = logits_bb - infinity_proxy * labels_bb
        labels_aa = 0.0 * labels_aa
        labels_bb = 0.0 * labels_bb

        ### Arrange Labels ###
        # (b, m, b * world_size * 2, m)
        labels_abaa = torch.cat([labels_ab, labels_aa], dim=2)
        labels_babb = torch.cat([labels_ba, labels_bb], dim=2)
        # (b, m, b * world_size * 2 * m)
        labels_0 = labels_abaa.view(b, m, -1)
        labels_1 = labels_babb.view(b, m, -1)

        ### Count Number of Positives For Every Region (per sample) ###
        num_positives_0 = torch.sum(labels_0, dim=-1, keepdim=True)
        num_positives_1 = torch.sum(labels_1, dim=-1, keepdim=True)

        ### Scale The Labels By The Number of Positives To Weight Loss Value ###
        labels_0 = labels_0 / torch.maximum(num_positives_0, torch.tensor(1))
        labels_1 = labels_1 / torch.maximum(num_positives_1, torch.tensor(1))

        ### Count How Many Overlapping Regions We Have Across Views ###
        obj_area_0 = torch.sum(same_mask_aa, dim=(2, 3))
        obj_area_1 = torch.sum(same_mask_bb, dim=(2, 3))
        # make sure we don't divide by zero
        obj_area_0 = torch.maximum(obj_area_0, torch.tensor(self.eps))
        obj_area_1 = torch.maximum(obj_area_1, torch.tensor(self.eps))

        ### Calculate Weights For The Loss ###
        # last dim of num_positives is anyway 1, from the torch.sum above
        weights_0 = torch.gt(num_positives_0.squeeze(-1), 1e-3).float()
        weights_0 = weights_0 / obj_area_0
        weights_1 = torch.gt(num_positives_1.squeeze(-1), 1e-3).float()
        weights_1 = weights_1 / obj_area_1

        ### Arrange Logits ###
        logits_abaa = torch.cat([logits_ab, logits_aa], dim=2)
        logits_babb = torch.cat([logits_ba, logits_bb], dim=2)
        logits_abaa = logits_abaa.view(b, m, -1)
        logits_babb = logits_babb.view(b, m, -1)

        # return labels_0, logits_abaa, weights_0, labels_1, logits_babb, weights_1

        ### Derive Cross Entropy Loss ###
        # targets/labels are are a weighted float tensor of same shape as logits,
        # which is why we can't use F.cross_entropy (expects integer targets)
        loss_a = _torch_manual_cross_entropy(labels_0, logits_abaa, weights_0)
        loss_b = _torch_manual_cross_entropy(labels_1, logits_babb, weights_1)
        loss = loss_a + loss_b
        return loss


def _same_mask(mask0: Tensor, mask1: Tensor) -> Tensor:
    """Find equal masks/regions across views of the same image.

    Args:
        mask0: Indices corresponding to the sampled masks for the first view,
            an integer tensor of shape :math:`(B, M)` with (possibly repeated)
            indices in the range :math:`[0, N)`.
        mask1: Indices corresponding to the sampled masks for the second view,
            an integer tensor of shape (B, M) with (possibly repeated) indices in
            the range :math:`[0, N)`.

    Returns:
        Tensor: A float tensor of shape :math:`(B, M, 1, M)` where the first :math:`M`
            dimensions is for the regions/masks of the first view and the last :math:`M`
            dimensions is for the regions/masks of the second view. For every sample
            :math:`k` in the batch (separately), the tensor is effectively a 2D index
            matrix where the entry :math:`(k, i, :, j)` is 1 if the masks :math:`mask0(k, i)`
            and :math:`mask1(k, j)'` are the same and 0 otherwise.
    """
    # Efficiently compute (B, M, M) bool mask and directly unsqueeze at dim=2 to get (B, M, 1, M)
    return (mask0.unsqueeze(2) == mask1.unsqueeze(1)).float().unsqueeze(2)


def _torch_manual_cross_entropy(
    labels: Tensor, logits: Tensor, weight: Tensor
) -> Tensor:
    ce = -weight * torch.sum(labels * F.log_softmax(logits, dim=-1), dim=-1)
    return torch.mean(ce)



================================================
FILE: lightly/loss/dino_loss.py
================================================
from __future__ import annotations

import warnings

import torch
import torch.nn.functional as F
from torch import Tensor
from torch.nn import Module, Parameter

from lightly.models.modules import center
from lightly.models.modules.center import CENTER_MODE_TO_FUNCTION


class DINOLoss(Module):
    """Implementation of the loss described in 'Emerging Properties in
    Self-Supervised Vision Transformers'. [0]

    This implementation follows the code published by the authors. [1]
    It supports global and local image crops. A linear warmup schedule for the
    teacher temperature is implemented to stabilize training at the beginning.
    Centering is applied to the teacher output to avoid model collapse.

    - [0]: DINO, 2021, https://arxiv.org/abs/2104.14294
    - [1]: https://github.com/facebookresearch/dino

    Attributes:
        output_dim:
            Dimension of the model output.
        teacher_temp:
            Temperature parameter for the teacher network.
        student_temp:
            Temperature parameter for the student network.
        center:
            Center used for the teacher output. It is updated with a moving average
            during training.
        center_momentum:
            Momentum term for the center calculation.
        warmup_teacher_temp_epochs:
                Number of epochs for the warmup phase of the teacher temperature (for backward compatibility).
        teacher_temp_schedule:
            A linear schedule for the teacher temperature during the warmup phase (for backward compatibility).

    Examples:
        >>> # initialize loss function
        >>> loss_fn = DINOLoss(128)
        >>>
        >>> # generate a view of the images with a random transform
        >>> view = transform(images)
        >>>
        >>> # embed the view with a student and teacher model
        >>> teacher_out = teacher(view)
        >>> student_out = student(view)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn([teacher_out], [student_out])
    """

    def __init__(
        self,
        output_dim: int = 65536,
        warmup_teacher_temp: float = 0.04,
        teacher_temp: float = 0.04,
        warmup_teacher_temp_epochs: int = 30,
        student_temp: float = 0.1,
        center_momentum: float = 0.9,
        center_mode: str = "mean",
    ) -> None:
        """Initializes the DINOLoss Module.

        Args:
            center_mode:
                Mode for center calculation. Only 'mean' is supported.
            warmup_teacher_temp:
                Initial temperature for the teacher network (for backward compatibility).
            warmup_teacher_temp_epochs:
                Number of epochs for the warmup phase of the teacher temperature (for backward compatibility).
        """
        super().__init__()

        self.teacher_temp = teacher_temp
        self.student_temp = student_temp

        # TODO(Guarin, 08/24): Refactor this to use the Center module directly once
        # we do a breaking change.
        if center_mode not in CENTER_MODE_TO_FUNCTION:
            raise ValueError(
                f"Unknown mode '{center_mode}'. Valid modes are "
                f"{sorted(CENTER_MODE_TO_FUNCTION.keys())}."
            )
        self._center_fn = CENTER_MODE_TO_FUNCTION[center_mode]
        self.center: Parameter
        self.register_buffer("center", torch.zeros(1, 1, output_dim))
        self.center_momentum = center_momentum

        # comput the warmup teacher temperature internally for backward compatibility
        self.warmup_teacher_temp_epochs = warmup_teacher_temp_epochs
        self.teacher_temp_schedule = torch.linspace(
            start=warmup_teacher_temp,
            end=teacher_temp,
            steps=warmup_teacher_temp_epochs,
        )

    def forward(
        self,
        teacher_out: list[Tensor],
        student_out: list[Tensor],
        teacher_temp: float | None = None,
        epoch: int | None = None,
    ) -> Tensor:
        """Cross-entropy between softmax outputs of the teacher and student networks.

        Args:
            teacher_out:
                List of tensors with shape (batch_size, output_dim) containing features
                from the teacher model. Each tensor must represent one view of the
                batch.
            student_out:
                List of tensors with shape (batch_size, output_dim) containing features
                from the student model. Each tensor must represent one view of the
                batch.
            teacher_temp:
                The temperature used for the teacher output. If None, the default
                temperature defined in __init__ is used.
            epoch:
                The current epoch for backward compatibility.

        Returns:
            The average cross-entropy loss.
        """

        # Get teacher temperature
        if teacher_temp is not None:
            teacher_temperature = torch.tensor(teacher_temp)
        elif epoch is not None:  # for backward compatibility
            if epoch < self.warmup_teacher_temp_epochs:
                teacher_temperature = self.teacher_temp_schedule[epoch]
            else:
                teacher_temperature = torch.tensor(self.teacher_temp)
        else:
            teacher_temperature = torch.tensor(self.teacher_temp)

        # Calculate cross-entropy loss.
        teacher_out_stacked = torch.stack(teacher_out)
        t_out: Tensor = F.softmax(
            (teacher_out_stacked - self.center) / teacher_temperature, dim=-1
        )
        student_out_stacked = torch.stack(student_out)
        s_out = F.log_softmax(student_out_stacked / self.student_temp, dim=-1)

        # Calculate feature similarities, ignoring the diagonal
        # b = batch_size, t = n_views_teacher, s = n_views_student, d = output_dim
        loss = -torch.einsum("tbd,sbd->ts", t_out, s_out)
        loss.fill_diagonal_(0)

        # Number of loss terms, ignoring the diagonal
        n_terms = loss.numel() - loss.diagonal().numel()
        batch_size = teacher_out_stacked.shape[1]

        loss = loss.sum() / (n_terms * batch_size)

        # Update the center used for the teacher output
        self.update_center(teacher_out_stacked)

        return loss

    @torch.no_grad()
    def update_center(self, teacher_out: Tensor) -> None:
        """Moving average update of the center used for the teacher output.

        Args:
            teacher_out:
                Tensor with shape (num_views, batch_size, output_dim) containing
                features from the teacher model.
        """

        # Calculate the batch center using the specified center function
        batch_center = self._center_fn(x=teacher_out, dim=(0, 1))

        # Update the center with a moving average
        self.center.data = center.center_momentum(
            center=self.center, batch_center=batch_center, momentum=self.center_momentum
        )



================================================
FILE: lightly/loss/directclr_loss.py
================================================
""" Contrastive Loss Functions """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved


from typing import Sequence, Union

from torch import Tensor

from lightly.loss.ntx_ent_loss import NTXentLoss


class DirectCLRLoss(NTXentLoss):
    """Implementation of the NT-Xent based DirectCLR Loss.

    Following the DirectCLR[0] paper, this loss should be used without projection
    head. Set `loss_dim` to the desired truncated representation length.
    DirectCLRLoss inherits from NTXentLoss, its parameters can be set after
    setting `loss_dim`.

    - [0] DirectCLR, 2021, https://arxiv.org/abs/2110.09348

    Attributes:
        loss_dim:
            Computes the loss only on the first loss_dim values of the encoding.
        temperature:
            From NTXentLoss: scale logits by the inverse of the temperature.
        memory_bank_size:
            From NTXentLoss: size of the memory bank as (num_features, dim) tuple.
            num_features are the number of negative samples stored in the memory bank.
            If num_features is 0, the memory bank is disabled. Use 0 for SimCLR. For
            MoCo we typically use numbers like 4096 or 65536.
            Deprecated: If only a single integer is passed, it is interpreted as the
            number of features and the feature dimension is inferred from the first
            batch stored in the memory bank. Leaving out the feature dimension might
            lead to errors in distributed training.
        gather_distributed:
            From NTXentLoss: if True then negatives from all GPUs are gathered before
            the loss calculation. If a memory bank is used and gather_distributed is
            True, then tensors from all gpus are gathered before the memory bank is
            updated.

    Examples:
        >>> # initialize loss function
        >>> loss_fn = DirectCLRLoss()
        >>>
        >>> # generate two random transforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # feed through backbone without projection head
        >>> out0, out1 = model(t0), model(t1)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(out0, out1)

    """

    def __init__(
        self,
        loss_dim: int = 64,
        temperature: float = 0.5,
        memory_bank_size: Union[int, Sequence[int]] = 0,
        gather_distributed: bool = False,
    ):
        """Initializes the DirectCLRLoss module with the specified parameters.

        Args:
            loss_dim:
                Computes the loss only on the first `loss_dim` values of the encoding.
            temperature:
                 Scale logits by the inverse of the temperature.
            memory_bank_size:
                 Size of the memory bank.
            gather_distributed:
                 If True, negatives from all GPUs are gathered before the loss calculation.
        """
        super().__init__(
            temperature=temperature,
            memory_bank_size=memory_bank_size,
            gather_distributed=gather_distributed,
        )
        self.loss_dim = loss_dim

    def forward(self, out0: Tensor, out1: Tensor) -> Tensor:
        """Forward pass through DirectCLR Loss.

        To be used directly on the encoding without projection head. Flattens
        each output encoding and truncates it to `loss_dim` length, then computes
        the NTXentLoss.

        Args:
            out0:
                Output projections of the first set of transformed images.
                Shape: (batch_size, embedding_size)
            out1:
                Output projections of the second set of transformed images.
                Shape: (batch_size, embedding_size)

        Returns:
            DirectCLR Loss value.
        """

        out0 = out0.flatten(start_dim=1)[:, : self.loss_dim]
        out1 = out1.flatten(start_dim=1)[:, : self.loss_dim]

        loss: Tensor = super().forward(out0, out1)

        return loss



================================================
FILE: lightly/loss/emp_ssl_loss.py
================================================
"""Code for EMP-SSL Loss, largely taken from https://github.com/tsb0601/EMP-SSL"""

from typing import List

import torch
import torch.nn.functional as F
from torch import Tensor
from torch.nn import Module


def tcr_loss(z: Tensor, eps: float) -> Tensor:
    """Computes the Total Coding Rate (TCR) loss.

    Args:
        z:
            Patch embeddings.
        eps:
            Epsilon value for numerical stability.

    Returns:
        TCR loss.
    """
    _, batch_size, dim = z.shape
    diag = torch.eye(dim, device=z.device).unsqueeze(0)
    # Matrix multiplication over the batch dimension
    einsum = torch.einsum("vbd,vbe->vde", z, z)

    # Calculate the log determinant
    logdet = torch.logdet(diag + dim / (batch_size * eps) * einsum)

    return 0.5 * logdet.mean()


def invariance_loss(z: Tensor) -> Tensor:
    """Calculates the invariance loss, representing the similiarity between the patch embeddings and the average of
    the patch embeddings.

    Args:
        z:
            Patch embeddings.
    Returns:
        Similarity loss.
    """
    # z has shape (num_views, batch_size, dim)

    # Calculate the mean of the patch embeddings across the batch dimension
    z_mean = z.mean(0, keepdim=True)

    return -F.cosine_similarity(z, z_mean, dim=-1).mean()


class EMPSSLLoss(Module):
    """Implementation of the loss from 'EMP-SSL: Towards Self-Supervised Learning in
    One Training Epoch' [0].

    - [0] EMP-SSL, 2023, https://arxiv.org/abs/2304.03977

    Attributes:
        tcr_eps:
            Total Coding Rate (TCR) epsilon. NOTE: While in the paper, this term is
            squared, we do not square it here as to follow the implementation in the
            official repository.
        inv_coef:
            Coefficient for the invariance loss (Lambda in the paper).

    Examples:
        >>> # initialize loss function
        >>> loss_fn = EMP_SSLLoss()
        >>> base_transform = VICRegViewTransform() # As discussed in paper
        >>> transform_fn = MultiCropTransform(transforms=base_transform, crop_counts=100)
        >>>
        >>> # generate the transformed samples
        >>> samples = transform_fn(image)
        >>>
        >>> # feed through encoder head
        >>> z = torch.cat([model(s) for s in samples])
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(z)
    """

    def __init__(
        self,
        tcr_eps: float = 0.2,
        inv_coef: float = 200.0,
    ) -> None:
        """Initializes the EMPSSLoss module.

        Args:
            tcr_eps:
                Total coding rate (TCR) epsilon.
            inv_coff:
                Coefficient for the invariance loss.
        """
        super().__init__()
        self.tcr_eps = tcr_eps
        self.inv_coef = inv_coef

    def forward(self, z_views: List[Tensor]) -> Tensor:
        """Computes the EMP-SSL loss, which is a combination of Total Coding Rate loss and invariance loss.

        Args:
            z_views:
                List of patch embeddings tensors from different views.

        Returns:
            The computed EMP-SSL loss.
        """

        # z has shape (num_views, batch_size, dim)
        z = torch.stack(z_views)

        return tcr_loss(z, eps=self.tcr_eps) + self.inv_coef * invariance_loss(z)



================================================
FILE: lightly/loss/hypersphere_loss.py
================================================
"""
FIXME: hypersphere is perhaps bad naming as I am not sure it is the essence;
 alignment-and-uniformity loss perhaps? Does not sound as nice.
"""

import torch
import torch.nn.functional as F
from torch import Tensor
from torch.nn import Module


class HypersphereLoss(Module):
    """Implementation of the loss described in 'Understanding Contrastive Representation Learning through
    Alignment and Uniformity on the Hypersphere.' [0]

    [0] Tongzhou Wang. et.al, 2020, ... https://arxiv.org/abs/2005.10242

    Note:
        In order for this loss to function as advertized, an L1-normalization to the hypersphere is required.
        This loss function applies this L1-normalization internally in the loss layer.
        However, it is recommended that the same normalization is also applied in your architecture,
        considering that this L1-loss is also intended to be applied during inference.
        Perhaps there may be merit in leaving it out of the inferrence pathway, but this use has not been tested.

        Moreover it is recommended that the layers preceeding this loss function are either a linear layer without activation,
        a batch-normalization layer, or both. The directly upstream architecture can have a large influence
        on the ability of this loss to achieve its stated aim of promoting uniformity on the hypersphere;
        and if by contrast the last layer going into the embedding is a RELU or similar nonlinearity,
        we may see that we will never get very close to achieving the goal of uniformity on the hypersphere,
        but will confine ourselves to the subspace of positive activations.
        Similar architectural considerations are relevant to most contrastive loss functions,
        but we call it out here explicitly.

    Examples:
        >>> # initialize loss function
        >>> loss_fn = HypersphereLoss()
        >>>
        >>> # generate two random transforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # feed through SimSiam model
        >>> out0, out1 = model(t0, t1)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(out0, out1)
    """

    def __init__(self, t: float = 1.0, lam: float = 1.0, alpha: float = 2.0):
        """Initializes the HypersphereLoss module with the specified parameters.

        Parameters as described in [0]

        Args:
            t:
                Temperature parameter; proportional to the inverse variance of the Gaussians used to measure uniformity.
            lam:
                Weight balancing the alignment and uniformity loss terms
            alpha:
                Power applied to the alignment term of the loss. At its default value of 2,
                distances between positive samples are penalized in an L2 sense.
        """
        super(HypersphereLoss, self).__init__()
        self.t = t
        self.lam = lam
        self.alpha = alpha

    def forward(self, z_a: Tensor, z_b: Tensor) -> Tensor:
        """Computes the Hypersphere loss, which combines alignment and uniformity loss terms.

        Args:
            z_a:
                Tensor of shape (batch_size, embedding_dim) for the first set of embeddings.
            z_b:
                Tensor of shape (batch_size, embedding_dim) for the second set of embeddings.

        Returns:
            The computed loss.
        """
        # Normalize the input embeddings
        x = F.normalize(z_a)
        y = F.normalize(z_b)

        # Calculate alignment loss
        def lalign(x: Tensor, y: Tensor) -> Tensor:
            lalign_: Tensor = (x - y).norm(dim=1).pow(self.alpha).mean()
            return lalign_

        # Calculate uniformity loss
        def lunif(x: Tensor) -> Tensor:
            sq_pdist = torch.pdist(x, p=2).pow(2)
            return sq_pdist.mul(-self.t).exp().mean().log()

        # Combine alignment and uniformity loss terms
        return lalign(x, y) + self.lam * (lunif(x) + lunif(y)) / 2.0



================================================
FILE: lightly/loss/ibot_loss.py
================================================
from __future__ import annotations

import torch
from torch import Tensor
from torch.nn import Module
from torch.nn import functional as F

from lightly.models.modules.center import Center


class IBOTPatchLoss(Module):
    """Implementation of the iBOT patch loss [0] as used in DINOv2 [1].

    Implementation is based on [2].

    - [0]: iBOT, 2021, https://arxiv.org/abs/2111.07832
    - [1]: DINOv2, 2023, https://arxiv.org/abs/2304.07193
    - [2]: https://github.com/facebookresearch/dinov2/blob/main/dinov2/loss/ibot_patch_loss.py

    Attributes:
        output_dim:
            Dimension of the model output.
        teacher_temp:
            Temperature for the teacher output.
        student_temp:
            Temperature for the student output.
        center_mode:
            Mode for center calculation. Only 'mean' is supported.
        center_momentum:
            Momentum term for the center update.
    """

    def __init__(
        self,
        output_dim: int = 65536,
        teacher_temp: float = 0.04,
        student_temp: float = 0.1,
        center_mode: str = "mean",
        center_momentum: float = 0.9,
    ) -> None:
        """Initializes the iBOTPatchLoss module with the specified parameters."""
        super().__init__()

        self.teacher_temp = teacher_temp
        self.student_temp = student_temp

        self.center = Center(
            size=(1, output_dim),
            mode=center_mode,
            momentum=center_momentum,
        )

    def forward(
        self,
        teacher_out: Tensor,
        student_out: Tensor,
        mask: Tensor,
        teacher_temp: float | None = None,
    ) -> Tensor:
        """Forward pass through the iBOT patch loss.

        Args:
            teacher_out:
                Tensor with shape (batch_size * sequence_length, embed_dim) containing
                the teacher output of the masked tokens.
            student_out:
                Tensor with shape (batch_size * sequence_length, embed_dim) containing
                the student output of the masked tokens.
            mask:
                Boolean tensor with shape (batch_size, height, width) containing the
                token mask. Exactly batch_size * sequence_length entries must be set to
                True in the mask.
            teacher_temp:
                The temperature used for the teacher output. If None, the default
                temperature defined in __init__ is used.

        Returns:
            The loss value.
        """
        # B = batch size, N = sequence length = number of masked tokens, D = embed dim
        # H = height (in tokens), W = width (in tokens)
        # Note that N <= H * W depending on how many tokens are masked.
        teacher_temperature = torch.tensor(
            teacher_temp if teacher_temp is not None else self.teacher_temp
        )

        # Calculate cross-entropy loss.
        teacher_softmax = F.softmax(
            (teacher_out - self.center.value) / teacher_temperature, dim=-1
        )
        student_log_softmax = F.log_softmax(student_out / self.student_temp, dim=-1)

        # (B * N, D) -> (B * N)
        loss = -torch.sum(teacher_softmax * student_log_softmax, dim=-1)

        # Get weights.
        # (B, H, W) -> (B, 1, 1)
        num_masked_per_image = mask.sum(dim=(1, 2), keepdim=True).clamp(min=1.0)
        # (B, 1, 1) -> (B, H, W) -> (B * N)
        weight = (1.0 / num_masked_per_image).expand_as(mask)[mask]

        # Apply weighting.
        B = mask.shape[0]
        loss = (loss * weight).sum() / B

        self.center.update(teacher_out)

        return loss



================================================
FILE: lightly/loss/koleo_loss.py
================================================
import torch
from torch import Tensor
from torch.nn import Module, PairwiseDistance, functional


class KoLeoLoss(Module):
    """KoLeo loss based on [0].

    KoLeo loss is a regularizer that encourages a uniform span of the features in a
    batch by penalizing the distance between the features and their nearest
    neighbors.

    Implementation is based on [1].

    - [0]: Spreading vectors for similarity search, 2019, https://arxiv.org/abs/1806.03198
    - [1]: https://github.com/facebookresearch/dinov2/blob/main/dinov2/loss/koleo_loss.py

    Attributes:
        p:
            The norm degree for pairwise distance calculation.
        eps:
            Small value to avoid division by zero.
    """

    def __init__(
        self,
        p: float = 2,
        eps: float = 1e-8,
    ):
        """Initializes the KoLeoLoss module with the specified parameters.

        Args:
            p:
                The norm degree for pairwise distance calculation.
            eps:
                Small value to avoid division by zero.
        """

        super().__init__()
        self.p = p
        self.eps = eps
        self.pairwise_distance = PairwiseDistance(p=p, eps=eps)

    def forward(self, x: Tensor) -> Tensor:
        """Forward pass through KoLeo Loss.

        Args:
            x: Tensor with shape (batch_size, embedding_size).

        Returns:
            Loss value.
        """
        # Normalize the input tensor
        x = functional.normalize(x, p=2, dim=-1, eps=self.eps)

        # Calculate cosine similarity.
        cos_sim = torch.mm(x, x.t())
        cos_sim.fill_diagonal_(-2)

        # Get nearest neighbors.
        nn_idx = cos_sim.argmax(dim=1)
        nn_dist: Tensor = self.pairwise_distance(x, x[nn_idx])

        # Compute the loss
        loss = -(nn_dist + self.eps).log().mean()

        return loss



================================================
FILE: lightly/loss/macl_loss.py
================================================
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor


class MACLLoss(nn.Module):
    """Implementation of the Model-Aware Contrastive Loss (MACL) from the paper:

    This implementation follows the MACL[0] paper.

    - [0] Model-Aware Contrastive Learning: Towards Escaping the Dilemmas, ICML 2023, https://arxiv.org/abs/2207.07874

    Attributes:
        temperature: Base temperature. Range: (0, inf)
        alpha: Scaling factor for controlling how much the temperature changes. Range: [0, 1]
        A_0: Initial threshold for the alignment magnitude. Range: [0, 1]

    Raises:
        ValueError:
            If the initial temperature is less than 1e-8.
            If the alpha value is not in the range [0, 1].
            If the A_0 value is not in the range [0, 1].

    Examples:
        >>> # initialize the loss function
        >>> loss_fn = MACLLoss()
        >>>
        >>> # generate two random transforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # feed through SimCLR or MoCo model
        >>> z0 = model(t0)
        >>> z1 = model(t1)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(z0, z1)
    """

    def __init__(self, temperature: float = 0.1, alpha: float = 0.5, A_0: float = 0.0):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha
        self.A_0 = A_0
        self.eps = 1e-8

        if self.temperature < self.eps:
            raise ValueError(
                f"Illegal initial temperature: abs({self.temperature}) < 1e-8"
            )

        if self.alpha < 0 or self.alpha > 1:
            raise ValueError("Alpha must be in the range [0, 1].")

        if self.A_0 < 0 or self.A_0 > 1:
            raise ValueError("A_0 must be in the range [0, 1].")

    def forward(self, z0: Tensor, z1: Tensor) -> Tensor:
        """Compute the Model-Aware Contrastive Loss (MACL) for a batch of embeddings.

        Args:
            z0:
                First view embeddings
                Shape (batch_size, embedding_size)
            z1:
                Second view embeddings
                Shape (batch_size, embedding_size)

        Returns:
            loss:
                The computed loss value
        """
        # Normalize embeddings
        z0 = F.normalize(z0, dim=-1, p=2)
        z1 = F.normalize(z1, dim=-1, p=2)

        # Concatenate embeddings
        out = torch.cat([z0, z1], dim=0)
        batch_size = z0.shape[0]
        n_samples = len(out)

        # Compute similarity matrix
        cov = out @ out.T

        # Get positive and negative pairs
        mask = cov.new_ones(cov.shape, dtype=torch.bool)
        mask.diagonal()[:] = False
        mask.diagonal(batch_size)[:] = False
        mask.diagonal(-batch_size)[:] = False
        neg = cov.masked_select(mask).view(n_samples, -1)

        # Get positive pairs from upper and lower diagonals
        u_b = torch.diag(cov, batch_size)
        l_b = torch.diag(cov, -batch_size)
        pos = torch.cat([u_b, l_b], dim=0).reshape(n_samples, 1)

        # Calculate model-aware temperature
        A = torch.mean(pos.detach())
        t = self.temperature * (1 + self.alpha * (A - self.A_0))

        # 6) Compute stable log_softmax
        logits = torch.cat([pos, neg], dim=1) / t
        log_prob = F.log_softmax(logits, dim=1)

        # 7) Extract log probability of the positive pair
        log_pos_prob = log_prob[:, 0]

        # 8) Compute P and V
        P = log_pos_prob.exp()
        V = 1.0 / (1.0 - P + self.eps)  # add eps to avoid division-by-zero

        # 9) Compute final loss
        loss: Tensor = -V.detach() * log_pos_prob

        return loss.mean()



================================================
FILE: lightly/loss/memory_bank.py
================================================
# For backwards compatibility as memory_bank module was previously in loss module.
from lightly.models.modules.memory_bank import MemoryBankModule



================================================
FILE: lightly/loss/mmcr_loss.py
================================================
import torch
import torch.nn as nn
from torch.linalg import svd


class MMCRLoss(nn.Module):
    """Implementation of the loss function from MMCR [0] using Manifold Capacity.
    All hyperparameters are set to the default values from the paper for ImageNet.

    - [0]: Efficient Coding of Natural Images using Maximum Manifold Capacity
        Representations, 2023, https://arxiv.org/pdf/2303.03307.pdf

    Examples:
        >>> # initialize loss function
        >>> loss_fn = MMCRLoss()
        >>> transform = MMCRTransform(k=2)
        >>>
        >>> # transform images, then feed through encoder and projector
        >>> x = transform(x)
        >>> online = online_network(x)
        >>> momentum = momentum_network(x)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(online, momentum)
    """

    def __init__(self, lmda: float = 5e-3):
        """Initializes the MMCRLoss module with the specified lambda parameter.

        Args:
            lmda: The regularization parameter.

        Raises:
            ValueError: If lmda is less than 0.
        """
        super().__init__()
        if lmda < 0:
            raise ValueError("lmda must be greater than or equal to 0")

        self.lmda = lmda

    def forward(self, online: torch.Tensor, momentum: torch.Tensor) -> torch.Tensor:
        """Computes the MMCR loss for the online and momentum network outputs.

        Args:
            online:
                Output of the online network for the current batch. Expected to be
                of shape (batch_size, k, embedding_size), where k represents the
                number of randomly augmented views for each sample.
            momentum:
                Output of the momentum network for the current batch. Expected to be
                of shape (batch_size, k, embedding_size), where k represents the
                number of randomly augmented views for each sample.

        Returns:
            The computed loss value.
        """
        assert (
            online.shape == momentum.shape
        ), "online and momentum need to have the same shape"

        B = online.shape[0]

        # Concatenate and calculate centroid
        z = torch.cat([online, momentum], dim=1)
        c = torch.mean(z, dim=1)  # B x D

        # Calculate singular values
        _, S_z, _ = svd(z)
        _, S_c, _ = svd(c)

        # Calculate loss
        loss = -1.0 * torch.sum(S_c) + self.lmda * torch.sum(S_z) / B

        return loss



================================================
FILE: lightly/loss/msn_loss.py
================================================
import math
import warnings
from typing import Optional

import torch
import torch.distributed as dist
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor


def prototype_probabilities(
    queries: Tensor,
    prototypes: Tensor,
    temperature: float,
) -> Tensor:
    """Returns probability for each query to belong to each prototype.

    Args:
        queries:
            Tensor with shape (batch_size, dim)
        prototypes:
            Tensor with shape (num_prototypes, dim)
        temperature:
            Inverse scaling factor for the similarity.

    Returns:
        Probability tensor with shape (batch_size, num_prototypes) which sums to 1 along
        the num_prototypes dimension.
    """
    return F.softmax(torch.matmul(queries, prototypes.T) / temperature, dim=1)


def sharpen(probabilities: Tensor, temperature: float) -> Tensor:
    """Sharpens the probabilities with the given temperature.

    Args:
        probabilities:
            Tensor with shape (batch_size, dim)
        temperature:
            Temperature in (0, 1]. Lower temperature results in stronger sharpening (
            output probabilities are less uniform).
    Returns:
        Probabilities tensor with shape (batch_size, dim).
    """
    probabilities = probabilities ** (1.0 / temperature)
    probabilities /= torch.sum(probabilities, dim=1, keepdim=True)
    return probabilities


@torch.no_grad()
def sinkhorn(
    probabilities: Tensor,
    iterations: int = 3,
    gather_distributed: bool = False,
) -> Tensor:
    """Runs sinkhorn normalization on the probabilities as described in [0].

    Code inspired by [1].

    - [0]: Masked Siamese Networks, 2022, https://arxiv.org/abs/2204.07141
    - [1]: https://github.com/facebookresearch/msn

    Args:
        probabilities:
            Probabilities tensor with shape (batch_size, num_prototypes).
        iterations:
            Number of iterations of the sinkhorn algorithms. Set to 0 to disable.
        gather_distributed:
            If True, features from all GPUs are gathered during normalization.
    Returns:
        A normalized probabilities tensor.
    """
    if iterations <= 0:
        return probabilities

    world_size = 1
    if gather_distributed and dist.is_initialized():
        world_size = dist.get_world_size()

    num_targets, num_prototypes = probabilities.shape
    probabilities = probabilities.T
    sum_probabilities = torch.sum(probabilities)
    if world_size > 1:
        dist.all_reduce(sum_probabilities)
    probabilities = probabilities / sum_probabilities

    for _ in range(iterations):
        # Normalize rows
        row_sum = torch.sum(probabilities, dim=1, keepdim=True)
        if world_size > 1:
            dist.all_reduce(row_sum)
        probabilities /= row_sum
        probabilities /= num_prototypes

        # Normalize columns
        probabilities /= torch.sum(probabilities, dim=0, keepdim=True)
        probabilities /= num_targets

    probabilities *= num_targets
    return probabilities.T


class MSNLoss(nn.Module):
    """Implementation of the loss function from MSN [0].

    Code inspired by [1].

    - [0]: Masked Siamese Networks, 2022, https://arxiv.org/abs/2204.07141
    - [1]: https://github.com/facebookresearch/msn

    Attributes:
        temperature:
            Similarities between anchors and targets are scaled by the inverse of
            the temperature. Must be in (0, inf).
        sinkhorn_iterations:
            Number of sinkhorn normalization iterations on the targets.
        regularization_weight:
            Weight factor lambda by which the regularization loss is scaled. Set to 0
            to disable regularization.
        me_max_weight:
            Deprecated, use `regularization_weight` instead. Takes precendence over
            `regularization_weight` if not None. Weight factor lambda by which the mean
            entropy maximization regularization loss is scaled. Set to 0 to disable
            mean entropy maximization reguliarization.
        gather_distributed:
            If True, then target probabilities are gathered from all GPUs.

    Examples:
        >>> # initialize loss function
        >>> loss_fn = MSNLoss()
        >>>
        >>> # generate anchors and targets of images
        >>> anchors = transforms(images)
        >>> targets = transforms(images)
        >>>
        >>> # feed through MSN model
        >>> anchors_out = model(anchors)
        >>> targets_out = model.target(targets)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(anchors_out, targets_out, prototypes=model.prototypes)
    """

    def __init__(
        self,
        temperature: float = 0.1,
        sinkhorn_iterations: int = 3,
        regularization_weight: float = 1.0,
        me_max_weight: Optional[float] = None,
        gather_distributed: bool = False,
    ):
        """Initializes the MSNLoss module with the specified parameters.

        Args:
            temperature:
                Similarities between anchors and targets are scaled by the inverse of the temperature. Must be in (0, inf).
            sinkhorn_iterations:
                Number of sinkhorn normalization iterations on the targets.
            regularization_weight:
                Weight factor lambda by which the regularization loss is scaled. Set to 0 to disable regularization.
            me_max_weight:
                Deprecated, use `regularization_weight` instead. Takes precedence over
                `regularization_weight` if not None. Weight factor lambda by which the mean
                entropy maximization regularization loss is scaled. Set to 0 to disable mean
                entropy maximization regularization.
            gather_distributed:
                If True, then target probabilities are gathered from all GPUs.

        Raises:
            ValueError: If temperature is not in (0, inf).
            ValueError: If sinkhorn_iterations is less than 0.
            ValueError: If gather_distributed is True but torch.distributed is not available.
        """
        super().__init__()
        if temperature <= 0:
            raise ValueError(f"temperature must be in (0, inf) but is {temperature}.")
        if sinkhorn_iterations < 0:
            raise ValueError(
                f"sinkhorn_iterations must be >= 0 but is {sinkhorn_iterations}."
            )
        if gather_distributed and not dist.is_available():
            raise ValueError(
                "gather_distributed is True but torch.distributed is not available. "
                "Please set gather_distributed=False or install a torch version with "
                "distributed support."
            )

        self.temperature = temperature
        self.sinkhorn_iterations = sinkhorn_iterations
        self.regularization_weight = regularization_weight
        # Set regularization_weight to me_max_weight for backwards compatibility
        if me_max_weight is not None:
            warnings.warn(
                DeprecationWarning(
                    "me_max_weight is deprecated in favor of regularization_weight and "
                    "will be removed in the future."
                )
            )
            self.regularization_weight = me_max_weight
        self.gather_distributed = gather_distributed

    def forward(
        self,
        anchors: Tensor,
        targets: Tensor,
        prototypes: Tensor,
        target_sharpen_temperature: float = 0.25,
    ) -> Tensor:
        """Computes the MSN loss for a set of anchors, targets, and prototypes.

        Args:
            anchors:
                Tensor with shape (batch_size * anchor_views, dim).
            targets:
                Tensor with shape (batch_size, dim).
            prototypes:
                Tensor with shape (num_prototypes, dim).
            target_sharpen_temperature:
                Temperature used to sharpen the target probabilities.

        Returns:
            Mean loss over all anchors.
        """
        num_views = anchors.shape[0] // targets.shape[0]

        # Normalize the inputs
        anchors = F.normalize(anchors, dim=1)
        targets = F.normalize(targets, dim=1)
        prototypes = F.normalize(prototypes, dim=1)

        # Anchor predictions
        anchor_probs = prototype_probabilities(
            anchors, prototypes, temperature=self.temperature
        )

        # Target predictions
        with torch.no_grad():
            target_probs = prototype_probabilities(
                targets, prototypes, temperature=self.temperature
            )
            target_probs = sharpen(target_probs, temperature=target_sharpen_temperature)
            if self.sinkhorn_iterations > 0:
                target_probs = sinkhorn(
                    probabilities=target_probs,
                    iterations=self.sinkhorn_iterations,
                    gather_distributed=self.gather_distributed,
                )
            target_probs = target_probs.repeat((num_views, 1))

        # Cross entropy loss
        loss = torch.mean(torch.sum(torch.log(anchor_probs ** (-target_probs)), dim=1))

        # Regularization loss
        if self.regularization_weight > 0:
            mean_anchor_probs = torch.mean(anchor_probs, dim=0)
            reg_loss = self.regularization_loss(mean_anchor_probs=mean_anchor_probs)
            loss += self.regularization_weight * reg_loss

        return loss

    def regularization_loss(self, mean_anchor_probs: Tensor) -> Tensor:
        """Calculates mean entropy regularization loss.

        Args:
            mean_anchor_probs: The mean anchor probabilities.

        Returns:
            The calculated regularization loss.
        """
        loss = -torch.sum(torch.log(mean_anchor_probs ** (-mean_anchor_probs)))
        loss += math.log(float(len(mean_anchor_probs)))
        return loss



================================================
FILE: lightly/loss/negative_cosine_similarity.py
================================================
""" Negative Cosine Similarity Loss Function """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

import torch
from torch.nn.functional import cosine_similarity


class NegativeCosineSimilarity(torch.nn.Module):
    """Implementation of the Negative Cosine Simililarity used in the SimSiam[0] paper.

    - [0] SimSiam, 2020, https://arxiv.org/abs/2011.10566

    Examples:
        >>> # initialize loss function
        >>> loss_fn = NegativeCosineSimilarity()
        >>>
        >>> # generate two representation tensors
        >>> # with batch size 10 and dimension 128
        >>> x0 = torch.randn(10, 128)
        >>> x1 = torch.randn(10, 128)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(x0, x1)
    """

    def __init__(self, dim: int = 1, eps: float = 1e-8) -> None:
        """Initializes the NegativeCosineSimilarity module the specified parameters.

        Same parameters as in torch.nn.CosineSimilarity

        Args:
            dim:
                Dimension where cosine similarity is computed.
            eps:
                Small value to avoid division by zero.
        """
        super().__init__()
        self.dim = dim
        self.eps = eps

    def forward(self, x0: torch.Tensor, x1: torch.Tensor) -> torch.Tensor:
        """Computes the negative cosine similarity between two tensors.

        Args:
            x0:
                First input tensor.
            x1:
                Second input tensor.

        Returns:
            The mean negative cosine similarity.
        """
        return -cosine_similarity(x0, x1, self.dim, self.eps).mean()



================================================
FILE: lightly/loss/ntx_ent_loss.py
================================================
""" Contrastive Loss Functions """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

from typing import Sequence, Union

import torch
from torch import Tensor
from torch import distributed as torch_dist
from torch import nn

from lightly.models.modules.memory_bank import MemoryBankModule
from lightly.utils import dist


class NTXentLoss(nn.Module):
    """Implementation of the Contrastive Cross Entropy Loss.

    This implementation follows the SimCLR[0] paper. If you enable the memory
    bank by setting the `memory_bank_size` value > 0 the loss behaves like
    the one described in the MoCo[1] paper.

    - [0] SimCLR, 2020, https://arxiv.org/abs/2002.05709
    - [1] MoCo, 2020, https://arxiv.org/abs/1911.05722

    Attributes:
        temperature:
            Scale logits by the inverse of the temperature.
        memory_bank_size:
            Size of the memory bank as (num_features, dim) tuple. num_features are the
            number of negative samples stored in the memory bank. If num_features is 0,
            the memory bank is disabled. Use 0 for SimCLR. For MoCo we typically use
            numbers like 4096 or 65536.
            Deprecated: If only a single integer is passed, it is interpreted as the
            number of features and the feature dimension is inferred from the first
            batch stored in the memory bank. Leaving out the feature dimension might
            lead to errors in distributed training.
        gather_distributed:
            If True then negatives from all GPUs are gathered before the
            loss calculation. If a memory bank is used and gather_distributed is True,
            then tensors from all gpus are gathered before the memory bank is updated.

    Raises:
        ValueError: If abs(temperature) < 1e-8 to prevent divide by zero.

    Examples:
        >>> # initialize loss function without memory bank
        >>> loss_fn = NTXentLoss(memory_bank_size=0)
        >>>
        >>> # generate two random transforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # feed through SimCLR or MoCo model
        >>> out0, out1 = model(t0), model(t1)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(out0, out1)

    """

    def __init__(
        self,
        temperature: float = 0.5,
        memory_bank_size: Union[int, Sequence[int]] = 0,
        gather_distributed: bool = False,
    ):
        """Initializes the NTXentLoss module with the specified parameters.

        Args:
            temperature:
                 Scale logits by the inverse of the temperature.
            memory_bank_size:
                 Size of the memory bank.
            gather_distributed:
                 If True, negatives from all GPUs are gathered before the loss calculation.

        Raises:
            ValueError: If temperature is less than 1e-8 to prevent divide by zero.
            ValueError: If gather_distributed is True but torch.distributed is not available.
        """
        super().__init__()
        self.memory_bank = MemoryBankModule(
            size=memory_bank_size, gather_distributed=gather_distributed
        )
        self.temperature = temperature
        self.gather_distributed = gather_distributed
        self.cross_entropy = nn.CrossEntropyLoss(reduction="mean")
        self.eps = 1e-8

        if abs(self.temperature) < self.eps:
            raise ValueError(
                "Illegal temperature: abs({}) < 1e-8".format(self.temperature)
            )
        if gather_distributed and not torch_dist.is_available():
            raise ValueError(
                "gather_distributed is True but torch.distributed is not available. "
                "Please set gather_distributed=False or install a torch version with "
                "distributed support."
            )

    def forward(self, out0: Tensor, out1: Tensor) -> Tensor:
        """Forward pass through Contrastive Cross-Entropy Loss.

        If used with a memory bank, the samples from the memory bank are used
        as negative examples. Otherwise, within-batch samples are used as
        negative samples.

        Args:
            out0:
                Output projections of the first set of transformed images.
                Shape: (batch_size, embedding_size)
            out1:
                Output projections of the second set of transformed images.
                Shape: (batch_size, embedding_size)

        Returns:
            Contrastive Cross Entropy Loss value.
        """

        device = out0.device
        batch_size, _ = out0.shape

        # Normalize the output to length 1
        out0 = nn.functional.normalize(out0, dim=1)
        out1 = nn.functional.normalize(out1, dim=1)

        # ask memory bank for negative samples and extend it with out1 if
        # out1 requires a gradient, otherwise keep the same vectors in the
        # memory bank (this allows for keeping the memory bank constant e.g.
        # for evaluating the loss on the test set)
        # out1: shape: (batch_size, embedding_size)
        # negatives: shape: (embedding_size, memory_bank_size)
        out1, negatives = self.memory_bank.forward(out1, update=out0.requires_grad)

        # Use cosine similarity (dot product) as all vectors are normalized to unit length
        # Notation in einsum: n = batch_size, c = embedding_size and k = memory_bank_size.

        if negatives is not None:
            # Use negatives from memory bank
            negatives = negatives.to(device)

            # sim_pos is of shape (batch_size, 1) and sim_pos[i] denotes the similarity
            # of the i-th sample in the batch to its positive pair
            sim_pos = torch.einsum("nc,nc->n", out0, out1).unsqueeze(-1)

            # sim_neg is of shape (batch_size, memory_bank_size) and sim_neg[i,j] denotes the similarity
            # of the i-th sample to the j-th negative sample
            sim_neg = torch.einsum("nc,ck->nk", out0, negatives)

            # Set the labels to maximize sim_pos in relation to sim_neg
            logits = torch.cat([sim_pos, sim_neg], dim=1) / self.temperature
            labels = torch.zeros(logits.shape[0], device=device, dtype=torch.long)

        else:
            # Use other samples from batch as negatives
            # and create diagonal mask that only selects similarities between
            # views of the same image
            if self.gather_distributed and dist.world_size() > 1:
                # Gather hidden representations from other processes
                out0_large = torch.cat(dist.gather(out0), 0)
                out1_large = torch.cat(dist.gather(out1), 0)
                diag_mask = dist.eye_rank(batch_size, device=out0.device)
            else:
                # Single process
                out0_large = out0
                out1_large = out1
                diag_mask = torch.eye(batch_size, device=out0.device, dtype=torch.bool)

            # Calculate similiarities
            # Here n = batch_size and m = batch_size * world_size
            # The resulting vectors have shape (n, m)
            logits_00 = torch.einsum("nc,mc->nm", out0, out0_large) / self.temperature
            logits_01 = torch.einsum("nc,mc->nm", out0, out1_large) / self.temperature
            logits_10 = torch.einsum("nc,mc->nm", out1, out0_large) / self.temperature
            logits_11 = torch.einsum("nc,mc->nm", out1, out1_large) / self.temperature

            # Remove simliarities between same views of the same image
            logits_00 = logits_00[~diag_mask].view(batch_size, -1)
            logits_11 = logits_11[~diag_mask].view(batch_size, -1)

            # Concatenate logits
            # The logits tensor in the end has shape (2*n, 2*m-1)
            logits_0100 = torch.cat([logits_01, logits_00], dim=1)
            logits_1011 = torch.cat([logits_10, logits_11], dim=1)
            logits = torch.cat([logits_0100, logits_1011], dim=0)

            # Create labels
            labels = torch.arange(batch_size, device=device, dtype=torch.long)
            if self.gather_distributed:
                labels = labels + dist.rank() * batch_size
            labels = labels.repeat(2)

        # Calculate the cross-entropy loss
        loss: Tensor = self.cross_entropy(logits, labels)

        return loss



================================================
FILE: lightly/loss/pmsn_loss.py
================================================
from typing import Callable

import torch
import torch.nn.functional as F
from torch import Tensor

from lightly.loss.msn_loss import MSNLoss


class PMSNLoss(MSNLoss):
    """Implementation of the loss function from PMSN [0] using a power law target
    distribution.

    - [0]: Prior Matching for Siamese Networks, 2022, https://arxiv.org/abs/2210.07277

    Attributes:
        temperature:
            Similarities between anchors and targets are scaled by the inverse of
            the temperature. Must be in (0, inf).
        sinkhorn_iterations:
            Number of sinkhorn normalization iterations on the targets.
        regularization_weight:
            Weight factor lambda by which the regularization loss is scaled. Set to 0
            to disable regularization.
        power_law_exponent:
            Exponent for power law distribution. Entry k of the distribution is
            proportional to (1 / k) ^ power_law_exponent, with k ranging from 1 to dim + 1.
        gather_distributed:
            If True, then target probabilities are gathered from all GPUs.

    Examples:
        >>> # initialize loss function
        >>> loss_fn = PMSNLoss()
        >>>
        >>> # generate anchors and targets of images
        >>> anchors = transforms(images)
        >>> targets = transforms(images)
        >>>
        >>> # feed through PMSN model
        >>> anchors_out = model(anchors)
        >>> targets_out = model.target(targets)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(anchors_out, targets_out, prototypes=model.prototypes)
    """

    def __init__(
        self,
        temperature: float = 0.1,
        sinkhorn_iterations: int = 3,
        regularization_weight: float = 1,
        power_law_exponent: float = 0.25,
        gather_distributed: bool = False,
    ):
        """Initializes the PMSNLoss module with the specified parameters."""
        super().__init__(
            temperature=temperature,
            sinkhorn_iterations=sinkhorn_iterations,
            regularization_weight=regularization_weight,
            gather_distributed=gather_distributed,
        )
        self.power_law_exponent = power_law_exponent

    def regularization_loss(self, mean_anchor_probs: Tensor) -> Tensor:
        """Calculates the regularization loss with a power law target distribution.

        Args:
            mean_anchor_probs: The mean anchor probabilities.

        Returns:
            The calculated regularization loss.
        """
        power_dist = _power_law_distribution(
            size=mean_anchor_probs.shape[0],
            exponent=self.power_law_exponent,
            device=mean_anchor_probs.device,
        )
        loss = F.kl_div(
            input=mean_anchor_probs.log(), target=power_dist, reduction="sum"
        )
        return loss


class PMSNCustomLoss(MSNLoss):
    """Implementation of the loss function from PMSN [0] with a custom target
    distribution.

    - [0]: Prior Matching for Siamese Networks, 2022, https://arxiv.org/abs/2210.07277

    Attributes:
        target_distribution:
            A function that takes the mean anchor probabilities tensor with shape (dim,)
            as input and returns a target probability distribution tensor with the same
            shape. The returned distribution should sum up to one. The final
            regularization loss is calculated as KL(mean_anchor_probs, target_dist)
            where KL is the Kullback-Leibler divergence.
        temperature:
            Similarities between anchors and targets are scaled by the inverse of
            the temperature. Must be in (0, inf).
        sinkhorn_iterations:
            Number of sinkhorn normalization iterations on the targets.
        regularization_weight:
            Weight factor lambda by which the regularization loss is scaled. Set to 0
            to disable regularization.
        gather_distributed:
            If True, then target probabilities are gathered from all GPUs.

    Examples:
        >>> # define custom target distribution
        >>> def my_uniform_distribution(mean_anchor_probabilities: Tensor) -> Tensor:
        >>>     dim = mean_anchor_probabilities.shape[0]
        >>>     return mean_anchor_probabilities.new_ones(dim) / dim
        >>>
        >>> # initialize loss function
        >>> loss_fn = PMSNCustomLoss(target_distribution=my_uniform_distribution)
        >>>
        >>> # generate anchors and targets of images
        >>> anchors = transforms(images)
        >>> targets = transforms(images)
        >>>
        >>> # feed through PMSN model
        >>> anchors_out = model(anchors)
        >>> targets_out = model.target(targets)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(anchors_out, targets_out, prototypes=model.prototypes)
    """

    def __init__(
        self,
        target_distribution: Callable[[Tensor], Tensor],
        temperature: float = 0.1,
        sinkhorn_iterations: int = 3,
        regularization_weight: float = 1,
        gather_distributed: bool = False,
    ):
        """Initializes the PMSNCustomLoss module with the specified parameters."""
        super().__init__(
            temperature=temperature,
            sinkhorn_iterations=sinkhorn_iterations,
            regularization_weight=regularization_weight,
            gather_distributed=gather_distributed,
        )
        self.target_distribution = target_distribution

    def regularization_loss(self, mean_anchor_probs: Tensor) -> Tensor:
        """Calculates regularization loss with a custom target distribution.

        Args:
            mean_anchor_probs:
                The mean anchor probabilities.

        Returns:
            The calculated regularization loss.
        """
        target_dist = self.target_distribution(mean_anchor_probs).to(
            mean_anchor_probs.device
        )
        loss = F.kl_div(
            input=mean_anchor_probs.log(), target=target_dist, reduction="sum"
        )
        return loss


def _power_law_distribution(size: int, exponent: float, device: torch.device) -> Tensor:
    """Returns a power law distribution summing up to 1.

    Args:
        size:
            The size of the distribution.
        exponent:
            The exponent for the power law distribution.
        device:
            The device to create tensor on.

    Returns:
        A power law distribution tensor summing up to 1.
    """
    k = torch.arange(1, size + 1, device=device)
    power_dist = torch.tensor(k ** (-exponent))
    power_dist = power_dist / power_dist.sum()
    return power_dist



================================================
FILE: lightly/loss/swav_loss.py
================================================
from typing import List, Union

import torch
import torch.distributed as dist
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor


@torch.no_grad()
def sinkhorn(
    out: Tensor,
    iterations: int = 3,
    epsilon: float = 0.05,
    gather_distributed: bool = False,
) -> Tensor:
    """Distributed sinkhorn algorithm.

    As outlined in [0] and implemented in [1].

    - [0]: SwaV, 2020, https://arxiv.org/abs/2006.09882
    - [1]: https://github.com/facebookresearch/swav/

    Args:
        out:
            Similarity of the features and the SwaV prototypes.
        iterations:
            Number of sinkhorn iterations.
        epsilon:
            Temperature parameter.
        gather_distributed:
            If True then features from all gpus are gathered to calculate the
            soft codes Q.

    Returns:
        Soft codes Q assigning each feature to a prototype.
    """
    world_size = 1
    if gather_distributed and dist.is_initialized():
        world_size = dist.get_world_size()

    # Get the exponential matrix and make it sum to 1
    Q = torch.exp(out / epsilon).t()
    sum_Q = torch.sum(Q)
    if world_size > 1:
        dist.all_reduce(sum_Q)
    Q /= sum_Q

    B = Q.shape[1] * world_size

    for _ in range(iterations):
        # Normalize rows
        sum_of_rows = torch.sum(Q, dim=1, keepdim=True)
        if world_size > 1:
            dist.all_reduce(sum_of_rows)
        Q /= sum_of_rows
        # Normalize columns
        Q /= torch.sum(Q, dim=0, keepdim=True)
        Q /= B

    Q *= B
    return Q.t()


class SwaVLoss(nn.Module):
    """Implementation of the SwaV loss.

    Attributes:
        temperature:
            Temperature parameter used for cross entropy calculations.
        sinkhorn_iterations:
            Number of iterations of the sinkhorn algorithm.
        sinkhorn_epsilon:
            Temperature parameter used in the sinkhorn algorithm.
        sinkhorn_gather_distributed:
            If True, features from all GPUs are gathered to calculate the
            soft codes in the sinkhorn algorithm.
    """

    def __init__(
        self,
        temperature: float = 0.1,
        sinkhorn_iterations: int = 3,
        sinkhorn_epsilon: float = 0.05,
        sinkhorn_gather_distributed: bool = False,
    ):
        """Initializes the SwaVLoss module with the specified parameters.

        Args:
            temperature:
                Temperature parameter used for cross-entropy calculations.
            sinkhorn_iterations:
                Number of iterations of the sinkhorn algorithm.
            sinkhorn_epsilon:
                Temperature parameter used in the sinkhorn algorithm.
            sinkhorn_gather_distributed:
                If True, features from all GPUs are gathered to calculate the
                soft codes in the sinkhorn algorithm.

        Raises:
            ValueError: If sinkhorn_gather_distributed is True but torch.distributed
                is not available.
        """
        super(SwaVLoss, self).__init__()
        if sinkhorn_gather_distributed and not dist.is_available():
            raise ValueError(
                "sinkhorn_gather_distributed is True but torch.distributed is not "
                "available. Please set gather_distributed=False or install a torch "
                "version with distributed support."
            )

        self.temperature = temperature
        self.sinkhorn_iterations = sinkhorn_iterations
        self.sinkhorn_epsilon = sinkhorn_epsilon
        self.sinkhorn_gather_distributed = sinkhorn_gather_distributed

    def subloss(self, z: Tensor, q: Tensor) -> Tensor:
        """Calculates the cross entropy for the SwaV prediction problem.

        Args:
            z:
                Similarity of the features and the SwaV prototypes.
            q:
                Codes obtained from Sinkhorn iterations.

        Returns:
            Cross entropy between predictions z and codes q.
        """
        return -torch.mean(
            torch.sum(q * F.log_softmax(z / self.temperature, dim=1), dim=1)
        )

    def forward(
        self,
        high_resolution_outputs: List[Tensor],
        low_resolution_outputs: List[Tensor],
        queue_outputs: Union[List[Tensor], None] = None,
    ) -> Tensor:
        """Computes the SwaV loss for a set of high and low resolution outputs.

        - [0]: SwaV, 2020, https://arxiv.org/abs/2006.09882

        Args:
            high_resolution_outputs:
                List of similarities of features and SwaV prototypes for the
                high resolution crops.
            low_resolution_outputs:
                List of similarities of features and SwaV prototypes for the
                low resolution crops.
            queue_outputs:
                List of similarities of features and SwaV prototypes for the
                queue of high resolution crops from previous batches.

        Returns:
            Swapping assignments between views loss (SwaV) as described in [0].
        """
        n_crops = len(high_resolution_outputs) + len(low_resolution_outputs)

        # Multi-crop iterations
        loss = high_resolution_outputs[0].new_zeros(1)
        for i in range(len(high_resolution_outputs)):
            # Compute codes of i-th high resolution crop
            with torch.no_grad():
                outputs = high_resolution_outputs[i].detach()

                # Append queue outputs
                if queue_outputs is not None:
                    outputs = torch.cat((outputs, queue_outputs[i].detach()))

                # Compute the codes
                q = sinkhorn(
                    outputs,
                    iterations=self.sinkhorn_iterations,
                    epsilon=self.sinkhorn_epsilon,
                    gather_distributed=self.sinkhorn_gather_distributed,
                )

                # Drop queue similarities
                if queue_outputs is not None:
                    q = q[: len(high_resolution_outputs[i])]

            # Compute subloss for each pair of crops
            subloss = high_resolution_outputs[i].new_zeros(1)
            for v in range(len(high_resolution_outputs)):
                if v != i:
                    subloss += self.subloss(high_resolution_outputs[v], q)

            for v in range(len(low_resolution_outputs)):
                subloss += self.subloss(low_resolution_outputs[v], q)

            loss += subloss / (n_crops - 1)

        return loss / len(high_resolution_outputs)



================================================
FILE: lightly/loss/sym_neg_cos_sim_loss.py
================================================
""" Symmetrized Negative Cosine Similarity Loss Functions """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

import warnings

import torch
from torch import Tensor
from torch.nn import Module


class SymNegCosineSimilarityLoss(Module):
    """Implementation of the Symmetrized Loss used in the SimSiam[0] paper.

    - [0] SimSiam, 2020, https://arxiv.org/abs/2011.10566

    Examples:
        >>> # initialize loss function
        >>> loss_fn = SymNegCosineSimilarityLoss()
        >>>
        >>> # generate two random transforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # feed through SimSiam model
        >>> out0, out1 = model(t0, t1)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(out0, out1)
    """

    def __init__(self) -> None:
        """Initializes the SymNegCosineSimilarityLoss module.

        Note:
            SymNegCosineSimilarityLoss will be deprecated in favor of NegativeCosineSimilarity in the future.
        """
        super().__init__()
        warnings.warn(
            Warning(
                "SymNegCosineSimiliarityLoss will be deprecated in favor of "
                + "NegativeCosineSimilarity in the future."
            ),
            DeprecationWarning,
        )

    def forward(self, out0: Tensor, out1: Tensor) -> Tensor:
        """Forward pass through Symmetric Loss.

        Args:
            out0:
                Output projections of the first set of transformed images.
                Expects the tuple to be of the form (z0, p0), where z0 is
                the output of the backbone and projection MLP, and p0 is the
                output of the prediction head.
            out1:
                Output projections of the second set of transformed images.
                Expects the tuple to be of the form (z1, p1), where z1 is
                the output of the backbone and projection MLP, and p1 is the
                output of the prediction head.

        Returns:
            Negative Cosine Similarity loss value.
        """
        z0, p0 = out0
        z1, p1 = out1

        loss: Tensor = (
            self._neg_cosine_simililarity(p0, z1) / 2
            + self._neg_cosine_simililarity(p1, z0) / 2
        )

        return loss

    def _neg_cosine_simililarity(self, x: Tensor, y: Tensor) -> Tensor:
        """Calculates the negative cosine similarity between two tensors.

        Args:
            x: First input tensor.
            y: Second input tensor.

        Returns:
            Negative cosine similarity value.
        """
        v = -torch.nn.functional.cosine_similarity(x, y.detach(), dim=-1).mean()
        return v



================================================
FILE: lightly/loss/tico_loss.py
================================================
from typing import Union

import torch
import torch.distributed as dist
from torch import Tensor

from lightly.utils.dist import gather


class TiCoLoss(torch.nn.Module):
    """Implementation of the Tico Loss from Tico[0] paper.

    This implementation takes inspiration from the code published
    by sayannag using Lightly. [1]

    - [0] Jiachen Zhu et. al, 2022, Tico... https://arxiv.org/abs/2206.10698
    - [1] https://github.com/sayannag/TiCo-pytorch

    Attributes:
        Args:
            beta:
                Coefficient for the EMA update of the covariance
                Defaults to 0.9 [0].
            rho:
                Weight for the covariance term of the loss
                Defaults to 8.0 [0].
            gather_distributed:
                If True, the cross-correlation matrices from all GPUs are
                gathered and summed before the loss calculation.

    Examples:
        >>> # initialize loss function
        >>> loss_fn = TiCoLoss()
        >>>
        >>> # generate two random transforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # feed through model
        >>> out0, out1 = model(t0, t1)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(out0, out1)
    """

    def __init__(
        self,
        beta: float = 0.9,
        rho: float = 8.0,
        gather_distributed: bool = False,
    ):
        """Initializes the TiCoLoss module with the specified parameters.

        Args:
            beta:
                Coefficient for the EMA update of the covariance.
            rho:
                Weight for the covariance term of the loss.
            gather_distributed:
                If True, the cross-correlation matrices from all GPUs are gathered
                    and summed before the loss calculation. Default is False.

        Raises:
            ValueError: If gather_distributed is True but torch.distributed is not available.
        """
        super(TiCoLoss, self).__init__()
        if gather_distributed and not dist.is_available():
            raise ValueError(
                "gather_distributed is True but torch.distributed is not available. "
                "Please set gather_distributed=False or install a torch version with "
                "distributed support."
            )

        self.beta = beta
        self.rho = rho
        self.C: Union[Tensor, None] = None
        self.gather_distributed = gather_distributed

    def forward(
        self,
        z_a: torch.Tensor,
        z_b: torch.Tensor,
        update_covariance_matrix: bool = True,
    ) -> torch.Tensor:
        """Computes the TiCo loss.

        It maximizes the agreement among embeddings of different distorted versions of the same image
        while avoiding collapse using Covariance matrix.

        Args:
            z_a:
                Tensor of shape [batch_size, num_features=256]. Output of the learned backbone.
            z_b:
                Tensor of shape [batch_size, num_features=256]. Output of the momentum updated backbone.
            update_covariance_matrix:
                Parameter to update the covariance matrix at each iteration.

        Returns:
            The computed loss.

        Raises:
            AssertionError: If z_a or z_b have a batch size <= 1.
            AssertionError: If z_a and z_b do not have the same shape.
        """

        assert (
            z_a.shape[0] > 1 and z_b.shape[0] > 1
        ), f"z_a and z_b must have batch size > 1 but found {z_a.shape[0]} and {z_b.shape[0]}"
        assert (
            z_a.shape == z_b.shape
        ), f"z_a and z_b must have same shape but found {z_a.shape} and {z_b.shape}."

        # gather all batches
        if self.gather_distributed and dist.is_initialized():
            world_size = dist.get_world_size()
            if world_size > 1:
                z_a = torch.cat(gather(z_a), dim=0)
                z_b = torch.cat(gather(z_b), dim=0)

        # Normalize image
        z_a = torch.nn.functional.normalize(z_a, dim=1)
        z_b = torch.nn.functional.normalize(z_b, dim=1)

        # Compute auxiliary matrix B
        B = torch.mm(z_a.T, z_a).detach() / z_a.shape[0]

        # Initialize covariance matrix
        if self.C is None:
            self.C = B.new_zeros(B.shape).detach()

        # Compute loss
        C = self.beta * self.C + (1 - self.beta) * B

        transformative_invariance_loss = 1.0 - (z_a * z_b).sum(dim=1).mean()
        covariance_contrast_loss = self.rho * (torch.mm(z_a, C) * z_a).sum(dim=1).mean()

        loss: Tensor = transformative_invariance_loss + covariance_contrast_loss

        # Update covariance matrix
        if update_covariance_matrix:
            self.C = C.detach()

        return loss



================================================
FILE: lightly/loss/vicreg_loss.py
================================================
import torch
import torch.distributed as dist
import torch.nn.functional as F
from torch import Tensor

from lightly.utils.dist import gather


class VICRegLoss(torch.nn.Module):
    """Implementation of the VICReg loss [0].

    This implementation is based on the code published by the authors [1].

    - [0] VICReg, 2022, https://arxiv.org/abs/2105.04906
    - [1] https://github.com/facebookresearch/vicreg/

    Attributes:
        lambda_param:
            Scaling coefficient for the invariance term of the loss.
        mu_param:
            Scaling coefficient for the variance term of the loss.
        nu_param:
            Scaling coefficient for the covariance term of the loss.
        gather_distributed:
            If True, the cross-correlation matrices from all GPUs are gathered and
            summed before the loss calculation.
        eps:
            Epsilon for numerical stability.

    Examples:
        >>> # initialize loss function
        >>> loss_fn = VICRegLoss()
        >>>
        >>> # generate two random transforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # feed through model
        >>> out0, out1 = model(t0, t1)
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(out0, out1)
    """

    def __init__(
        self,
        lambda_param: float = 25.0,
        mu_param: float = 25.0,
        nu_param: float = 1.0,
        gather_distributed: bool = False,
        eps: float = 0.0001,
    ):
        """Initializes the VICRegLoss module with the specified parameters.

        Raises:
            ValueError: If gather_distributed is True but torch.distributed is not available.
        """
        super(VICRegLoss, self).__init__()
        if gather_distributed and not dist.is_available():
            raise ValueError(
                "gather_distributed is True but torch.distributed is not available. "
                "Please set gather_distributed=False or install a torch version with "
                "distributed support."
            )

        self.lambda_param = lambda_param
        self.mu_param = mu_param
        self.nu_param = nu_param
        self.gather_distributed = gather_distributed
        self.eps = eps

    def forward(self, z_a: torch.Tensor, z_b: torch.Tensor) -> torch.Tensor:
        """Returns VICReg loss.

        Args:
            z_a:
                Tensor with shape (batch_size, ..., dim).
            z_b:
                Tensor with shape (batch_size, ..., dim).

        Returns:
            The computed VICReg loss.

        Raises:
            AssertionError: If z_a or z_b have a batch size <= 1.
            AssertionError: If z_a and z_b do not have the same shape.
        """
        assert (
            z_a.shape[0] > 1 and z_b.shape[0] > 1
        ), f"z_a and z_b must have batch size > 1 but found {z_a.shape[0]} and {z_b.shape[0]}"
        assert (
            z_a.shape == z_b.shape
        ), f"z_a and z_b must have same shape but found {z_a.shape} and {z_b.shape}."

        # Invariance term of the loss
        inv_loss = invariance_loss(x=z_a, y=z_b)

        # Gather all batches
        if self.gather_distributed and dist.is_initialized():
            world_size = dist.get_world_size()
            if world_size > 1:
                z_a = torch.cat(gather(z_a), dim=0)
                z_b = torch.cat(gather(z_b), dim=0)

        # Variance and covariance terms of the loss
        var_loss = 0.5 * (
            variance_loss(x=z_a, eps=self.eps) + variance_loss(x=z_b, eps=self.eps)
        )
        cov_loss = covariance_loss(x=z_a) + covariance_loss(x=z_b)

        # Total VICReg loss
        loss = (
            self.lambda_param * inv_loss
            + self.mu_param * var_loss
            + self.nu_param * cov_loss
        )
        return loss


def invariance_loss(x: Tensor, y: Tensor) -> Tensor:
    """Returns VICReg invariance loss.

    Args:
        x:
            Tensor with shape (batch_size, ..., dim).
        y:
            Tensor with shape (batch_size, ..., dim).

    Returns:
        The computed VICReg invariance loss.
    """
    return F.mse_loss(x, y)


def variance_loss(x: Tensor, eps: float = 0.0001) -> Tensor:
    """Returns VICReg variance loss.

    Args:
        x:
            Tensor with shape (batch_size, ..., dim).
        eps:
            Epsilon for numerical stability.

    Returns:
        The computed VICReg variance loss.
    """
    std = torch.sqrt(x.var(dim=0) + eps)
    loss = torch.mean(F.relu(1.0 - std))
    return loss


def covariance_loss(x: Tensor) -> Tensor:
    """Returns VICReg covariance loss.

    Generalized version of the covariance loss with support for tensors with more than
    two dimensions. Adapted from VICRegL:
    https://github.com/facebookresearch/VICRegL/blob/803ae4c8cd1649a820f03afb4793763e95317620/main_vicregl.py#L299

    Args:
        x: Tensor with shape (batch_size, ..., dim).

    Returns:
          The computed VICReg covariance loss.
    """
    x = x - x.mean(dim=0)
    batch_size = x.size(0)
    dim = x.size(-1)
    # nondiag_mask has shape (dim, dim) with 1s on all non-diagonal entries.
    nondiag_mask = ~torch.eye(dim, device=x.device, dtype=torch.bool)

    # cov has shape (..., dim, dim)
    cov = torch.einsum("b...c,b...d->...cd", x, x) / (batch_size - 1)

    loss = cov[..., nondiag_mask].pow(2).sum(-1) / dim
    return loss.mean()



================================================
FILE: lightly/loss/vicregl_loss.py
================================================
from typing import Optional, Sequence, Tuple

import torch
import torch.distributed as dist
from torch import Tensor
from torch.nn import Module

from lightly.loss.vicreg_loss import (
    VICRegLoss,
    covariance_loss,
    invariance_loss,
    variance_loss,
)
from lightly.models import utils
from lightly.utils.dist import gather


class VICRegLLoss(Module):
    """Implementation of the VICRegL loss from VICRegL paper [0].

    This implementation follows the code published by the authors [1].

    - [0]: VICRegL, 2022, https://arxiv.org/abs/2210.01571
    - [1]: https://github.com/facebookresearch/VICRegL

    Attributes:
        lambda_param:
            Coefficient for the invariance term of the loss.
        mu_param:
            Coefficient for the variance term of the loss.
        nu_param:
            Coefficient for the covariance term of the loss.
        alpha:
            Coefficient to weight global with local loss. The final loss is computed as
            (self.alpha * global_loss + (1-self.alpha) * local_loss).
        gather_distributed:
            If True, the cross-correlation matrices from all gpus are gathered and
            summed before the loss calculation.
        eps:
            Epsilon for numerical stability.
        num_matches:
            Number of local features to match using nearest neighbors.

    Examples:
        >>> # initialize loss function
        >>> criterion = VICRegLLoss()
        >>> transform = VICRegLTransform(n_global_views=2, n_local_views=4)
        >>>
        >>> # generate two random transforms of images
        >>> views_and_grids = transform(images)
        >>> views = views_and_grids[:6] # 2 global views + 4 local views
        >>> grids = views_and_grids[6:]
        >>>
        >>> # feed through model images
        >>> features = [model(view) for view in views]
        >>>
        >>> # calculate loss
        >>> loss = criterion(
        ...     global_view_features=features[:2],
        ...     global_view_grids=grids[:2],
        ...     local_view_features=features[2:],
        ...     local_view_grids=grids[2:],
        ... )
    """

    def __init__(
        self,
        lambda_param: float = 25.0,
        mu_param: float = 25.0,
        nu_param: float = 1.0,
        alpha: float = 0.75,
        gather_distributed: bool = False,
        eps: float = 0.0001,
        num_matches: Tuple[int, int] = (20, 4),
    ):
        """Initializes the VICRegL loss module with the specified parameters.

        Raises:
            ValueError: If gather_distributed is True but torch.distributed is not available.
        """
        super(VICRegLLoss, self).__init__()
        self.alpha = alpha
        self.num_matches = num_matches
        self.lambda_param = lambda_param
        self.mu_param = mu_param
        self.nu_param = nu_param
        self.eps = eps
        self.gather_distributed = gather_distributed
        # Note: We multiply nu_param by 0.5 because the implementations of the VICReg
        # covariance loss differ by a factor of 0.5 between the original VICReg and
        # VICRegL codebases. See:
        # - VICReg: https://github.com/facebookresearch/vicreg/blob/4e12602fd495af83efd1631fbe82523e6db092e0/main_vicreg.py#L211-L213
        # - VICRegL: https://github.com/facebookresearch/VICRegL/blob/803ae4c8cd1649a820f03afb4793763e95317620/main_vicregl.py#L308-L312
        self.vicreg_loss = VICRegLoss(
            lambda_param=lambda_param,
            mu_param=mu_param,
            nu_param=0.5 * nu_param,
            eps=eps,
            gather_distributed=gather_distributed,
        )

    def forward(
        self,
        global_view_features: Sequence[Tuple[Tensor, Tensor]],
        global_view_grids: Sequence[Tensor],
        local_view_features: Optional[Sequence[Tuple[Tensor, Tensor]]] = None,
        local_view_grids: Optional[Sequence[Tensor]] = None,
    ) -> Tensor:
        """Computes the global and local VICRegL loss from the input features.

        Args:
            global_view_features:
                Sequence of (global_features, local_features) tuples from the global
                crop views. global_features must have size
                (batch_size, global_feature_dim) and local_features must have size
                (batch_size, grid_height, grid_width, local_feature_dim).
            global_view_grids:
                Sequence of grid tensors from the global crop views. Every tensor must
                have shape (batch_size, grid_height, grid_width, 2).
            local_view_features:
                Sequence of (global_features, local_features) tuples from the local crop
                views. global_features must have size
                (batch_size, global_feature_dim) and local_features must have size
                (batch_size, grid_height, grid_width, local_feature_dim). Note that
                grid_height and grid_width can differ between global_view_features and
                local_view_features.
            local_view_grids:
                Sequence of grid tensors from the local crop views. Every tensor must
                have shape (batch_size, grid_height, grid_width, 2). Note that
                grid_height and grid_width can differ between global_view_features and
                local_view_features.

        Returns:
            Weighted sum of the global and local loss, calculated as:
            `self.alpha * global_loss + (1-self.alpha) * local_loss`.

        Raises:
            ValueError: If the lengths of global_view_features and global_view_grids are not the same.
            ValueError: If the lengths of local_view_features and local_view_grids are not the same.
            ValueError: If only one of local_view_features or local_view_grids is set.
        """
        if len(global_view_features) != len(global_view_grids):
            raise ValueError(
                f"global_view_features and global_view_grids must have same length "
                f"but found {len(global_view_features)} and {len(global_view_grids)}."
            )
        if local_view_features is not None and local_view_grids is not None:
            if len(local_view_features) != len(local_view_grids):
                raise ValueError(
                    f"local_view_features and local_view_grids must have same length "
                    f"but found {len(local_view_features)} and {len(local_view_grids)}."
                )
        elif local_view_features is not None or local_view_grids is not None:
            raise ValueError(
                f"local_view_features and local_view_grids must either both be set or "
                f"None but found {type(local_view_features)} and {type(local_view_grids)}."
            )

        # Calculate loss from global features
        global_loss = self._global_loss(
            global_view_features=global_view_features,
            local_view_features=local_view_features,
        )

        # Calculate loss from local features
        local_loss = self._local_loss(
            global_view_features=global_view_features,
            global_view_grids=global_view_grids,
            local_view_features=local_view_features,
            local_view_grids=local_view_grids,
        )

        loss = self.alpha * global_loss + (1 - self.alpha) * local_loss
        return loss

    def _global_loss(
        self,
        global_view_features: Sequence[Tuple[Tensor, Tensor]],
        local_view_features: Optional[Sequence[Tuple[Tensor, Tensor]]] = None,
    ) -> Tensor:
        """Returns global features loss.

        Args:
        global_view_features:
                Sequence of (global_features, local_features)
                tuples from the global crop views.
        local_view_features:
                Sequence of (global_features,local_features)
                tuples from the local crop views.

        Returns:
            The computed global features loss.
        """
        inv_loss = self._global_invariance_loss(
            global_view_features=global_view_features,
            local_view_features=local_view_features,
        )
        var_loss, cov_loss = self._global_variance_and_covariance_loss(
            global_view_features=global_view_features,
            local_view_features=local_view_features,
        )
        return (
            self.lambda_param * inv_loss
            + self.mu_param * var_loss
            + self.nu_param * cov_loss
        )

    def _global_invariance_loss(
        self,
        global_view_features: Sequence[Tuple[Tensor, Tensor]],
        local_view_features: Optional[Sequence[Tuple[Tensor, Tensor]]] = None,
    ) -> Tensor:
        """Returns invariance loss from global features.

        Args:
            global_view_features:
                        Sequence of (global_features, local_features)
                        tuples from the global crop views.
            local_view_features:
                        Sequence of (global_features,local_features)
                        tuples from the local crop views.

        Returns:
            The computed invariance loss from global features.
        """
        loss = global_view_features[0][0].new_zeros(1)
        loss_count = loss.new_zeros(1)

        # Compute invariance loss between global views
        for global_features_a, _ in global_view_features:
            for global_features_b, _ in global_view_features:
                if global_features_a is not global_features_b:
                    loss += invariance_loss(global_features_a, global_features_b)
                    loss_count += 1

            # Compute invariance loss between global and local views
            if local_view_features is not None:
                for global_features_b, _ in local_view_features:
                    loss += invariance_loss(global_features_a, global_features_b)
                    loss_count += 1

        return loss / loss_count

    def _global_variance_and_covariance_loss(
        self,
        global_view_features: Sequence[Tuple[Tensor, Tensor]],
        local_view_features: Optional[Sequence[Tuple[Tensor, Tensor]]] = None,
    ) -> Tuple[Tensor, Tensor]:
        """Returns variance and covariance loss from global features.

        Args:
            global_view_features: Sequence of (global_features, local_features)
                    tuples from the global crop views.
            local_view_features: Sequence of (global_features,local_features)
                    tuples from the local crop views.

        Returns:
            The computed variance and covariance loss from global features.
        """
        view_features = list(global_view_features)
        if local_view_features is not None:
            view_features = view_features + list(local_view_features)

        var_loss = global_view_features[0][0].new_zeros(1)
        cov_loss = var_loss.new_zeros(1)
        loss_count = var_loss.new_zeros(1)
        for global_features, _ in view_features:
            if self.gather_distributed and dist.is_initialized():
                world_size = dist.get_world_size()
                if world_size > 1:
                    global_features = torch.cat(gather(global_features), dim=0)

            var_loss += variance_loss(x=global_features, eps=self.eps)
            cov_loss += covariance_loss(x=global_features)
            loss_count += 1
        return var_loss / loss_count, cov_loss / loss_count

    def _local_loss(
        self,
        global_view_features: Sequence[Tuple[Tensor, Tensor]],
        global_view_grids: Sequence[Tensor],
        local_view_features: Optional[Sequence[Tuple[Tensor, Tensor]]] = None,
        local_view_grids: Optional[Sequence[Tensor]] = None,
    ) -> Tensor:
        """Returns loss from local features based on nearest neighbor matching.

        Note: Our nearest neighbor implementation returns the selected features sorted
        by increasing matching distance, whereas the implementation by the VICRegL
        authors returns features in a different order [1]. This results in slight
        differences of the final local loss. The difference results from feature
        centering which depends on the order.

        Note: Nearest neighbor matching slightly differs between the paper [0] and the
        original implementation of the authors [1]. The paper mentions that
        num_matches is set to 20 for global views and 4 for local views. The code
        uses 20 matches for the first NN search and 4 matches for the second search,
        regardless of global or local views:
        https://github.com/facebookresearch/VICRegL/blob/803ae4c8cd1649a820f03afb4793763e95317620/main_vicregl.py#L329-L334
        Our implementation follows the original code and ignores view type.

        Args:
            global_view_features:
                Sequence of (global_features, local_features) tuples from the global crop views.
            global_view_grids:
                Sequence of grid tensors from the global crop views.
            local_view_features:
                Sequence of (global_features,local_features) tuples from the local crop views.
            local_view_grids:
                Sequence of grid tensors from the local crop views.

        Returns:
            The computed loss from local features based on nearest neighbor matching.
        """
        loss = global_view_features[0][0].new_zeros(1)
        loss_count = loss.new_zeros(1)

        # Compute the loss for global views
        for (_, z_a_local_features), grid_a in zip(
            global_view_features, global_view_grids
        ):
            for (_, z_b_local_features), grid_b in zip(
                global_view_features, global_view_grids
            ):
                if z_a_local_features is not z_b_local_features:
                    loss += self._local_l2_loss(
                        z_a=z_a_local_features,
                        z_b=z_b_local_features,
                    )
                    loss += self._local_location_loss(
                        z_a=z_a_local_features,
                        z_b=z_b_local_features,
                        grid_a=grid_a,
                        grid_b=grid_b,
                    )
                    loss_count += 1

            # Compute the loss for local views
            if local_view_features is not None and local_view_grids is not None:
                for (_, z_b_local_features), grid_b in zip(
                    local_view_features, local_view_grids
                ):
                    loss += self._local_l2_loss(
                        z_a=z_a_local_features,
                        z_b=z_b_local_features,
                    )
                    loss += self._local_location_loss(
                        z_a=z_a_local_features,
                        z_b=z_b_local_features,
                        grid_a=grid_a,
                        grid_b=grid_b,
                    )
                    loss_count += 1
        return loss / loss_count

    def _local_l2_loss(
        self,
        z_a: Tensor,
        z_b: Tensor,
    ) -> Tensor:
        """Returns loss for local features matched with neareast neighbors using L2
        distance in the feature space.

        Args:
            z_a:
                Local feature tensor with shape (batch_size, height, width, dim).
            z_b:
                Local feature tensor with shape (batch_size, height, width, dim).

        Returns:
            The computed loss for local features.
        """
        # (batch_size, height, width, dim) -> (batch_size, height * width, dim)
        z_a = z_a.flatten(start_dim=1, end_dim=2)
        z_b = z_b.flatten(start_dim=1, end_dim=2)

        # Find nearest neighbours using L2 distance
        z_a_filtered, z_a_nn = self._nearest_neighbors_on_l2(
            input_features=z_a, candidate_features=z_b, num_matches=self.num_matches[0]
        )
        z_b_filtered, z_b_nn = self._nearest_neighbors_on_l2(
            input_features=z_b, candidate_features=z_a, num_matches=self.num_matches[1]
        )

        # Compute VICReg losses
        loss_a = self.vicreg_loss.forward(z_a=z_a_filtered, z_b=z_a_nn)
        loss_b = self.vicreg_loss.forward(z_a=z_b_filtered, z_b=z_b_nn)

        return 0.5 * (loss_a + loss_b)

    def _local_location_loss(
        self,
        z_a: Tensor,
        z_b: Tensor,
        grid_a: Tensor,
        grid_b: Tensor,
    ) -> Tensor:
        """Returns loss for local features matched with nearest neighbors based on
        the feature location.

        Args:
            z_a:
                Local feature tensor with shape (batch_size, height, width, dim).
            z_b:
                Local feature tensor with shape (batch_size, height, width, dim).
                Note that height and width can be different from z_a.
            grid_a:
                Grid tensor with shape (batch_size, height, width, 2).
            grid_b:
                Grid tensor with shape (batch_size, height, width, 2).
                Note that height and width can be different from grid_a.

        Returns:
            The computed loss for local features based on nearest neighbour matching.
        """
        # (batch_size, height, width, dim) -> (batch_size, height * width, dim)
        z_a = z_a.flatten(start_dim=1, end_dim=2)
        z_b = z_b.flatten(start_dim=1, end_dim=2)

        # (batch_size, height, width, 2) -> (batch_size, height * width, 2)
        grid_a = grid_a.flatten(start_dim=1, end_dim=2)
        grid_b = grid_b.flatten(start_dim=1, end_dim=2)

        # Find nearest neighbours based on grid location
        z_a_filtered, z_a_nn = self._nearest_neighbors_on_grid(
            input_features=z_a,
            candidate_features=z_b,
            input_grid=grid_a,
            candidate_grid=grid_b,
            num_matches=self.num_matches[0],
        )
        z_b_filtered, z_b_nn = self._nearest_neighbors_on_grid(
            input_features=z_b,
            candidate_features=z_a,
            input_grid=grid_b,
            candidate_grid=grid_a,
            num_matches=self.num_matches[1],
        )

        # Compute VICReg losses
        loss_a = self.vicreg_loss.forward(z_a=z_a_filtered, z_b=z_a_nn)
        loss_b = self.vicreg_loss.forward(z_a=z_b_filtered, z_b=z_b_nn)
        return 0.5 * (loss_a + loss_b)

    def _nearest_neighbors_on_l2(
        self, input_features: Tensor, candidate_features: Tensor, num_matches: int
    ) -> Tuple[Tensor, Tensor]:
        """Finds num_matches closest neighbors of input_features in candidate_features.

        Args:
            input_features:
                Local features tensor with shape (batch_size, height * width, dim).
            candidate_features:
                Local features tensor with shape (batch_size, height * width, dim).
                Note that height and width can be different from input_features.

        Returns:
            (nn_input, nn_candidate) tuple containing two tensors with shape
            (batch_size, num_matches, dim).
        """
        distances = torch.cdist(input_features, candidate_features)
        # TODO(Philipp, 12/24): Remove type ignore when utils are typechecked.
        return utils.nearest_neighbors(  # type: ignore[no-any-return]
            input_features, candidate_features, distances, num_matches
        )

    def _nearest_neighbors_on_grid(
        self,
        input_features: Tensor,
        candidate_features: Tensor,
        input_grid: Tensor,
        candidate_grid: Tensor,
        num_matches: int,
    ) -> Tuple[Tensor, Tensor]:
        """Finds num_matches closest neighbors of input_features in candidate_features
        based on the distance between the features defined by input_grid and
        candidate_grid.

        Args:
            input_features:
                Local features tensor with shape (batch_size, height * width, dim).
            candidate_features:
                Local features tensor with shape (batch_size, height * width, dim).
                Note that height and width can be different from input_features.
            input_grid:
                Grid tensor with shape (batch_size, height, width, 2).
            candidate_grid:
                Grid tensor with shape (batch_size, height, width, 2). Note that height
                and width can be different from input_grid.

        Returns:
            (nn_input, nn_candidate) tuple containing two tensors with shape
            (batch_size, num_matches, dim).
        """
        distances: Tensor = torch.cdist(input_grid, candidate_grid)
        # TODO(Philipp, 12/24): Remove type ignore when utils are typechecked.
        return utils.nearest_neighbors(  # type: ignore[no-any-return]
            input_features, candidate_features, distances, num_matches
        )



================================================
FILE: lightly/loss/wmse_loss.py
================================================
"""Code for W-MSE Loss, largely taken from https://github.com/htdt/self-supervised"""

from typing import Callable

import torch
import torch.nn as nn
import torch.nn.functional as F

try:
    import torch.linalg.solve_triangular
except ImportError:
    # Only available in PyTorch >=1.11.
    _SOLVE_TRIANGULAR_AVAILABLE = False
else:
    _SOLVE_TRIANGULAR_AVAILABLE = True


def norm_mse_loss(x0: torch.Tensor, x1: torch.Tensor) -> torch.Tensor:
    """Normalized MSE Loss as implemented in https://github.com/htdt/self-supervised.

    Args:
        x0: First input tensor.
        x1: Second input tensor.

    Returns:
        The computed normalized MSE loss.
    """
    x0 = F.normalize(x0)
    x1 = F.normalize(x1)
    return torch.sub(input=2, other=(x0 * x1).sum(dim=-1).mean(), alpha=2)


class Whitening2d(nn.Module):
    """Implementation of the whitening layer as described in [0].

    - [0] W-MSE, 2021, https://arxiv.org/pdf/2007.06346.pdf
    """

    def __init__(
        self,
        num_features: int,
        momentum: float = 0.01,
        track_running_stats: bool = True,
        eps: float = 0,
    ):
        """Initializes the Whitening2d module with the specified parameters.

        Args:
            num_features:
                Number of features in the input.
            momentum:
                Momentum for the running mean and variance.
            track_running_stats:
                If True, tracks the running mean and variance.
            eps:
                Epsilon for numerical stability.

        Raises:
            RuntimeError: If torch.linalg.solve_triangular is not available in the PyTorch installation.
        """

        super(Whitening2d, self).__init__()

        if not _SOLVE_TRIANGULAR_AVAILABLE:
            raise RuntimeError(
                "Whitening2d depends on torch.linalg.solve_triangular which is not "
                "available in your PyTorch installation. Please update to PyTorch 1.11 "
                "or newer."
            )

        self.running_mean: torch.Tensor
        self.running_variance: torch.Tensor
        self.num_features = num_features
        self.momentum = momentum
        self.track_running_stats = track_running_stats
        self.eps = eps

        if self.track_running_stats:
            self.register_buffer(
                "running_mean", torch.zeros([1, self.num_features, 1, 1])
            )
            self.register_buffer("running_variance", torch.eye(self.num_features))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass of the Whitening2d layer.

        Args:
            x: Input tensor.

        Returns:
            Decorrelated output tensor.

        """
        x = x.unsqueeze(2).unsqueeze(3)
        m = x.mean(0).view(self.num_features, -1).mean(-1).view(1, -1, 1, 1)
        if not self.training and self.track_running_stats:  # for inference
            m = self.running_mean
        xn = x - m

        # Reshape for covariance computation
        T = xn.permute(1, 0, 2, 3).contiguous().view(self.num_features, -1)

        # Compute covariance matrix
        f_cov = torch.mm(T, T.permute(1, 0)) / (T.shape[-1] - 1)

        eye = torch.eye(self.num_features).type(f_cov.type())

        if not self.training and self.track_running_stats:  # for inference
            f_cov = self.running_variance

        f_cov_shrinked = (1 - self.eps) * f_cov + self.eps * eye

        inv_sqrt = torch.linalg.solve_triangular(
            torch.linalg.cholesky(f_cov_shrinked), eye, upper=False
        )

        inv_sqrt = inv_sqrt.contiguous().view(
            self.num_features, self.num_features, 1, 1
        )

        # Decorrelate the features
        decorrelated = F.conv2d(xn, inv_sqrt)

        if self.training and self.track_running_stats:
            self.running_mean = torch.add(
                self.momentum * m.detach(),
                (1 - self.momentum) * self.running_mean,
                out=self.running_mean,
            )
            self.running_variance = torch.add(
                self.momentum * f_cov.detach(),
                (1 - self.momentum) * self.running_variance,
                out=self.running_variance,
            )

        return decorrelated.squeeze(2).squeeze(2)


class WMSELoss(torch.nn.Module):
    """Implementation of the loss described in 'Whitening for
    Self-Supervised Representation Learning' [0].

    - [0] W-MSE, 2021, https://arxiv.org/pdf/2007.06346.pdf

    Examples:
        >>> # initialize loss function
        >>> loss_fn = WMSELoss(num_samples=2)
        >>> transform_fn = WMSETransform(num_samples=2)
        >>>
        >>> # generate the transformed samples
        >>> samples = transform_fn(image)
        >>>
        >>> # feed through encoder head
        >>> h = torch.cat([model(s) for s in samples])
        >>>
        >>> # calculate loss
        >>> loss = loss_fn(samples, h)

    """

    def __init__(
        self,
        embedding_dim: int = 128,
        momentum: float = 0.01,
        eps: float = 0.0,
        track_running_stats: bool = True,
        w_iter: int = 1,
        w_size: int = 256,
        loss_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = norm_mse_loss,
        num_samples: int = 2,
    ):
        """Initializes the WMSELoss module with the specified parameters.

        Parameters as described in [0].

        Args:
            embedding_dim:
                Dimensionality of the embedding.
            momentum:
                Momentum for the running statistics.
            eps:
                Epsilon for the running statistics.
            track_running_stats:
                Whether to track running statistics.
            w_iter:
                Number of iterations for the whitening.
            w_size:
                Sub-batch size to use for whitening.
            loss_fn:
                Loss function to use for the whitening.
            num_samples:
                Number of samples generated by the transforms for each image.

        Raises:
            ValueError: If w_size is less than twice the size of embedding_dim.
        """
        super().__init__()
        self.whitening = Whitening2d(
            num_features=embedding_dim,
            momentum=momentum,
            eps=eps,
            track_running_stats=track_running_stats,
        )
        if embedding_dim * 2 > w_size:
            raise ValueError(
                "w_size should be at least twice the size of embedding_dim to avoid instabiliy"
            )
        self.w_iter = w_iter
        self.w_size = w_size
        self.loss_f = loss_fn
        self.num_samples = num_samples
        self.num_pairs = num_samples * (num_samples - 1) // 2

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        """Calculates the W-MSE loss.

        Args:
            input: Tensor with shape (batch_size * num_samples, embedding_dim).

        Returns:
            Aggregate W-MSE loss over all sub-batches.

        Raises:
            RuntimeError: If the batch size is not divisible by num_samples.
            ValueError: If the batch size is smaller than w_size.
        """
        if input.shape[0] % self.num_samples != 0:
            raise RuntimeError("input batch size must be divisible by num_samples")

        bs = input.shape[0] // self.num_samples

        if bs < self.w_size:
            raise ValueError("batch size must be greater than or equal to w_size")
        loss = torch.tensor(0.0, device=input.device, requires_grad=True)

        for _ in range(self.w_iter):
            z = torch.empty_like(input)
            perm = torch.randperm(bs).view(-1, self.w_size)
            for idx in perm:
                for i in range(self.num_samples):
                    z[idx + i * bs] = self.whitening(input[idx + i * bs])
            for i in range(self.num_samples - 1):
                for j in range(i + 1, self.num_samples):
                    x0 = z[i * bs : (i + 1) * bs]
                    x1 = z[j * bs : (j + 1) * bs]
                    loss = loss + self.loss_f(x0, x1)
        loss = loss / (self.w_iter * self.num_pairs)
        return loss



================================================
FILE: lightly/loss/regularizer/__init__.py
================================================
"""The lightly.loss.regularizer package provides regularizers for self-supervised learning. """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

from lightly.loss.regularizer.co2 import CO2Regularizer



================================================
FILE: lightly/loss/regularizer/co2.py
================================================
""" CO2 Regularizer """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

from typing import Sequence, Union

import torch
from torch import Tensor
from torch.nn import Module

from lightly.models.modules.memory_bank import MemoryBankModule


class CO2Regularizer(Module):
    """Implementation of the CO2 regularizer [0] for self-supervised learning.

    - [0] CO2, 2021, https://arxiv.org/abs/2010.02217

    Attributes:
        alpha:
            Weight of the regularization term.
        t_consistency:
            Temperature used during softmax calculations.
        memory_bank_size:
            Size of the memory bank as (num_features, dim) tuple. num_features is the
            number of negatives stored in the bank. If set to 0, the memory bank is
            disabled. Deprecated: If only a single integer is passed, it is interpreted
            as the number of features and the feature dimension is inferred from the
            first batch stored in the memory bank. Leaving out the feature dimension
            might lead to errors in distributed training.

    Examples:
        >>> # initialize loss function for MoCo
        >>> loss_fn = NTXentLoss(memory_bank_size=(4096, 128))
        >>>
        >>> # initialize CO2 regularizer
        >>> co2 = CO2Regularizer(alpha=1.0, memory_bank_size=(4096, 128))
        >>>
        >>> # generate two random trasnforms of images
        >>> t0 = transforms(images)
        >>> t1 = transforms(images)
        >>>
        >>> # feed through the MoCo model
        >>> out0, out1 = model(t0, t1)
        >>>
        >>> # calculate loss and apply regularizer
        >>> loss = loss_fn(out0, out1) + co2(out0, out1)
    """

    def __init__(
        self,
        alpha: float = 1,
        t_consistency: float = 0.05,
        memory_bank_size: Union[int, Sequence[int]] = 0,
    ):
        """Initializes the CO2Regularizer with the specified parameters.

        Args:
            alpha:
                Weight of the regularization term.
            t_consistency:
                Temperature used during softmax calculations.
            memory_bank_size:
                Size of the memory bank.
        """
        super().__init__()
        self.memory_bank = MemoryBankModule(size=memory_bank_size)
        # Try-catch the KLDivLoss construction for backwards compatability
        self.log_target = True
        try:
            self.kl_div = torch.nn.KLDivLoss(reduction="batchmean", log_target=True)
        except TypeError:
            self.log_target = False
            self.kl_div = torch.nn.KLDivLoss(reduction="batchmean")

        self.t_consistency = t_consistency
        self.alpha = alpha

    def forward(self, out0: Tensor, out1: Tensor) -> Tensor:
        """Computes the CO2 regularization term for two model outputs.

        Args:
            out0:
                Output projections of the first set of transformed images.
            out1:
                Output projections of the second set of transformed images.

        Returns:
            The regularization term multiplied by the weight factor alpha.
        """

        # Normalize the output to length 1
        out0 = torch.nn.functional.normalize(out0, dim=1)
        out1 = torch.nn.functional.normalize(out1, dim=1)

        # Update the memory bank with out1 and get negatives(if memory bank size > 0)
        # If the memory_bank size is 0, negatives will be None
        out1, negatives = self.memory_bank.forward(out1, update=True)

        # Get log probabilities
        p = self._get_pseudo_labels(out0, out1, negatives)
        q = self._get_pseudo_labels(out1, out0, negatives)

        # Calculate symmetrized Kullback-Leibler divergence
        if self.log_target:
            div = self.kl_div(p, q) + self.kl_div(q, p)
        else:
            # Can't use log_target because of early torch version
            div = self.kl_div(p, torch.exp(q)) + self.kl_div(q, torch.exp(p))

        return torch.tensor(self.alpha * 0.5 * div)

    def _get_pseudo_labels(
        self, out0: Tensor, out1: Tensor, negatives: Union[Tensor, None] = None
    ) -> Tensor:
        """Computes the soft pseudo labels across negative samples.

        Args:
            out0:
                Output projections of the first set of transformed images (query).
                Shape: bsz x n_ftrs
            out1:
                Output projections of the second set of transformed images (positive sample).
                Shape: bsz x n_ftrs
            negatives:
                Negative samples to compare against. If this is None, the second
                batch of images will be used as negative samples.
                Shape: memory_bank_size x n_ftrs

        Returns:
            Log probability that a positive samples will classify each negative
            sample as the positive sample.
            Shape: bsz x (bsz - 1) or bsz x memory_bank_size
        """
        batch_size, _ = out0.shape
        if negatives is None:
            # Use second batch as negative samples
            # l_pos has shape bsz x 1 and l_neg has shape bsz x bsz
            l_pos = torch.einsum("nc,nc->n", [out0, out1]).unsqueeze(-1)
            l_neg = torch.einsum("nc,ck->nk", [out0, out1.t()])

            # Remove elements on the diagonal
            # l_neg has shape bsz x (bsz - 1)
            l_neg = l_neg.masked_select(
                ~torch.eye(batch_size, dtype=torch.bool, device=l_neg.device)
            ).view(batch_size, batch_size - 1)
        else:
            # Use memory bank as negative samples
            # l_pos has shape bsz x 1 and l_neg has shape bsz x memory_bank_size
            negatives = negatives.to(out0.device)
            l_pos = torch.einsum("nc,nc->n", [out0, out1]).unsqueeze(-1)
            l_neg = torch.einsum("nc,ck->nk", [out0, negatives.clone().detach()])

        # Concatenate such that positive samples are at index 0
        logits = torch.cat([l_pos, l_neg], dim=1)
        # Divide by temperature
        logits = logits / self.t_consistency

        # The input to kl_div is expected to be log(p)
        return torch.nn.functional.log_softmax(logits, dim=-1)



================================================
FILE: lightly/models/__init__.py
================================================
"""The lightly.models package provides model implementations.

Note that the high-level building blocks will be deprecated with 
lightly version 1.3.0. Instead, use low-level building blocks to build the
models yourself.

Example implementations for all models can be found here:
`Model Examples <https://docs.lightly.ai/self-supervised-learning/examples/models.html>`_

The package contains an implementation of the commonly used ResNet and
adaptations of the architecture which make self-supervised learning simpler.

The package also hosts the Lightly model zoo - a list of downloadable ResNet
checkpoints.

"""

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

from lightly.models import utils
from lightly.models.barlowtwins import BarlowTwins
from lightly.models.byol import BYOL
from lightly.models.moco import MoCo
from lightly.models.nnclr import NNCLR
from lightly.models.resnet import ResNetGenerator
from lightly.models.simclr import SimCLR
from lightly.models.simsiam import SimSiam
from lightly.models.zoo import ZOO, checkpoints



================================================
FILE: lightly/models/_momentum.py
================================================
""" Momentum Encoder """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

import copy
from typing import Iterable, Tuple

import torch
import torch.nn as nn
from torch import Tensor
from torch.nn.parameter import Parameter


def _deactivate_requires_grad(params: Iterable[Parameter]) -> None:
    """Deactivates the requires_grad flag for all parameters."""
    for param in params:
        param.requires_grad = False


def _do_momentum_update(
    prev_params: Iterable[Parameter], params: Iterable[Parameter], m: float
) -> None:
    """Updates the weights of the previous parameters."""
    for prev_param, param in zip(prev_params, params):
        prev_param.data = prev_param.data * m + param.data * (1.0 - m)


class _MomentumEncoderMixin:
    """Mixin to provide momentum encoder functionalities.

    Provides the following functionalities:
        - Momentum encoder initialization.
        - Momentum updates.
        - Batch shuffling and unshuffling.

    To make use of the mixin, simply inherit from it:

    >>> class MyMoCo(nn.Module, _MomentumEncoderMixin):
    >>>
    >>>     def __init__(self, backbone):
    >>>         super(MyMoCo, self).__init__()
    >>>
    >>>         self.backbone = backbone
    >>>         self.projection_head = get_projection_head()
    >>>
    >>>         # initialize momentum_backbone and momentum_projection_head
    >>>         self._init_momentum_encoder()
    >>>
    >>>     def forward(self, x: Tensor):
    >>>
    >>>         # do the momentum update
    >>>         self._momentum_update(0.999)
    >>>
    >>>         # use momentum backbone
    >>>         y = self.momentum_backbone(x)
    >>>         y = self.momentum_projection_head(y)

    """

    m: float
    backbone: nn.Module
    projection_head: nn.Module
    momentum_backbone: nn.Module
    momentum_projection_head: nn.Module

    def _init_momentum_encoder(self) -> None:
        """Initializes momentum backbone and a momentum projection head."""
        assert self.backbone is not None
        assert self.projection_head is not None

        self.momentum_backbone = copy.deepcopy(self.backbone)
        self.momentum_projection_head = copy.deepcopy(self.projection_head)

        _deactivate_requires_grad(self.momentum_backbone.parameters())
        _deactivate_requires_grad(self.momentum_projection_head.parameters())

    @torch.no_grad()
    def _momentum_update(self, m: float = 0.999) -> None:
        """Performs the momentum update for the backbone and projection head."""
        _do_momentum_update(
            self.momentum_backbone.parameters(),
            self.backbone.parameters(),
            m=m,
        )
        _do_momentum_update(
            self.momentum_projection_head.parameters(),
            self.projection_head.parameters(),
            m=m,
        )

    @torch.no_grad()
    def _batch_shuffle(self, batch: Tensor) -> Tuple[Tensor, Tensor]:
        """Returns the shuffled batch and the indices to undo."""
        batch_size = batch.shape[0]
        shuffle = torch.randperm(batch_size, device=batch.device)
        return batch[shuffle], shuffle

    @torch.no_grad()
    def _batch_unshuffle(self, batch: Tensor, shuffle: Tensor) -> Tensor:
        """Returns the unshuffled batch."""
        unshuffle = torch.argsort(shuffle)
        return batch[unshuffle]



================================================
FILE: lightly/models/barlowtwins.py
================================================
""" Barlow Twins resnet-based Model [0]
[0] Zbontar,J. et.al. 2021. Barlow Twins... https://arxiv.org/abs/2103.03230
"""

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

import warnings

import torch
import torch.nn as nn

from lightly.models.modules import BarlowTwinsProjectionHead


class BarlowTwins(nn.Module):
    """Implementation of BarlowTwins[0] network.

    Recommended loss: :py:class:`lightly.loss.barlow_twins_loss.BarlowTwinsLoss`

    Default params are the ones explained in the original paper [0].
    [0] Zbontar,J. et.al. 2021. Barlow Twins... https://arxiv.org/abs/2103.03230

    Attributes:
        backbone:
            Backbone model to extract features from images.
            ResNet-50 in original paper [0].
        num_ftrs:
            Dimension of the embedding (before the projection head).
        proj_hidden_dim:
            Dimension of the hidden layer of the projection head. This should
            be the same size as `num_ftrs`.
        out_dim:
            Dimension of the output (after the projection head).

    """

    def __init__(
        self,
        backbone: nn.Module,
        num_ftrs: int = 2048,
        proj_hidden_dim: int = 8192,
        out_dim: int = 8192,
    ):
        super(BarlowTwins, self).__init__()

        self.backbone = backbone
        self.num_ftrs = num_ftrs
        self.proj_hidden_dim = proj_hidden_dim
        self.out_dim = out_dim

        self.projection_mlp = BarlowTwinsProjectionHead(
            num_ftrs, proj_hidden_dim, out_dim
        )

        warnings.warn(
            Warning(
                "The high-level building block BarlowTwins will be deprecated in version 1.3.0. "
                + "Use low-level building blocks instead. "
                + "See https://docs.lightly.ai/self-supervised-learning/lightly.models.html for more information"
            ),
            DeprecationWarning,
        )

    def forward(
        self, x0: torch.Tensor, x1: torch.Tensor = None, return_features: bool = False
    ):
        """Forward pass through BarlowTwins.

        Extracts features with the backbone and applies the projection
        head to the output space. If both x0 and x1 are not None, both will be
        passed through the backbone and projection. If x1 is None, only x0 will
        be forwarded.
        Barlow Twins only implement a projection head unlike SimSiam.

        Args:
            x0:
                Tensor of shape bsz x channels x W x H.
            x1:
                Tensor of shape bsz x channels x W x H.
            return_features:
                Whether or not to return the intermediate features backbone(x).

        Returns:
            The output projection of x0 and (if x1 is not None)
            the output projection of x1. If return_features is
            True, the output for each x is a tuple (out, f) where f are the
            features before the projection head.

        Examples:
            >>> # single input, single output
            >>> out = model(x)
            >>>
            >>> # single input with return_features=True
            >>> out, f = model(x, return_features=True)
            >>>
            >>> # two inputs, two outputs
            >>> out0, out1 = model(x0, x1)
            >>>
            >>> # two inputs, two outputs with return_features=True
            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)
        """
        # forward pass first input
        f0 = self.backbone(x0).flatten(start_dim=1)
        out0 = self.projection_mlp(f0)

        # append features if requested
        if return_features:
            out0 = (out0, f0)

        if x1 is None:
            return out0

        # forward pass second input
        f1 = self.backbone(x1).flatten(start_dim=1)
        out1 = self.projection_mlp(f1)

        # append features if requested
        if return_features:
            out1 = (out1, f1)

        return out0, out1



================================================
FILE: lightly/models/batchnorm.py
================================================
""" SplitBatchNorm Implementation """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

from __future__ import annotations

from typing import Any

import torch
import torch.nn as nn
from torch import Tensor


class SplitBatchNorm(nn.BatchNorm2d):
    """Simulates multi-gpu behaviour of BatchNorm in one gpu by splitting.

    Implementation was adapted from:
    https://github.com/davidcpage/cifar10-fast/blob/master/torch_backend.py

    Attributes:
        num_features:
            Number of input features.
        num_splits:
            Number of splits.

    """

    def __init__(self, num_features: int, num_splits: int, **kw: Any) -> None:
        super().__init__(num_features, **kw)
        self.num_splits = num_splits
        # Register buffers
        self.register_buffer(
            "running_mean", torch.zeros(num_features * self.num_splits)
        )
        self.register_buffer("running_var", torch.ones(num_features * self.num_splits))

    def train(self, mode: bool = True) -> SplitBatchNorm:
        # lazily collate stats when we are going to use them
        if (self.training is True) and (mode is False):
            assert self.running_mean is not None
            self.running_mean = torch.mean(
                self.running_mean.view(self.num_splits, self.num_features), dim=0
            ).repeat(self.num_splits)
            assert self.running_var is not None
            self.running_var = torch.mean(
                self.running_var.view(self.num_splits, self.num_features), dim=0
            ).repeat(self.num_splits)

        return super().train(mode)

    def forward(self, input: Tensor) -> Tensor:
        """Computes the SplitBatchNorm on the input."""
        # get input shape
        N, C, H, W = input.shape

        # during training, use different stats for each split and otherwise
        # use the stats from the first split
        momentum = 0.0 if self.momentum is None else self.momentum
        if self.training or not self.track_running_stats:
            result = nn.functional.batch_norm(
                input=input.view(-1, C * self.num_splits, H, W),
                running_mean=self.running_mean,
                running_var=self.running_var,
                weight=self.weight.repeat(self.num_splits),
                bias=self.bias.repeat(self.num_splits),
                training=True,
                momentum=momentum,
                eps=self.eps,
            ).view(N, C, H, W)
        else:
            # We have to ignore the type errors here, because we know that running_mean
            # and running_var are not None, but the type checker does not.
            result = nn.functional.batch_norm(
                input=input,
                running_mean=self.running_mean[: self.num_features],  # type: ignore[index]
                running_var=self.running_var[: self.num_features],  # type: ignore[index]
                weight=self.weight,
                bias=self.bias,
                training=False,
                momentum=momentum,
                eps=self.eps,
            )

        return result


def get_norm_layer(num_features: int, num_splits: int, **kw: Any) -> nn.Module:
    """Utility to switch between BatchNorm2d and SplitBatchNorm."""
    if num_splits > 0:
        return SplitBatchNorm(num_features, num_splits)
    else:
        return nn.BatchNorm2d(num_features)



================================================
FILE: lightly/models/byol.py
================================================
""" BYOL Model """

# Copyright (c) 2021. Lightly AG and its affiliates.
# All Rights Reserved

import warnings

import torch
import torch.nn as nn

from lightly.models._momentum import _MomentumEncoderMixin
from lightly.models.modules import BYOLProjectionHead


def _get_byol_mlp(num_ftrs: int, hidden_dim: int, out_dim: int):
    """Returns a 2-layer MLP with batch norm on the hidden layer.

    Reference (12.03.2021)
    https://arxiv.org/abs/2006.07733

    """
    modules = [
        nn.Linear(num_ftrs, hidden_dim),
        nn.BatchNorm1d(hidden_dim),
        nn.ReLU(),
        nn.Linear(hidden_dim, out_dim),
    ]
    return nn.Sequential(*modules)


class BYOL(nn.Module, _MomentumEncoderMixin):
    """Implementation of the BYOL architecture.

    Attributes:
        backbone:
            Backbone model to extract features from images.
        num_ftrs:
            Dimension of the embedding (before the projection mlp).
        hidden_dim:
            Dimension of the hidden layer in the projection and prediction mlp.
        out_dim:
            Dimension of the output (after the projection/prediction mlp).
        m:
            Momentum for the momentum update of encoder.
    """

    def __init__(
        self,
        backbone: nn.Module,
        num_ftrs: int = 2048,
        hidden_dim: int = 4096,
        out_dim: int = 256,
        m: float = 0.9,
    ):
        super(BYOL, self).__init__()

        self.backbone = backbone
        # the architecture of the projection and prediction head is the same
        self.projection_head = BYOLProjectionHead(num_ftrs, hidden_dim, out_dim)
        self.prediction_head = BYOLProjectionHead(out_dim, hidden_dim, out_dim)
        self.momentum_backbone = None
        self.momentum_projection_head = None

        self._init_momentum_encoder()
        self.m = m

        warnings.warn(
            Warning(
                "The high-level building block BYOL will be deprecated in version 1.3.0. "
                + "Use low-level building blocks instead. "
                + "See https://docs.lightly.ai/self-supervised-learning/lightly.models.html for more information"
            ),
            DeprecationWarning,
        )

    def _forward(self, x0: torch.Tensor, x1: torch.Tensor = None):
        """Forward pass through the encoder and the momentum encoder.

        Performs the momentum update, extracts features with the backbone and
        applies the projection (and prediciton) head to the output space. If
        x1 is None, only x0 will be processed otherwise, x0 is processed with
        the encoder and x1 with the momentum encoder.

        Args:
            x0:
                Tensor of shape bsz x channels x W x H.
            x1:
                Tensor of shape bsz x channels x W x H.

        Returns:
            The output proejction of x0 and (if x1 is not None) the output
            projection of x1.

        Examples:
            >>> # single input, single output
            >>> out = model._forward(x)
            >>>
            >>> # two inputs, two outputs
            >>> out0, out1 = model._forward(x0, x1)

        """

        self._momentum_update(self.m)

        # forward pass of first input x0
        f0 = self.backbone(x0).flatten(start_dim=1)
        z0 = self.projection_head(f0)
        out0 = self.prediction_head(z0)

        if x1 is None:
            return out0

        # forward pass of second input x1
        with torch.no_grad():
            f1 = self.momentum_backbone(x1).flatten(start_dim=1)
            out1 = self.momentum_projection_head(f1)

        return out0, out1

    def forward(
        self, x0: torch.Tensor, x1: torch.Tensor, return_features: bool = False
    ):
        """Symmetrizes the forward pass (see _forward).

        Performs two forward passes, once where x0 is passed through the encoder
        and x1 through the momentum encoder and once the other way around.

        Note that this model currently requires two inputs for the forward pass
        (x0 and x1) which correspond to the two augmentations.
        Furthermore, `the return_features` argument does not work yet.

        Args:
            x0:
                Tensor of shape bsz x channels x W x H.
            x1:
                Tensor of shape bsz x channels x W x H.

        Returns:
            A tuple out0, out1, where out0 and out1 are tuples containing the
            predictions and projections of x0 and x1: out0 = (z0, p0) and
            out1 = (z1, p1).

        Examples:
            >>> # initialize the model and the loss function
            >>> model = BYOL()
            >>> criterion = SymNegCosineSimilarityLoss()
            >>>
            >>> # forward pass for two batches of transformed images x1 and x2
            >>> out0, out1 = model(x0, x1)
            >>> loss = criterion(out0, out1)

        """

        if x0 is None:
            raise ValueError("x0 must not be None!")
        if x1 is None:
            raise ValueError("x1 must not be None!")

        if not all([s0 == s1 for s0, s1 in zip(x0.shape, x1.shape)]):
            raise ValueError(
                f"x0 and x1 must have same shape but got shapes {x0.shape} and {x1.shape}!"
            )

        p0, z1 = self._forward(x0, x1)
        p1, z0 = self._forward(x1, x0)

        return (z0, p0), (z1, p1)



================================================
FILE: lightly/models/moco.py
================================================
""" MoCo Model """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

import warnings

import torch
import torch.nn as nn

from lightly.models._momentum import _MomentumEncoderMixin
from lightly.models.modules import MoCoProjectionHead


class MoCo(nn.Module, _MomentumEncoderMixin):
    """Implementation of the MoCo (Momentum Contrast)[0] architecture.

    Recommended loss: :py:class:`lightly.loss.ntx_ent_loss.NTXentLoss` with
    a memory bank.

    [0] MoCo, 2020, https://arxiv.org/abs/1911.05722

    Attributes:
        backbone:
            Backbone model to extract features from images.
        num_ftrs:
            Dimension of the embedding (before the projection head).
        out_dim:
            Dimension of the output (after the projection head).
        m:
            Momentum for momentum update of the key-encoder.

    """

    def __init__(
        self,
        backbone: nn.Module,
        num_ftrs: int = 32,
        out_dim: int = 128,
        m: float = 0.999,
        batch_shuffle: bool = False,
    ):
        super(MoCo, self).__init__()

        self.backbone = backbone
        self.projection_head = MoCoProjectionHead(num_ftrs, num_ftrs, out_dim)
        self.momentum_features = None
        self.momentum_projection_head = None

        self.m = m
        self.batch_shuffle = batch_shuffle

        # initialize momentum features and momentum projection head
        self._init_momentum_encoder()

        warnings.warn(
            Warning(
                "The high-level building block MoCo will be deprecated in version 1.3.0. "
                + "Use low-level building blocks instead. "
                + "See https://docs.lightly.ai/self-supervised-learning/lightly.models.html for more information"
            ),
            DeprecationWarning,
        )

    def forward(
        self, x0: torch.Tensor, x1: torch.Tensor = None, return_features: bool = False
    ):
        """Embeds and projects the input image.

        Performs the momentum update, extracts features with the backbone and
        applies the projection head to the output space. If both x0 and x1 are
        not None, both will be passed through the backbone and projection head.
        If x1 is None, only x0 will be forwarded.

        Args:
            x0:
                Tensor of shape bsz x channels x W x H.
            x1:
                Tensor of shape bsz x channels x W x H.
            return_features:
                Whether or not to return the intermediate features backbone(x).

        Returns:
            The output projection of x0 and (if x1 is not None) the output
            projection of x1. If return_features is True, the output for each x
            is a tuple (out, f) where f are the features before the projection
            head.

        Examples:
            >>> # single input, single output
            >>> out = model(x)
            >>>
            >>> # single input with return_features=True
            >>> out, f = model(x, return_features=True)
            >>>
            >>> # two inputs, two outputs
            >>> out0, out1 = model(x0, x1)
            >>>
            >>> # two inputs, two outputs with return_features=True
            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)

        """
        self._momentum_update(self.m)

        # forward pass of first input x0
        f0 = self.backbone(x0).flatten(start_dim=1)
        out0 = self.projection_head(f0)

        # append features if requested
        if return_features:
            out0 = (out0, f0)

        # return out0 if x1 is None
        if x1 is None:
            return out0

        # forward pass of second input x1
        with torch.no_grad():
            # shuffle for batchnorm
            if self.batch_shuffle:
                x1, shuffle = self._batch_shuffle(x1)

            # run x1 through momentum encoder
            f1 = self.momentum_backbone(x1).flatten(start_dim=1)
            out1 = self.momentum_projection_head(f1).detach()

            # unshuffle for batchnorm
            if self.batch_shuffle:
                f1 = self._batch_unshuffle(f1, shuffle)
                out1 = self._batch_unshuffle(out1, shuffle)

            # append features if requested
            if return_features:
                out1 = (out1, f1)

        return out0, out1



================================================
FILE: lightly/models/nnclr.py
================================================
""" NNCLR Model """

# Copyright (c) 2021. Lightly AG and its affiliates.
# All Rights Reserved

import warnings

import torch
import torch.nn as nn

from lightly.models.modules import NNCLRPredictionHead, NNCLRProjectionHead


def _prediction_mlp(in_dims: int, h_dims: int, out_dims: int) -> nn.Sequential:
    """Prediction MLP. The original paper's implementation has 2 layers, with
    BN applied to its hidden fc layers but no BN or ReLU on the output fc layer.

    Note that the hidden dimensions should be smaller than the input/output
    dimensions (bottleneck structure). The default implementation using a
    ResNet50 backbone has an input dimension of 2048, hidden dimension of 512,
    and output dimension of 2048

    Args:
        in_dims:
            Input dimension of the first linear layer.
        h_dims:
            Hidden dimension of all the fully connected layers (should be a
            bottleneck!)
        out_dims:
            Output Dimension of the final linear layer.

    Returns:
        nn.Sequential:
            The projection head.
    """
    l1 = nn.Sequential(
        nn.Linear(in_dims, h_dims), nn.BatchNorm1d(h_dims), nn.ReLU(inplace=True)
    )

    l2 = nn.Linear(h_dims, out_dims)

    prediction = nn.Sequential(l1, l2)
    return prediction


def _projection_mlp(
    num_ftrs: int, h_dims: int, out_dim: int, num_layers: int = 3
) -> nn.Sequential:
    """Projection MLP. The original paper's implementation has 3 layers, with
    BN applied to its hidden fc layers but no ReLU on the output fc layer.
    The CIFAR-10 study used a MLP with only two layers.

    Args:
        in_dims:
            Input dimension of the first linear layer.
        h_dims:
            Hidden dimension of all the fully connected layers.
        out_dims:
            Output Dimension of the final linear layer.
        num_layers:
            Controls the number of layers; must be 2 or 3. Defaults to 3.

    Returns:
        nn.Sequential:
            The projection head.
    """
    l1 = nn.Sequential(
        nn.Linear(num_ftrs, h_dims), nn.BatchNorm1d(h_dims), nn.ReLU(inplace=True)
    )

    l2 = nn.Sequential(
        nn.Linear(h_dims, h_dims), nn.BatchNorm1d(h_dims), nn.ReLU(inplace=True)
    )

    l3 = nn.Sequential(nn.Linear(h_dims, out_dim), nn.BatchNorm1d(out_dim))

    if num_layers == 3:
        projection = nn.Sequential(l1, l2, l3)
    elif num_layers == 2:
        projection = nn.Sequential(l1, l3)
    else:
        raise NotImplementedError("Only MLPs with 2 and 3 layers are implemented.")

    return projection


class NNCLR(nn.Module):
    """Implementation of the NNCLR[0] architecture

    Recommended loss: :py:class:`lightly.loss.ntx_ent_loss.NTXentLoss`
    Recommended module: :py:class:`lightly.models.modules.nn_memory_bank.NNmemoryBankModule`

    [0] NNCLR, 2021, https://arxiv.org/abs/2104.14548

    Attributes:
        backbone:
            Backbone model to extract features from images.
        num_ftrs:
            Dimension of the embedding (before the projection head).
        proj_hidden_dim:
            Dimension of the hidden layer of the projection head.
        pred_hidden_dim:
            Dimension of the hidden layer of the predicion head.
        out_dim:
            Dimension of the output (after the projection head).
        num_mlp_layers:
            Number of linear layers for MLP.

    Examples:
        >>> model = NNCLR(backbone)
        >>> criterion = NTXentLoss(temperature=0.1)
        >>>
        >>> nn_replacer = NNmemoryBankModule(size=2 ** 16)
        >>>
        >>> # forward pass
        >>> (z0, p0), (z1, p1) = model(x0, x1)
        >>> z0 = nn_replacer(z0.detach(), update=False)
        >>> z1 = nn_replacer(z1.detach(), update=True)
        >>>
        >>> loss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))

    """

    def __init__(
        self,
        backbone: nn.Module,
        num_ftrs: int = 512,
        proj_hidden_dim: int = 2048,
        pred_hidden_dim: int = 4096,
        out_dim: int = 256,
    ):
        super(NNCLR, self).__init__()

        self.backbone = backbone
        self.num_ftrs = num_ftrs
        self.proj_hidden_dim = proj_hidden_dim
        self.pred_hidden_dim = pred_hidden_dim
        self.out_dim = out_dim

        self.projection_mlp = NNCLRProjectionHead(
            num_ftrs,
            proj_hidden_dim,
            out_dim,
        )

        self.prediction_mlp = NNCLRPredictionHead(
            out_dim,
            pred_hidden_dim,
            out_dim,
        )

        warnings.warn(
            Warning(
                "The high-level building block NNCLR will be deprecated in version 1.3.0. "
                + "Use low-level building blocks instead. "
                + "See https://docs.lightly.ai/self-supervised-learning/lightly.models.html for more information"
            ),
            DeprecationWarning,
        )

    def forward(
        self, x0: torch.Tensor, x1: torch.Tensor = None, return_features: bool = False
    ):
        """Embeds and projects the input images.

        Extracts features with the backbone and applies the projection
        head to the output space. If both x0 and x1 are not None, both will be
        passed through the backbone and projection head. If x1 is None, only
        x0 will be forwarded.

        Args:
            x0:
                Tensor of shape bsz x channels x W x H.
            x1:
                Tensor of shape bsz x channels x W x H.
            return_features:
                Whether or not to return the intermediate features backbone(x).

        Returns:
            The output projection of x0 and (if x1 is not None) the output
            projection of x1. If return_features is True, the output for each x
            is a tuple (out, f) where f are the features before the projection
            head.

        Examples:
            >>> # single input, single output
            >>> out = model(x)
            >>>
            >>> # single input with return_features=True
            >>> out, f = model(x, return_features=True)
            >>>
            >>> # two inputs, two outputs
            >>> out0, out1 = model(x0, x1)
            >>>
            >>> # two inputs, two outputs with return_features=True
            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)

        """

        # forward pass of first input x0
        f0 = self.backbone(x0).flatten(start_dim=1)
        z0 = self.projection_mlp(f0)
        p0 = self.prediction_mlp(z0)

        out0 = (z0, p0)

        # append features if requested
        if return_features:
            out0 = (out0, f0)

        # return out0 if x1 is None
        if x1 is None:
            return out0

        # forward pass of second input x1
        f1 = self.backbone(x1).flatten(start_dim=1)
        z1 = self.projection_mlp(f1)
        p1 = self.prediction_mlp(z1)

        out1 = (z1, p1)

        # append features if requested
        if return_features:
            out1 = (out1, f1)

        # return both outputs
        return out0, out1



================================================
FILE: lightly/models/resnet.py
================================================
"""Custom ResNet Implementation

Note that the architecture we present here differs from the one used in
torchvision. We replace the first 7x7 convolution by a 3x3 convolution to make
the model faster and run better on smaller input image resolutions.

Furthermore, we introduce a resnet-9 variant for extra small models. These can 
run for example on a microcontroller with 100kBytes of storage.
"""

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved
from __future__ import annotations

from typing import List

import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor

from lightly.models.batchnorm import get_norm_layer


class BasicBlock(nn.Module):
    """Implementation of the ResNet Basic Block.

    Attributes:
       in_planes:
           Number of input channels.
       planes:
           Number of channels.
       stride:
           Stride of the first convolutional.
    """

    expansion = 1

    def __init__(
        self, in_planes: int, planes: int, stride: int = 1, num_splits: int = 0
    ):
        super(BasicBlock, self).__init__()

        self.conv1 = nn.Conv2d(
            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False
        )
        self.bn1 = get_norm_layer(planes, num_splits)

        self.conv2 = nn.Conv2d(
            planes, planes, kernel_size=3, stride=1, padding=1, bias=False
        )
        self.bn2 = get_norm_layer(planes, num_splits)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(
                    in_planes,
                    self.expansion * planes,
                    kernel_size=1,
                    stride=stride,
                    bias=False,
                ),
                get_norm_layer(self.expansion * planes, num_splits),
            )

    def forward(self, x: Tensor) -> Tensor:
        """Forward pass through basic ResNet block.

        Args:
            x:
                Tensor of shape bsz x channels x W x H

        Returns:
            Tensor of shape bsz x channels x W x H
        """

        out: Tensor = self.conv1(x)
        out = self.bn1(out)
        out = F.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        out += self.shortcut(x)
        out = F.relu(out)

        return out


class Bottleneck(nn.Module):
    """Implementation of the ResNet Bottleneck Block.

    Attributes:
        in_planes:
            Number of input channels.
        planes:
            Number of channels.
        stride:
            Stride of the first convolutional.

    """

    expansion = 4

    def __init__(
        self, in_planes: int, planes: int, stride: int = 1, num_splits: int = 0
    ):
        super(Bottleneck, self).__init__()

        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)
        self.bn1 = get_norm_layer(planes, num_splits)

        self.conv2 = nn.Conv2d(
            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False
        )
        self.bn2 = get_norm_layer(planes, num_splits)

        self.conv3 = nn.Conv2d(
            planes, self.expansion * planes, kernel_size=1, bias=False
        )
        self.bn3 = get_norm_layer(self.expansion * planes, num_splits)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(
                    in_planes,
                    self.expansion * planes,
                    kernel_size=1,
                    stride=stride,
                    bias=False,
                ),
                get_norm_layer(self.expansion * planes, num_splits),
            )

    def forward(self, x: Tensor) -> Tensor:
        """Forward pass through bottleneck ResNet block.

        Args:
            x:
                Tensor of shape bsz x channels x W x H

        Returns:
            Tensor of shape bsz x channels x W x H
        """

        out: Tensor = self.conv1(x)
        out = self.bn1(out)
        out = F.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = F.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        out += self.shortcut(x)
        out = F.relu(out)

        return out


class ResNet(nn.Module):
    """ResNet implementation.

    [1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun
    Deep Residual Learning for Image Recognition. arXiv:1512.03385

    Attributes:
        block:
            ResNet building block type.
        layers:
            List of blocks per layer.
        num_classes:
            Number of classes in final softmax layer.
        width:
            Multiplier for ResNet width.
    """

    def __init__(
        self,
        block: type[BasicBlock] = BasicBlock,
        layers: List[int] = [2, 2, 2, 2],
        num_classes: int = 10,
        width: float = 1.0,
        num_splits: int = 0,
    ):
        super(ResNet, self).__init__()
        self.in_planes = int(64 * width)

        self.base = int(64 * width)

        self.conv1 = nn.Conv2d(
            3, self.base, kernel_size=3, stride=1, padding=1, bias=False
        )
        self.bn1 = get_norm_layer(self.base, num_splits)
        self.layer1 = self._make_layer(
            block, self.base, layers[0], stride=1, num_splits=num_splits
        )
        self.layer2 = self._make_layer(
            block, self.base * 2, layers[1], stride=2, num_splits=num_splits
        )
        self.layer3 = self._make_layer(
            block, self.base * 4, layers[2], stride=2, num_splits=num_splits
        )
        self.layer4 = self._make_layer(
            block, self.base * 8, layers[3], stride=2, num_splits=num_splits
        )
        self.linear = nn.Linear(self.base * 8 * block.expansion, num_classes)

    def _make_layer(
        self,
        block: type[BasicBlock],
        planes: int,
        num_layers: int,
        stride: int,
        num_splits: int,
    ) -> nn.Sequential:
        strides = [stride] + [1] * (num_layers - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride, num_splits))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)

    def forward(self, x: Tensor) -> Tensor:
        """Forward pass through ResNet.

        Args:
            x:
                Tensor of shape bsz x channels x W x H

        Returns:
            Output tensor of shape bsz x num_classes

        """
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.avg_pool2d(out, 4)
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out


def ResNetGenerator(
    name: str = "resnet-18",
    width: float = 1,
    num_classes: int = 10,
    num_splits: int = 0,
) -> ResNet:
    """Builds and returns the specified ResNet.

    Args:
        name:
            ResNet version from resnet-{9, 18, 34, 50, 101, 152}.
        width:
            ResNet width.
        num_classes:
            Output dim of the last layer.
        num_splits:
            Number of splits to use for SplitBatchNorm (for MoCo model).
            Increase this number to simulate multi-gpu behavior.
            E.g. `num_splits=8` simulates a 8-GPU cluster.
            `num_splits=0` uses normal PyTorch BatchNorm.

    Returns:
        ResNet as nn.Module.

    Examples:
        >>> # binary classifier with ResNet-34
        >>> from lightly.models import ResNetGenerator
        >>> resnet = ResNetGenerator('resnet-34', num_classes=2)

    """

    model_params = {
        "resnet-9": {"block": BasicBlock, "layers": [1, 1, 1, 1]},
        "resnet-18": {"block": BasicBlock, "layers": [2, 2, 2, 2]},
        "resnet-34": {"block": BasicBlock, "layers": [3, 4, 6, 3]},
        "resnet-50": {"block": Bottleneck, "layers": [3, 4, 6, 3]},
        "resnet-101": {"block": Bottleneck, "layers": [3, 4, 23, 3]},
        "resnet-152": {"block": Bottleneck, "layers": [3, 8, 36, 3]},
    }

    if name not in model_params.keys():
        raise ValueError(
            "Illegal name: {%s}. \
        Try resnet-9, resnet-18, resnet-34, resnet-50, resnet-101, resnet-152."
            % (name)
        )

    return ResNet(
        **model_params[name],  # type: ignore # Cannot unpack dict to type "ResNet".
        width=width,
        num_classes=num_classes,
        num_splits=num_splits,
    )



================================================
FILE: lightly/models/simclr.py
================================================
""" SimCLR Model """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

import warnings

import torch
import torch.nn as nn

from lightly.models.modules import SimCLRProjectionHead


class SimCLR(nn.Module):
    """Implementation of the SimCLR[0] architecture

    Recommended loss: :py:class:`lightly.loss.ntx_ent_loss.NTXentLoss`

    [0] SimCLR, 2020, https://arxiv.org/abs/2002.05709

    Attributes:
        backbone:
            Backbone model to extract features from images.
        num_ftrs:
            Dimension of the embedding (before the projection head).
        out_dim:
            Dimension of the output (after the projection head).

    """

    def __init__(self, backbone: nn.Module, num_ftrs: int = 32, out_dim: int = 128):
        super(SimCLR, self).__init__()

        self.backbone = backbone
        self.projection_head = SimCLRProjectionHead(
            num_ftrs, num_ftrs, out_dim, batch_norm=False
        )

        warnings.warn(
            Warning(
                "The high-level building block SimCLR will be deprecated in version 1.3.0. "
                + "Use low-level building blocks instead. "
                + "See https://docs.lightly.ai/self-supervised-learning/lightly.models.html for more information"
            ),
            DeprecationWarning,
        )

    def forward(
        self, x0: torch.Tensor, x1: torch.Tensor = None, return_features: bool = False
    ):
        """Embeds and projects the input images.

        Extracts features with the backbone and applies the projection
        head to the output space. If both x0 and x1 are not None, both will be
        passed through the backbone and projection head. If x1 is None, only
        x0 will be forwarded.

        Args:
            x0:
                Tensor of shape bsz x channels x W x H.
            x1:
                Tensor of shape bsz x channels x W x H.
            return_features:
                Whether or not to return the intermediate features backbone(x).

        Returns:
            The output projection of x0 and (if x1 is not None) the output
            projection of x1. If return_features is True, the output for each x
            is a tuple (out, f) where f are the features before the projection
            head.

        Examples:
            >>> # single input, single output
            >>> out = model(x)
            >>>
            >>> # single input with return_features=True
            >>> out, f = model(x, return_features=True)
            >>>
            >>> # two inputs, two outputs
            >>> out0, out1 = model(x0, x1)
            >>>
            >>> # two inputs, two outputs with return_features=True
            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)

        """

        # forward pass of first input x0
        f0 = self.backbone(x0).flatten(start_dim=1)
        out0 = self.projection_head(f0)

        # append features if requested
        if return_features:
            out0 = (out0, f0)

        # return out0 if x1 is None
        if x1 is None:
            return out0

        # forward pass of second input x1
        f1 = self.backbone(x1).flatten(start_dim=1)
        out1 = self.projection_head(f1)

        # append features if requested
        if return_features:
            out1 = (out1, f1)

        # return both outputs
        return out0, out1



================================================
FILE: lightly/models/simsiam.py
================================================
""" SimSiam Model """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

import warnings

import torch
import torch.nn as nn

from lightly.models.modules import SimSiamPredictionHead, SimSiamProjectionHead


class SimSiam(nn.Module):
    """Implementation of SimSiam[0] network

    Recommended loss: :py:class:`lightly.loss.sym_neg_cos_sim_loss.SymNegCosineSimilarityLoss`

    [0] SimSiam, 2020, https://arxiv.org/abs/2011.10566

    Attributes:
        backbone:
            Backbone model to extract features from images.
        num_ftrs:
            Dimension of the embedding (before the projection head).
        proj_hidden_dim:
            Dimension of the hidden layer of the projection head. This should
            be the same size as `num_ftrs`.
        pred_hidden_dim:
            Dimension of the hidden layer of the predicion head. This should
            be `num_ftrs` / 4.
        out_dim:
            Dimension of the output (after the projection head).

    """

    def __init__(
        self,
        backbone: nn.Module,
        num_ftrs: int = 2048,
        proj_hidden_dim: int = 2048,
        pred_hidden_dim: int = 512,
        out_dim: int = 2048,
    ):
        super(SimSiam, self).__init__()

        self.backbone = backbone
        self.num_ftrs = num_ftrs
        self.proj_hidden_dim = proj_hidden_dim
        self.pred_hidden_dim = pred_hidden_dim
        self.out_dim = out_dim

        self.projection_mlp = SimSiamProjectionHead(
            num_ftrs,
            proj_hidden_dim,
            out_dim,
        )

        self.prediction_mlp = SimSiamPredictionHead(
            out_dim,
            pred_hidden_dim,
            out_dim,
        )

        warnings.warn(
            Warning(
                "The high-level building block SimSiam will be deprecated in version 1.3.0. "
                + "Use low-level building blocks instead. "
                + "See https://docs.lightly.ai/self-supervised-learning/lightly.models.html for more information"
            ),
            DeprecationWarning,
        )

    def forward(
        self, x0: torch.Tensor, x1: torch.Tensor = None, return_features: bool = False
    ):
        """Forward pass through SimSiam.

        Extracts features with the backbone and applies the projection
        head and prediction head to the output space. If both x0 and x1 are not
        None, both will be passed through the backbone, projection, and
        prediction head. If x1 is None, only x0 will be forwarded.

        Args:
            x0:
                Tensor of shape bsz x channels x W x H.
            x1:
                Tensor of shape bsz x channels x W x H.
            return_features:
                Whether or not to return the intermediate features backbone(x).

        Returns:
            The output prediction and projection of x0 and (if x1 is not None)
            the output prediction and projection of x1. If return_features is
            True, the output for each x is a tuple (out, f) where f are the
            features before the projection head.

        Examples:
            >>> # single input, single output
            >>> out = model(x)
            >>>
            >>> # single input with return_features=True
            >>> out, f = model(x, return_features=True)
            >>>
            >>> # two inputs, two outputs
            >>> out0, out1 = model(x0, x1)
            >>>
            >>> # two inputs, two outputs with return_features=True
            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)
        """
        f0 = self.backbone(x0).flatten(start_dim=1)
        z0 = self.projection_mlp(f0)
        p0 = self.prediction_mlp(z0)

        out0 = (z0, p0)

        # append features if requested
        if return_features:
            out0 = (out0, f0)

        if x1 is None:
            return out0

        f1 = self.backbone(x1).flatten(start_dim=1)
        z1 = self.projection_mlp(f1)
        p1 = self.prediction_mlp(z1)

        out1 = (z1, p1)

        # append features if requested
        if return_features:
            out1 = (out1, f1)

        return out0, out1



================================================
FILE: lightly/models/utils.py
================================================
""" Utils for working with SSL models """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved
from __future__ import annotations

import math
import random
import warnings
from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Tuple, Type, Union

import numpy as np
import torch
import torch.distributed as dist
import torch.nn as nn
from torch import Tensor
from torch.nn import Identity, Module, Sequential, functional, init
from torch.nn.modules import CrossMapLRN2d, GroupNorm, LayerNorm, LocalResponseNorm
from torch.nn.modules.batchnorm import _NormBase
from torch.nn.parameter import Parameter
from torchvision.ops import StochasticDepth

from lightly.utils import dependency

if TYPE_CHECKING:
    from numpy.typing import NDArray
    from timm.models.vision_transformer import VisionTransformer


def pool_masked(
    source: Tensor, mask: Tensor, num_cls: int, reduce: str = "mean"
) -> Tensor:
    """Reduce image feature maps :math:`(B, C, H, W)` or :math:`(C, H, W)` according to
    an integer index given by `mask` :math:`(B, H, W)` or :math:`(H, W)`.

    Args:
        source: Float tensor of shape :math:`(B, C, H, W)` or :math:`(C, H, W)` to be
            reduced.
        mask: Integer tensor of shape :math:`(B, H, W)` or :math:`(H, W)` containing the
            integer indices.
        num_cls: The number of classes in the possible masks.

    Returns:
        A tensor of shape :math:`(B, C, num_cls)` or :math:`(C, num_cls)`.
    """
    if source.dim() == 3:
        return _mask_reduce(source, mask, reduce, num_cls)
    elif source.dim() == 4:
        return _mask_reduce_batched(source, mask, num_cls)
    else:
        raise ValueError("source must have 3 or 4 dimensions")


def _mask_reduce(
    source: Tensor, mask: Tensor, num_cls: int, reduce: str = "mean"
) -> Tensor:
    output = _mask_reduce_batched(
        source.unsqueeze(0), mask.unsqueeze(0), num_cls=num_cls, reduce=reduce
    )
    return output.squeeze(0)


def _mask_reduce_batched(
    source: Tensor, mask: Tensor, num_cls: int, reduce: str = "mean"
) -> Tensor:
    b, c, h, w = source.shape
    cls = torch.arange(num_cls, device=mask.device)
    num_cls = cls.size(0)
    # create output tensor
    output = source.new_zeros((b, c, num_cls))  # (B C N)
    mask = mask.unsqueeze(1).expand(-1, c, -1, -1).view(b, c, -1)  # (B C HW)
    source = source.view(b, c, -1)  # (B C HW)
    output.scatter_reduce_(
        dim=2, index=mask, src=source, reduce=reduce, include_self=False
    )  # (B C N)
    # scatter_reduce_ produces NaNs if the count is zero
    output = torch.nan_to_num(output, nan=0.0)
    return output


@torch.no_grad()
def batch_shuffle(
    batch: torch.Tensor, distributed: bool = False
) -> Tuple[torch.Tensor, torch.Tensor]:
    """Randomly shuffles all tensors in the batch.

    Args:
        batch:
            The batch to shuffle.
        distributed:
            If True then batches are shuffled across multiple gpus.

    Returns:
        A (batch, shuffle) tuple where batch is the shuffled version of the
        input batch and shuffle is an index to restore the original order.

    Examples:
        >>> # forward pass through the momentum model with batch shuffling
        >>> x1_shuffled, shuffle = batch_shuffle(x1)
        >>> f1 = moco_momentum(x1)
        >>> out0 = projection_head_momentum(f0)
        >>> out1 = batch_unshuffle(out1, shuffle)
    """
    if distributed:
        return batch_shuffle_distributed(batch)
    batch_size = batch.shape[0]
    shuffle = torch.randperm(batch_size, device=batch.device)
    return batch[shuffle], shuffle


@torch.no_grad()
def batch_unshuffle(
    batch: torch.Tensor,
    shuffle: torch.Tensor,
    distributed: bool = False,
) -> torch.Tensor:
    """Unshuffles a batch.

    Args:
        batch:
            The batch to unshuffle.
        shuffle:
            Index to unshuffle the batch.
        distributed:
            If True then the batch is unshuffled across multiple gpus.

    Returns:
        The unshuffled batch.

    Examples:
        >>> # forward pass through the momentum model with batch shuffling
        >>> x1_shuffled, shuffle = batch_shuffle(x1)
        >>> f1 = moco_momentum(x1)
        >>> out0 = projection_head_momentum(f0)
        >>> out1 = batch_unshuffle(out1, shuffle)
    """
    if distributed:
        return batch_unshuffle_distributed(batch, shuffle)
    unshuffle = torch.argsort(shuffle)
    return batch[unshuffle]


@torch.no_grad()
def concat_all_gather(x: torch.Tensor) -> torch.Tensor:
    """Returns concatenated instances of x gathered from all gpus.

    This code was taken and adapted from here:
    https://github.com/facebookresearch/moco.

    """
    output = [torch.empty_like(x) for _ in range(dist.get_world_size())]
    dist.all_gather(output, x, async_op=False)
    output = torch.cat(output, dim=0)
    return output


@torch.no_grad()
def batch_shuffle_distributed(batch: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
    """Shuffles batch over multiple devices.

    This code was taken and adapted from here:
    https://github.com/facebookresearch/moco.

    Args:
        batch:
            The tensor to shuffle.

    Returns:
        A (batch, shuffle) tuple where batch is the shuffled version of the
        input batch and shuffle is an index to restore the original order.

    """
    # gather from all devices
    batch_size_this = batch.shape[0]
    batch_gather = concat_all_gather(batch)
    batch_size_all = batch_gather.shape[0]

    # Calculate the number of devices
    num_devices = batch_size_all // batch_size_this

    # random shuffle index
    idx_shuffle = torch.randperm(batch_size_all, device=batch.device)

    # broadcast to all devices
    dist.broadcast(idx_shuffle, src=0)

    # index for restoring
    shuffle = torch.argsort(idx_shuffle)

    # shuffled index for this device
    rank = dist.get_rank()
    idx_this = idx_shuffle.view(num_devices, -1)[rank]
    return batch_gather[idx_this], shuffle


@torch.no_grad()
def batch_unshuffle_distributed(
    batch: torch.Tensor, shuffle: torch.Tensor
) -> torch.Tensor:
    """Undo batch shuffle over multiple devices.

    This code was taken and adapted from here:
    https://github.com/facebookresearch/moco.

    Args:
        batch:
            The tensor to unshuffle.
        shuffle:
            Index to restore the original tensor.

    Returns:
        The unshuffled tensor.

    """
    # gather from all devices
    batch_size_this = batch.shape[0]
    batch_gather = concat_all_gather(batch)
    batch_size_all = batch_gather.shape[0]

    # Calculate the number of devices
    num_devices = batch_size_all // batch_size_this

    # Get the rank of the current device
    rank = dist.get_rank()

    # Index for this device after unshuffling
    idx_this = shuffle.view(num_devices, -1)[rank]

    # Returns the unshuffled batch for this device
    return batch_gather[idx_this]


def deactivate_requires_grad(model: nn.Module):
    """Deactivates the requires_grad flag for all parameters of a model.

    This has the same effect as permanently executing the model within a `torch.no_grad()`
    context. Use this method to disable gradient computation and therefore
    training for a model.

    Examples:
        >>> backbone = resnet18()
        >>> deactivate_requires_grad(backbone)
    """
    for param in model.parameters():
        param.requires_grad = False


def activate_requires_grad(model: nn.Module):
    """Activates the requires_grad flag for all parameters of a model.

    Use this method to activate gradients for a model (e.g. after deactivating
    them using `deactivate_requires_grad(...)`).

    Examples:
        >>> backbone = resnet18()
        >>> activate_requires_grad(backbone)
    """
    for param in model.parameters():
        param.requires_grad = True


@torch.no_grad()
def update_momentum(model: nn.Module, model_ema: nn.Module, m: float):
    """Updates parameters of `model_ema` with Exponential Moving Average of `model`

    Momentum encoders are a crucial component for models such as MoCo or BYOL.

    Args:
        model:
            The current model.
        model_ema:
            The model with exponential moving average (EMA) parameters.
        m:
            The momentum factor, between 0 and 1.

    Examples:
        >>> backbone = resnet18()
        >>> projection_head = MoCoProjectionHead()
        >>> backbone_momentum = copy.deepcopy(moco)
        >>> projection_head_momentum = copy.deepcopy(projection_head)
        >>>
        >>> # update momentum
        >>> update_momentum(moco, moco_momentum, m=0.999)
        >>> update_momentum(projection_head, projection_head_momentum, m=0.999)
    """
    for model_ema, model in zip(model_ema.parameters(), model.parameters()):
        model_ema.data = model_ema.data * m + model.data * (1.0 - m)


@torch.no_grad()
def normalize_weight(weight: nn.Parameter, dim: int = 1, keepdim: bool = True):
    """Normalizes the weight to unit length along the specified dimension."""
    weight.div_(torch.norm(weight, dim=dim, keepdim=keepdim))


# copy paste from PyTorch master branch as it is not available in older releases
# source: https://github.com/pytorch/pytorch/blob/20ac7362009dd8e0aca6e72fc9357773136a83b8/torch/nn/init.py#L22-L54
def _no_grad_trunc_normal(
    tensor: torch.Tensor,
    mean: float,
    std: float,
    a: float,
    b: float,
) -> torch.Tensor:
    """Initializes the input tensor with a truncated normal distribution.

    This method is based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf

    Args:
        tensor:
            The tensor to initialize.
        mean:
            Mean of the distribution.
        std:
            Standard deviation of the distribution.
        a:
            Minimum value of the distribution, values below will be clamped.
        b:
            Maximum value of the distribution, values above will be clamped.

    """

    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
            "The distribution of values may be incorrect.",
            stacklevel=2,
        )

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def repeat_token(token: torch.Tensor, size: Tuple[int, int]) -> torch.Tensor:
    """Repeats a token size times.

    Args:
        token:
            Token tensor with shape (1, 1, dim).
        size:
            (batch_size, sequence_length) tuple.

    Returns:
        Tensor with shape (batch_size, sequence_length, dim) containing copies
        of the input token.

    """
    batch_size, sequence_length = size
    return token.repeat(batch_size, sequence_length, 1)


def expand_index_like(index: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:
    """Expands the index along the last dimension of the input tokens.

    Args:
        index:
            Index tensor with shape (batch_size, idx_length) where each entry is
            an index in [0, sequence_length).
        tokens:
            Tokens tensor with shape (batch_size, sequence_length, dim).

    Returns:
        Index tensor with shape (batch_size, idx_length, dim) where the original
        indices are repeated dim times along the last dimension.

    """
    dim = tokens.shape[-1]
    index = index.unsqueeze(-1).expand(-1, -1, dim)
    return index


def get_at_index(tokens: torch.Tensor, index: torch.Tensor) -> torch.Tensor:
    """Selects tokens at index.

    Args:
        tokens:
            Token tensor with shape (batch_size, sequence_length, dim).
        index:
            Index tensor with shape (batch_size, index_length) where each entry is
            an index in [0, sequence_length).

    Returns:
        Token tensor with shape (batch_size, index_length, dim) containing the
        selected tokens.

    """
    # Expand the index tensor to match the shape of tokens tensor
    index = expand_index_like(index, tokens)

    return torch.gather(tokens, 1, index)


def set_at_index(
    tokens: torch.Tensor, index: torch.Tensor, value: torch.Tensor
) -> torch.Tensor:
    """Copies all values into the input tensor at the given indices.

    Args:
        tokens:
            Tokens tensor with shape (batch_size, sequence_length, dim).
        index:
            Index tensor with shape (batch_size, index_length).
        value:
            Value tensor with shape (batch_size, index_length, dim).

    Returns:
        Tokens tensor with shape (batch_size, sequence_length, dim) containing
        the new values.

    """
    index = expand_index_like(index, tokens)
    return torch.scatter(tokens, 1, index, value)


def mask_at_index(
    tokens: torch.Tensor, index: torch.Tensor, mask_token: torch.Tensor
) -> torch.Tensor:
    """Returns a tensor where the tokens at the given indices are replaced by the
    mask token.

    Args:
        tokens:
            Tokens tensor with shape (batch_size, sequence_length, dim).
        index:
            Index tensor with shape (batch_size, index_length).
        mask_token:
            Value tensor with shape (1, 1, dim).

    Returns:
        Tokens tensor with shape (batch_size, sequence_length, dim) containing
        the new values.

    """
    mask = tokens.new_zeros(tokens.shape)
    mask = set_at_index(mask, index, 1)
    return (1 - mask) * tokens + mask * mask_token


def mask_bool(tokens: Tensor, mask: Tensor, mask_token: Tensor) -> Tensor:
    """Returns a tensor with tokens replaced by the mask tokens in all positions where
    the mask is True.

    Args:
        tokens:
            Tokens tensor with shape (batch_size, sequence_length, dim).
        mask:
            Boolean mask tensor with shape (batch_size, sequence_length).
        mask_token:
            Mask token with shape (1, 1, dim).

    Returns:
        Tokens tensor with shape (batch_size, sequence_length, dim) where tokens[i, j]
        is replaced by the mask token if mask[i, j] is True.
    """
    # Convert to int for multiplication.
    mask = mask.unsqueeze(-1).to(torch.bool).to(torch.int)
    return (1 - mask) * tokens + mask * mask_token


def prepend_class_token(
    tokens: torch.Tensor, class_token: torch.Tensor
) -> torch.Tensor:
    """Prepends class token to tokens.

    Args:
        tokens:
            Tokens tensor with shape (batch_size, sequence_length, dim).
        class_token:
            Class token with shape (1, 1, dim).

    Returns:
        Tokens tensor with the class token prepended at index 0 in every
        sequence. The tensor has shape (batch_size, sequence_length + 1, dim).
    """
    batch_size = tokens.shape[0]
    batch_class_token = class_token.expand(batch_size, -1, -1)
    return torch.cat([batch_class_token, tokens], dim=1)


def patchify(images: torch.Tensor, patch_size: int) -> torch.Tensor:
    """Converts a batch of input images into patches.

    Args:
        images:
            Images tensor with shape (batch_size, channels, height, width)
        patch_size:
            Patch size in pixels. Image width and height must be multiples of
            the patch size.

    Returns:
        Patches tensor with shape (batch_size, num_patches, channels * patch_size ** 2)
        where num_patches = image_width / patch_size * image_height / patch_size.

    """
    # N, C, H, W = (batch_size, channels, height, width)
    N, C, H, W = images.shape
    assert H == W and H % patch_size == 0

    patch_h = patch_w = H // patch_size
    num_patches = patch_h * patch_w

    # Reshape images to form patches
    patches = images.reshape(shape=(N, C, patch_h, patch_size, patch_w, patch_size))

    # Reorder dimensions for patches
    patches = torch.einsum("nchpwq->nhwpqc", patches)

    # Flatten patches
    patches = patches.reshape(shape=(N, num_patches, patch_size**2 * C))

    return patches


def unpatchify(
    patches: torch.Tensor, patch_size: int, channels: int = 3
) -> torch.Tensor:
    """
    Reconstructs images from their patches.

     Args:
         patches:
             Patches tensor with shape (batch_size, num_patches, channels * patch_size ** 2).
         patch_size:
             The patch size in pixels used to create the patches.
         channels:
             The number of channels the image must have

     Returns:
         Reconstructed images tensor with shape (batch_size, channels, height, width).
    """
    N, C = patches.shape[0], channels
    patch_h = patch_w = int(patches.shape[1] ** 0.5)
    assert patch_h * patch_w == patches.shape[1]

    images = patches.reshape(shape=(N, patch_h, patch_w, patch_size, patch_size, C))
    images = torch.einsum("nhwpqc->nchpwq", images)
    images = images.reshape(shape=(N, C, patch_h * patch_size, patch_h * patch_size))
    return images


def random_token_mask(
    size: Tuple[int, int],
    mask_ratio: float = 0.6,
    mask_class_token: bool = False,
    device: Optional[Union[torch.device, str]] = None,
) -> Tuple[Tensor, Tensor]:
    """Creates random token masks.

    Args:
        size:
            Size of the token batch for which to generate masks.
            Should be (batch_size, sequence_length).
        mask_ratio:
            Proportion of tokens to mask.
        mask_class_token:
            If False the class token is never masked. If True the class token
            might be masked.
        device:
            Device on which to create the index masks.

    Returns:
        A (index_keep, index_mask) tuple where each index is a tensor.
        index_keep contains the indices of the unmasked tokens and has shape
        (batch_size, num_keep). index_mask contains the indices of the masked
        tokens and has shape (batch_size, sequence_length - num_keep).
        num_keep is equal to sequence_length * (1 - mask_ratio).

    """
    batch_size, sequence_length = size
    # Remove 1 from the considered sequence length if the class token cannot be masked.
    # This only impacts the calculation of the number of tokens to keep.
    mask_sequence_length = sequence_length - int(not mask_class_token)
    num_keep = int(mask_sequence_length * (1 - mask_ratio))

    noise = torch.rand(batch_size, sequence_length, device=device)
    if not mask_class_token and sequence_length > 0:
        # Make sure that class token is not masked
        noise[:, 0] = -1
        num_keep = max(1, num_keep + 1)

    # Get indices of tokens to keep by sorting the noise
    indices = torch.argsort(noise, dim=1)
    idx_keep = indices[:, :num_keep]
    idx_mask = indices[:, num_keep:]

    return idx_keep, idx_mask


def random_prefix_mask(
    size: Tuple[int, int],
    max_prefix_length: int,
    device: Optional[Union[torch.device, str]] = None,
) -> torch.Tensor:
    """Creates a random prefix mask.

    The mask is created by uniformly sampling a prefix length in [0, max_prefix_length]
    for each sequence in the batch. All tokens with an index greater or equal to
    the prefix length are masked.

    Args:
        size:
            Size of the token batch for which to generate masks.
            Should be (batch_size, sequence_length).
        max_prefix_length:
            Maximum length of the prefix to mask.
        device:
            Device on which to create the mask.

    Returns:
        A mask tensor with shape (batch_size, sequence_length) where each entry
        is True if the token should be masked and False otherwise.

    """
    batch_size, sequence_length = size

    # Create an arange tensor and expand it to match batch size
    arange = torch.arange(sequence_length, device=device).expand(
        batch_size, sequence_length
    )

    # Generate random indices for the prefix length
    indices = torch.randint(0, max_prefix_length, (batch_size, 1), device=device)

    # Create the mask based on arange and indices
    mask = arange >= indices

    return mask


def random_block_mask(
    size: Tuple[int, int, int],
    batch_mask_ratio: float = 0.5,
    min_image_mask_ratio: float = 0.1,
    max_image_mask_ratio: float = 0.5,
    min_num_masks_per_block: int = 4,
    max_num_masks_per_block: Optional[int] = None,
    min_block_aspect_ratio: float = 0.3,
    max_block_aspect_ratio: Optional[float] = None,
    max_attempts_per_block: int = 10,
    device: Optional[Union[torch.device, str]] = None,
) -> Tensor:
    """Creates a random block mask for a batch of images.

    A block is in this context a rectangle of patches in an image that are
    masked together. The function generates block masks until the desired number of
    patches per image are masked. DINOv2 uses a more complex masking strategy that
    only generates masks for mask_ratio of the images. On top of that, it also masks
    a different number of patches for every image. This is controlled by the
    min_image_mask_ratio and max_image_mask_ratio arguments.

    Based on the implementation of the block mask in DINOv2 [0]. For details see [1]
    and [2].

    - [0]: DINOv2, 2023, https://arxiv.org/abs/2304.07193
    - [1]: https://github.com/facebookresearch/dinov2/blob/main/dinov2/data/masking.py
    - [2]: https://github.com/facebookresearch/dinov2/blob/main/dinov2/data/collate.py

    Args:
        size:
            Size of the image batch for which to generate masks.
            Should be (batch_size, height, width).
        batch_mask_ratio:
            Percentage of images per batch for which to generate block masks.
            The remaining images are not masked.
        min_image_mask_ratio:
            Minimum percentage of the image to mask. In practice, fewer than
            min_image_mask_ratio patches of the image can be masked due to additional
            constraints.
        max_image_mask_ratio:
            Maximum percentage of the image to mask.
        min_num_masks_per_block:
            Minimum number of patches to mask per block.
        max_num_masks_per_block:
            Maximum number of patches to mask per block.
        min_block_aspect_ratio:
            Minimum aspect ratio (height/width) of a masked block.
        max_block_aspect_ratio:
            Maximum aspect ratio (height/width) of a masked block.
        max_attempts_per_block:
            Maximum number of attempts to find a valid block mask for an image.
        device:
            Device on which to create the mask.
    Returns:
        A boolean tensor with shape (batch_size, height, width) where each entry
        is True if the patch should be masked and False otherwise.

    Raises:
        ValueError: If 'max_image_mask_ratio' is less than 'min_image_mask_ratio'.
    """

    if max_image_mask_ratio < min_image_mask_ratio:
        raise ValueError(
            "max_image_mask_ratio must be greater or equal to min_image_mask_ratio."
        )

    # B is batch size(number of images), H is height, W is width
    B, H, W = size
    num_images_masked = int(B * batch_mask_ratio)
    probs = torch.linspace(
        min_image_mask_ratio, max_image_mask_ratio, num_images_masked + 1
    ).tolist()
    image_masks = []
    for prob_min, prob_max in zip(probs[:-1], probs[1:]):
        num_mask = int(H * W * random.uniform(prob_min, prob_max))
        image_masks.append(
            random_block_mask_image(
                size=(H, W),
                num_masks=num_mask,
                min_num_masks_per_block=min_num_masks_per_block,
                max_num_masks_per_block=max_num_masks_per_block,
                min_block_aspect_ratio=min_block_aspect_ratio,
                max_block_aspect_ratio=max_block_aspect_ratio,
                max_attempts_per_block=max_attempts_per_block,
                device=device,
            )
        )

    # Add non-masked images to fill the batch
    for _ in range(num_images_masked, B):
        image_masks.append(torch.zeros((H, W), dtype=torch.bool, device=device))

    random.shuffle(image_masks)
    return torch.stack(image_masks)


def random_block_mask_image(
    size: Tuple[int, int],
    num_masks: int,
    min_num_masks_per_block: int = 4,
    max_num_masks_per_block: Optional[int] = None,
    min_block_aspect_ratio: float = 0.3,
    max_block_aspect_ratio: Optional[float] = None,
    max_attempts_per_block: int = 10,
    device: Optional[Union[torch.device, str]] = None,
) -> Tensor:
    """Creates a random block mask for a single image.

    Args:
        size:
            Size of the image for which to generate a mask.
            Should be (height, width).
        num_masks:
            Number of patches to mask.
        min_num_masks_per_block:
            Minimum number of patches to mask per block.
        max_num_masks_per_block:
            Maximum number of patches to mask per block.
        min_block_aspect_ratio:
            Minimum aspect ratio (height/width) of a masked block.
        max_block_aspect_ratio:
            Maximum aspect ratio (height/width) of a masked block.
        max_attempts_per_block:
            Maximum number of attempts to find a valid block mask.
        device:
            Device on which to create the mask.
    Returns:
        A boolean tensor with shape (height, width) where each entry is True if the
        patch should be masked and False otherwise.

    Raises:
        ValueError: If 'max_num_masks_per_block' is less than 'min_num_masks_per_block' or
            if 'max_block_aspect_ratio' is less than 'min_block_aspect_ratio'
    """

    if max_block_aspect_ratio is None:
        max_block_aspect_ratio = 1 / min_block_aspect_ratio
    if max_num_masks_per_block is None:
        max_num_masks_per_block = num_masks

    if max_num_masks_per_block < min_num_masks_per_block:
        raise ValueError(
            "max_num_masks_per_block must be greater or equal to min_num_masks_per_block."
        )
    if max_block_aspect_ratio < min_block_aspect_ratio:
        raise ValueError(
            "max_block_aspect_ratio must be greater or equal to min_block_aspect_ratio."
        )

    log_min_aspect = math.log(min_block_aspect_ratio)
    log_max_aspect = math.log(max_block_aspect_ratio)

    H, W = size
    mask = torch.zeros((H, W), dtype=torch.bool, device=device)
    mask_count = 0
    while mask_count < num_masks:
        # Try masking a block
        max_new_masked = min(num_masks - mask_count, max_num_masks_per_block)
        delta = 0
        for _ in range(max_attempts_per_block):
            target_area = random.uniform(min_num_masks_per_block, max_new_masked)
            aspect_ratio = math.exp(random.uniform(log_min_aspect, log_max_aspect))
            h = int(round(math.sqrt(target_area * aspect_ratio)))
            w = int(round(math.sqrt(target_area / aspect_ratio)))
            if w < W and h < H:
                top = random.randint(0, H - h)
                left = random.randint(0, W - w)
                num_already_masked = mask[top : top + h, left : left + w].sum().item()
                num_new_masked = h * w - num_already_masked
                if 0 < num_new_masked <= max_new_masked:
                    mask[top : top + h, left : left + w] = 1
                    delta += num_new_masked
            if delta > 0:
                break
        if delta == 0:
            break
        else:
            mask_count += delta
    return mask


def nearest_neighbors(
    input_maps: torch.Tensor,
    candidate_maps: torch.Tensor,
    distances: torch.Tensor,
    num_matches: int,
) -> Tuple[torch.Tensor, torch.Tensor]:
    """Finds the nearest neighbors of the maps in input_maps in candidate_maps.

    Args:
        input_maps:
            A tensor of maps for which to find nearest neighbors.
            It has shape: [batch_size, input_map_size, feature_dimension]
        candidate_maps:
            A tensor of maps to search for nearest neighbors.
            It has shape: [batch_size, candidate_map_size, feature_dimension]
        distances:
            A tensor of distances between the maps in input_maps and candidate_maps.
            It has shape: [batch_size, input_map_size, candidate_map_size]
        num_matches:
            Number of nearest neighbors to return. If num_matches is None or -1,
            all the maps in candidate_maps are considered.

    Returns:
        A tuple of tensors, containing the nearest neighbors in input_maps and candidate_maps.
        They both have shape: [batch_size, input_map_size, feature_dimension]
    """

    if num_matches is None or num_matches == -1 or num_matches > input_maps.size(1):
        num_matches = input_maps.size(1)

    # Find nearest neighbour of each input element in the candidate map
    topk_values, topk_indices = distances.topk(
        k=1, dim=2, largest=False
    )  # [bsz, input_map_size, 1]
    topk_values = topk_values.squeeze(-1)  # [bsz, input_map_size]

    # Select num_matches neighbors pairs having the lowest distance value.
    _, min_indices = topk_values.topk(
        k=num_matches, dim=1, largest=False
    )  # [bsz, num_matches]

    # Create the filtered input map with num_matches lowest distance values.
    feature_dimension = input_maps.shape[2]
    filtered_input_maps = torch.gather(
        input_maps, 1, min_indices.unsqueeze(-1).expand(-1, -1, feature_dimension)
    )  # [bsz, num_matches, feature_dimension]

    # Create candidate maps in the same way as input maps, but using corresponding candidate values
    selected_candidate_maps = torch.gather(
        candidate_maps, 1, topk_indices.expand(-1, -1, feature_dimension)
    )  # [bsz, input_map_size, feature_dimension]
    filtered_candidate_maps = torch.gather(
        selected_candidate_maps,
        1,
        min_indices.unsqueeze(-1).expand(-1, -1, feature_dimension),
    )  # [bsz, num_matches, feature_dimension]

    return filtered_input_maps, filtered_candidate_maps


def most_similar_index(
    x: Tensor,
    y: Tensor,
) -> Tensor:
    """For each feature in x, searches the most similar feature in y and returns the
    corresponding index.

    Args:
        x:
            Tensor with shape (B, N, C) containing the features to compare.
        y:
            Tensor with shape (B, N, C) containing the features to search for similarity.
    Returns:
        Index with shape (B, N) such that y[i, index[i, j]] is most similar to x[i, j]
        over all y[i, ...].

    """
    # Normalize the input tensors along the last dimension
    x = functional.normalize(x, dim=-1)
    y = functional.normalize(y, dim=-1)

    similarity = torch.einsum("bnc,bmc->bnm", x, y)
    return similarity.argmax(dim=2)


def select_most_similar(
    x: Tensor,
    y: Tensor,
    y_values: Tensor,
) -> Tensor:
    """For each feature in x, searches the most similar feature in y and returns the
    corresponding value from y_values.

    Args:
        x:
            Tensor with shape (B, N, C).
        y:
            Tensor with shape (B, N, C).
        y_values:
            Tensor with shape (B, N, D).
    Returns:
        Values with shape (B, N, D) where values[i, j] is the entry in y_values[i, ...]
        such that x[i, j] is the most similar to y[i, ...].
    """
    y_index = most_similar_index(x, y)
    y_index = y_index.unsqueeze(-1).expand(y_values.shape)
    return torch.gather(y_values, dim=1, index=y_index)


_NORM_LAYERS = (_NormBase, LayerNorm, CrossMapLRN2d, LocalResponseNorm, GroupNorm)


def get_weight_decay_parameters(
    modules: Iterable[Module],
    decay_norm: bool = False,
    decay_bias: bool = False,
    norm_layers: Tuple[Type[Module], ...] = _NORM_LAYERS,
) -> Tuple[List[Parameter], List[Parameter]]:
    """Returns all parameters of the modules that should be decayed and not decayed.

    Args:
        modules:
            List of modules to get the parameters from.
        decay_norm:
            If True, normalization parameters are decayed.
        decay_bias:
            If True, bias parameters are decayed.
        norm_layers:
            Tuple of normalization classes to decay if decay_norm is True.

    Returns:
        (params, params_no_weight_decay) tuple.
    """
    params = []
    params_no_weight_decay = []

    # Iterate through each module and categorize its parameters into ones that should be decayed and those that should not
    for module in modules:
        for mod in module.modules():
            if isinstance(mod, norm_layers):
                if not decay_norm:
                    params_no_weight_decay.extend(mod.parameters(recurse=False))
                else:
                    params.extend(mod.parameters(recurse=False))
            else:
                for name, param in mod.named_parameters(recurse=False):
                    if not decay_bias and name.endswith("bias"):
                        params_no_weight_decay.append(param)
                    else:
                        params.append(param)
    return params, params_no_weight_decay


def get_named_leaf_modules(module: Module) -> Dict[str, Module]:
    """Returns all leaf modules of the model with their names."""
    return {
        name: mod for name, mod in module.named_modules() if not any(mod.children())
    }


def add_stochastic_depth_to_blocks(vit: Module, prob: float = 0.0, mode="row") -> None:
    """Adds stochastic depth dropout to all transformer blocks in a Vision Transformer Model

    Args:
        vit:
            Vision Transformer Model to which stochastic depth dropout will be added.
        prob:
            Probability of dropping a layer.
        mode:
            Mode for stochastic depth. Default is "row".

    Raises:
        Runtime Error: If torchvision version is less than 0.12.
    """
    if dependency.torchvision_vit_available():
        # Requires torchvision >=0.12
        from torchvision.models.vision_transformer import EncoderBlock
    else:
        raise RuntimeError("add_stochastic_depth_to_blocks requires torchvision>=0.12.")

    if prob <= 0:
        return

    for mod in vit.modules():
        if isinstance(mod, EncoderBlock):
            mod.dropout = Sequential(mod.dropout, StochasticDepth(p=prob, mode=mode))
            mod.mlp = Sequential(mod.mlp, StochasticDepth(p=prob, mode=mode))


def initialize_positional_embedding(
    pos_embedding: Parameter,
    strategy: str,
    num_prefix_tokens: int,
) -> None:
    """Initializes the positional embedding with the given strategy.

    Args:
        pos_embedding:
            Positional embedding parameter.
        strategy:
            Positional embedding initialization strategy. Valid options are:
            ['learn', 'sincos', 'skip']. 'learn' makes the embedding learnable,
            'sincos' creates a fixed 2D sine-cosine positional embedding, and 'skip'
            does not initialize the positional embedding.
        num_prefix_tokens:
            Number of prefix tokens in the positional embedding. This includes the class
            token.
    Raises:
        ValueError: If an invalid strategy is provided.
    """
    strategies = ["learn", "sincos", "skip"]

    # Validate the strategy
    if strategy not in strategies:
        raise ValueError(
            f"Invalid positional embedding strategy: '{strategy}'. Valid options are: "
            f"{strategies}."
        )

    # Initialize the positional embedding based on the startegy
    if strategy == "learn":
        initialize_learnable_positional_embedding(pos_embedding)
    elif strategy == "sincos":
        initialize_2d_sine_cosine_positional_embedding(
            pos_embedding=pos_embedding,
            has_class_token=num_prefix_tokens > 0,
        )
    elif strategy == "skip":
        return


def initialize_learnable_positional_embedding(pos_embedding: Parameter) -> None:
    """Initializes a learnable positional embedding.

    Uses standard initialization for ViT models, see [0].

    - [0]: https://github.com/huggingface/pytorch-image-models/blob/cec70b6779ea81cec0ca08ee4a257b52affd235a/timm/models/vision_transformer.py#L590

    Args:
        pos_embedding:
            Positional embedding parameter.
    """
    init.trunc_normal_(pos_embedding, std=0.02)
    pos_embedding.requires_grad = True


def initialize_2d_sine_cosine_positional_embedding(
    pos_embedding: Parameter, has_class_token: bool
) -> None:
    _, seq_length, hidden_dim = pos_embedding.shape
    grid_size = int((seq_length - int(has_class_token)) ** 0.5)
    sine_cosine_embedding = get_2d_sine_cosine_positional_embedding(
        embed_dim=hidden_dim,
        grid_size=grid_size,
        cls_token=has_class_token,
    )
    pos_embedding.data.copy_(
        torch.from_numpy(sine_cosine_embedding).float().unsqueeze(0)
    )
    # Freeze positional embedding.
    pos_embedding.requires_grad = False


def get_2d_sine_cosine_positional_embedding(
    embed_dim: int, grid_size: int, cls_token: bool
) -> NDArray[np.float32]:
    """Generates 2D sine-cosine positional embedding.

    Code follows: https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py

    Args:
        embed_dim:
            Embedding dimension.
        grid_size:
            Height and width of the grid.
        cls_token:
            If True, a positional embedding for the class token is generated.
    Returns:
        Positional embedding with shape (grid_size * grid_size, embed_dim) or
        (1 + grid_size * grid_size, embed_dim) if cls_token is True.
    """
    grid_h = np.arange(grid_size, dtype=np.float32)
    grid_w = np.arange(grid_size, dtype=np.float32)
    grid = np.meshgrid(grid_w, grid_h)  # here w goes first
    grid = np.stack(grid, axis=0)

    grid = grid.reshape([2, 1, grid_size, grid_size])
    pos_embed = get_2d_sine_cosine_positional_embedding_from_grid(embed_dim, grid)
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed


# TODO(guarin): Remove alias and rename function instead. get_2d_sincos_pos_embed
# was introduced by ijepa while get_2d_sine_cosine_positional_embedding was introduced
# by mae.
get_2d_sincos_pos_embed = get_2d_sine_cosine_positional_embedding


def get_2d_sine_cosine_positional_embedding_from_grid(
    embed_dim: int, grid: NDArray[np.float32]
) -> NDArray[np.float32]:
    """Generates 2D sine-cosine positional embedding from a grid.

    Code follows: https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py

    Args:
        embed_dim:
            Embedding dimension.
        grid:
            Grid of shape (2, grid_size, grid_size) with x and y coordinates.
    Returns:
        Positional embedding with shape (grid_size * grid_size, embed_dim).
    """
    assert embed_dim % 2 == 0

    # Use half of dimensions to encode grid_h
    # (grid_size * grid_size, embed_dim/2)
    emb_h = get_1d_sine_cosine_positional_embedding_from_positions(
        embed_dim // 2, grid[0]
    )

    # Use the other half of dimensions to encode grid_w
    # (grid_size * grid_size, embed_dim/2)
    emb_w = get_1d_sine_cosine_positional_embedding_from_positions(
        embed_dim // 2, grid[1]
    )

    emb = np.concatenate([emb_h, emb_w], axis=1)  # (grid_size * grid_size, embed_dim)
    return emb


def get_1d_sine_cosine_positional_embedding_from_positions(
    embed_dim: int, pos: NDArray[np.float32]
) -> NDArray[np.float32]:
    """Generates 1D sine-cosine positional embedding from positions.

    Code follows: https://github.com/facebookresearch/mae/blob/main/util/pos_embed.py

    Args:
        embed_dim:
            Embedding dimension.
        pos:
            Positions to be encoded with shape (N, M).
    Returns:
        Positional embedding with shape (N * M, embed_dim).
    """
    assert embed_dim % 2 == 0
    omega = np.arange(embed_dim // 2, dtype=np.float32)
    omega /= embed_dim / 2.0
    omega = 1.0 / 10000**omega  # (embed_dim/2,)

    pos = pos.reshape(-1)  # (N*M,)
    out = np.einsum("m,d->md", pos, omega)  # (N*M, embed_dim/2), outer product

    emb_sin = np.sin(out)  # (N*M, embed_dim/2)
    emb_cos = np.cos(out)  # (N*M, embed_dim/2)

    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (N*M, embed_dim)
    return emb


def normalize_mean_var(x: Tensor, dim: int = -1, eps: float = 1.0e-6) -> Tensor:
    """Normalizes the input tensor to zero mean and unit variance.

    Args:
        x:
            Input tensor.
        dim:
            Dimension along which to compute mean and standard deviation. Takes last
            dimension by default.
        eps:
            Epsilon value to avoid division by zero.

    Returns:
        Normalized tensor.
    """
    mean = x.mean(dim=dim, keepdim=True)
    var = x.var(dim=dim, keepdim=True)
    return (x - mean) / (var + eps).sqrt()


def update_drop_path_rate(
    model: VisionTransformer,
    drop_path_rate: float,
    mode: str = "linear",
) -> None:
    """Updates the drop path rate in a TIMM VisionTransformer model.

    Args:
        model:
            TIMM VisionTransformer model.
        drop_path_rate:
            Maximum drop path rate.
        mode:
            Drop path rate update mode. Can be "linear" or "uniform". Linear increases
            the drop path rate from 0 to drop_path_rate over the depth of the model.
            Uniform sets the drop path rate to drop_path_rate for all blocks.
    Raises:
        ValueError: If an unknown mode is provided.
    """
    from timm.layers import DropPath

    total_depth = len(model.blocks)

    # Determine drop path rates based on the specified mode
    if mode == "linear":
        drop_probabilities = np.linspace(0, drop_path_rate, total_depth)
    elif mode == "uniform":
        drop_probabilities = [drop_path_rate for _ in range(total_depth)]
    else:
        raise ValueError(
            f"Unknown mode: '{mode}', supported modes are 'linear' and 'uniform'."
        )

    # Update the drop path rate for each block in the model
    for block, drop_prob in zip(model.blocks, drop_probabilities):
        if drop_prob > 0.0:
            block.drop_path1 = DropPath(drop_prob=drop_path_rate)
            block.drop_path2 = DropPath(drop_prob=drop_path_rate)
        else:
            block.drop_path1 = Identity()
            block.drop_path2 = Identity()


def repeat_interleave_batch(x: Tensor, B: int, repeat: int) -> Tensor:
    """Repeat and interleave the input tensor.

    Args:
        x:
            Tensor with shape (B * N, ...) where B is the batch size and N the number of
            batches.
        B:
            Batch size.
        repeat:
            Number of times to repeat each batch.

    Returns:
        Tensor with shape (B * repeat * N, ...) where each batch is repeated `repeat`
        times.
    """
    N = len(x) // B
    x = torch.cat(
        [
            torch.cat([x[i * B : (i + 1) * B] for _ in range(repeat)], dim=0)
            for i in range(N)
        ],
        dim=0,
    )
    return x


def apply_masks(x: Tensor, masks: Tensor | list[Tensor]) -> Tensor:
    """Apply masks to the input tensor.

    From https://github.com/facebookresearch/ijepa/blob/main/src/masks/utils.py

    Args:
        x:
            Tensor of shape (B, N, D) where N is the number of patches.
        masks:
            Tensor or list of tensors containing indices of patches in
            [0, N-1] to keep. Each tensor musth have shape (B, K) where K is the number
            of patches to keep. All masks must have the same K.

    Returns:
        Tensor of shape (B * num_masks, K, D) where K is the number of patches to keep.
    """

    if not isinstance(masks, list):
        masks = [masks]

    all_x = []
    for m in masks:
        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))
        all_x += [torch.gather(x, dim=1, index=mask_keep)]
    return torch.cat(all_x, dim=0)



================================================
FILE: lightly/models/zoo.py
================================================
""" Lightly Model Zoo """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

from typing import List

ZOO = {
    "resnet-9/simclr/d16/w0.0625": "https://storage.googleapis.com/models_boris/whattolabel-resnet9-simclr-d16-w0.0625-i-ce0d6bd9.pth",
    "resnet-9/simclr/d16/w0.125": "https://storage.googleapis.com/models_boris/whattolabel-resnet9-simclr-d16-w0.125-i-7269c38d.pth",
    "resnet-18/simclr/d16/w1.0": "https://storage.googleapis.com/models_boris/whattolabel-resnet18-simclr-d16-w1.0-i-58852cb9.pth",
    "resnet-18/simclr/d32/w1.0": "https://storage.googleapis.com/models_boris/whattolabel-resnet18-simclr-d32-w1.0-i-085d0693.pth",
    "resnet-34/simclr/d16/w1.0": "https://storage.googleapis.com/models_boris/whattolabel-resnet34-simclr-d16-w1.0-i-6e80d963.pth",
    "resnet-34/simclr/d32/w1.0": "https://storage.googleapis.com/models_boris/whattolabel-resnet34-simclr-d32-w1.0-i-9f185b45.pth",
}


def checkpoints() -> List[str]:
    """Returns the Lightly model zoo as a list of checkpoints.

    Checkpoints:
        ResNet-9:
            SimCLR with width = 0.0625 and num_ftrs = 16
        ResNet-9:
            SimCLR with width = 0.125 and num_ftrs = 16
        ResNet-18:
            SimCLR with width = 1.0 and num_ftrs = 16
        ResNet-18:
            SimCLR with width = 1.0 and num_ftrs = 32
        ResNet-34:
            SimCLR with width = 1.0 and num_ftrs = 16
        ResNet-34:
            SimCLR with width = 1.0 and num_ftrs = 32

    Returns:
        A list of available checkpoints as URLs.

    """
    return [item for key, item in ZOO.items()]



================================================
FILE: lightly/models/modules/__init__.py
================================================
"""The lightly.models.modules package provides reusable modules.

This package contains reusable modules such as the NNmemoryBankModule which
can be combined with any lightly model.

"""

# Copyright (c) 2021. Lightly AG and its affiliates.
# All Rights Reserved


from lightly.models.modules.heads import (
    BarlowTwinsProjectionHead,
    BYOLPredictionHead,
    BYOLProjectionHead,
    DenseCLProjectionHead,
    DINOProjectionHead,
    DINOv2ProjectionHead,
    MMCRProjectionHead,
    MoCoProjectionHead,
    NNCLRPredictionHead,
    NNCLRProjectionHead,
    SimCLRProjectionHead,
    SimSiamPredictionHead,
    SimSiamProjectionHead,
    SMoGPredictionHead,
    SMoGProjectionHead,
    SMoGPrototypes,
    SwaVProjectionHead,
    SwaVPrototypes,
)
from lightly.models.modules.nn_memory_bank import NNMemoryBankModule
from lightly.utils import dependency as _dependency

if _dependency.torchvision_vit_available():
    # Requires torchvision >=0.12
    from lightly.models.modules.masked_autoencoder import (
        MAEBackbone,
        MAEDecoder,
        MAEEncoder,
    )
    from lightly.models.modules.masked_vision_transformer_torchvision import (
        MaskedVisionTransformerTorchvision,
    )

if _dependency.timm_vit_available():
    # Requires timm >= 0.9.9
    from lightly.models.modules.heads_timm import AIMPredictionHead
    from lightly.models.modules.ijepa_timm import IJEPAPredictorTIMM
    from lightly.models.modules.masked_autoencoder_timm import MAEDecoderTIMM
    from lightly.models.modules.masked_causal_vision_transformer import (
        MaskedCausalVisionTransformer,
    )
    from lightly.models.modules.masked_vision_transformer_timm import (
        MaskedVisionTransformerTIMM,
    )



================================================
FILE: lightly/models/modules/center.py
================================================
from typing import Tuple

import torch
import torch.distributed as dist
from torch import Tensor
from torch.nn import Module


class Center(Module):
    """Center module to compute and store the center of a feature tensor as used
    in DINO [0].

    - [0]: DINO, 2021, https://arxiv.org/abs/2104.14294

    Attributes:
        size:
            Size of the tracked center tensor. Dimensions across which the center
            is computed must be set to 1. For example, if the feature tensor has shape
            (batch_size, sequence_length, feature_dim) and the center should be computed
            across the batch and sequence dimensions, the size should be
            (1, 1, feature_dim).
        mode:
            Mode to compute the center. Currently only 'mean' is supported.
        momentum:
            Momentum term for the center calculation.
    """

    def __init__(
        self,
        size: Tuple[int, ...],
        mode: str = "mean",
        momentum: float = 0.9,
    ) -> None:
        """Initializes the Center module with the specified parameters.

        Raises:
            ValueError: If an unknown mode is provided.
        """
        super().__init__()

        center_fn = CENTER_MODE_TO_FUNCTION.get(mode)
        if center_fn is None:
            raise ValueError(
                f"Unknown mode '{mode}'. Valid modes are "
                f"{sorted(CENTER_MODE_TO_FUNCTION.keys())}."
            )
        self._center_fn = center_fn

        self.size = size
        self.dim = tuple(i for i, s in enumerate(size) if s == 1)
        self.center: Tensor  # For mypy
        self.register_buffer("center", torch.zeros(self.size))
        self.momentum = momentum

    @property
    def value(self) -> Tensor:
        """The current value of the center.

        Use this property to do any operations based on the center.
        """
        return self.center

    @torch.no_grad()
    def update(self, x: Tensor) -> None:
        """Update the center with a new batch of features.

        Args:
            x:
                Feature tensor used to update the center. Must have the same number of
                dimensions as self.size.
        """
        batch_center = self._center_fn(x=x, dim=self.dim)
        self.center = center_momentum(
            center=self.center, batch_center=batch_center, momentum=self.momentum
        )

    @torch.no_grad()
    def _center_mean(self, x: Tensor) -> Tensor:
        """Returns the center of the input tensor by calculating the mean."""
        return center_mean(x=x, dim=self.dim)


@torch.no_grad()
def center_mean(x: Tensor, dim: Tuple[int, ...]) -> Tensor:
    """Returns the center of the input tensor by calculating the mean.

    Args:
        x:
            Input tensor.
        dim:
            Dimensions along which the mean is calculated.

    Returns:
        The center of the input tensor.
    """
    batch_center = torch.mean(x, dim=dim, keepdim=True)
    if dist.is_available() and dist.is_initialized():
        dist.all_reduce(batch_center)
        batch_center = batch_center / dist.get_world_size()
    return batch_center


@torch.no_grad()
def center_momentum(center: Tensor, batch_center: Tensor, momentum: float) -> Tensor:
    """Returns the new center with momentum update."""
    return center * momentum + batch_center * (1 - momentum)


CENTER_MODE_TO_FUNCTION = {
    "mean": center_mean,
}



================================================
FILE: lightly/models/modules/heads.py
================================================
""" Projection and Prediction Heads for Self-supervised Learning """

# Copyright (c) 2021. Lightly AG and its affiliates.
# All Rights Reserved

from typing import List, Optional, Sequence, Tuple, Union

import torch
import torch.nn as nn
from torch import Tensor

from lightly.models import utils


class ProjectionHead(nn.Module):
    """Base class for all projection and prediction heads.

    Args:
        blocks:
            List of tuples, each denoting one block of the projection head MLP.
            Each tuple reads (in_features, out_features, batch_norm_layer,
            non_linearity_layer, use_bias (optional)).

    Examples:
        >>> # the following projection head has two blocks
        >>> # the first block uses batch norm an a ReLU non-linearity
        >>> # the second block is a simple linear layer
        >>> projection_head = ProjectionHead([
        >>>     (256, 256, nn.BatchNorm1d(256), nn.ReLU()),
        >>>     (256, 128, None, None)
        >>> ])
    """

    def __init__(
        self,
        blocks: Sequence[
            Union[
                Tuple[int, int, Optional[nn.Module], Optional[nn.Module]],
                Tuple[int, int, Optional[nn.Module], Optional[nn.Module], bool],
            ],
        ],
    ) -> None:
        """Initializes the ProjectionHead module with the specified blocks."""
        super().__init__()

        layers: List[nn.Module] = []
        for block in blocks:
            input_dim, output_dim, batch_norm, non_linearity, *bias = block
            use_bias = bias[0] if bias else not bool(batch_norm)
            layers.append(nn.Linear(input_dim, output_dim, bias=use_bias))
            if batch_norm:
                layers.append(batch_norm)
            if non_linearity:
                layers.append(non_linearity)
        self.layers = nn.Sequential(*layers)

    def forward(self, x: Tensor) -> Tensor:
        """Computes one forward pass through the projection head.

        Args:
            x:
                Input of shape bsz x num_ftrs.
        """
        projection: Tensor = self.layers(x)
        return projection


class BarlowTwinsProjectionHead(ProjectionHead):
    """Projection head used for Barlow Twins.

    "The projector network has three linear layers, each with 8192 output
    units. The first two layers of the projector are followed by a batch
    normalization layer and rectified linear units." [0]

    - [0]: 2021, Barlow Twins, https://arxiv.org/abs/2103.03230
    """

    def __init__(
        self, input_dim: int = 2048, hidden_dim: int = 8192, output_dim: int = 8192
    ):
        """Initializes the BarlowTwinsProjectionHead with the specified dimensions.

        Args:
            input_dim:
                Dimensionality of the input features.
            hidden_dim:
                Dimensionality of the hidden layers.
            output_dim:
                Dimensionality of the output features.
        """
        super(BarlowTwinsProjectionHead, self).__init__(
            [
                (input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()),
                (hidden_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()),
                (hidden_dim, output_dim, None, None),
            ]
        )


class BYOLProjectionHead(ProjectionHead):
    """Projection head used for BYOL.

    "This MLP consists in a linear layer with output size 4096 followed by
    batch normalization, rectified linear units (ReLU), and a final
    linear layer with output dimension 256." [0]

    - [0]: BYOL, 2020, https://arxiv.org/abs/2006.07733
    """

    def __init__(
        self, input_dim: int = 2048, hidden_dim: int = 4096, output_dim: int = 256
    ):
        """Initializes the BYOLProjectionHead with the specified dimensions."""
        super(BYOLProjectionHead, self).__init__(
            [
                (input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()),
                (hidden_dim, output_dim, None, None),
            ]
        )


class BYOLPredictionHead(ProjectionHead):
    """Prediction head used for BYOL.

    "This MLP consists in a linear layer with output size 4096 followed by
    batch normalization, rectified linear units (ReLU), and a final
    linear layer with output dimension 256." [0]

    - [0]: BYOL, 2020, https://arxiv.org/abs/2006.07733
    """

    def __init__(
        self, input_dim: int = 256, hidden_dim: int = 4096, output_dim: int = 256
    ):
        super(BYOLPredictionHead, self).__init__(
            [
                (input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()),
                (hidden_dim, output_dim, None, None),
            ]
        )


class MoCoProjectionHead(ProjectionHead):
    """Projection head used for MoCo.

    "(...) we replace the fc head in MoCo with a 2-layer MLP head (hidden layer
    2048-d, with ReLU)" [1]

    "The projection head is a 3-layer MLP. The prediction head is a 2-layer MLP. The
    hidden layers of both MLPs are 4096-d and are with ReLU; the output layers of both
    MLPs are 256-d, without ReLU. In MoCo v3, all layers in both MLPs have BN" [2]

    - [0]: MoCo v1, 2020, https://arxiv.org/abs/1911.05722
    - [1]: MoCo v2, 2020, https://arxiv.org/abs/2003.04297
    - [2]: MoCo v3, 2021, https://arxiv.org/abs/2104.02057
    """

    def __init__(
        self,
        input_dim: int = 2048,
        hidden_dim: int = 2048,
        output_dim: int = 128,
        num_layers: int = 2,
        batch_norm: bool = False,
    ):
        """Initialize a new MoCoProjectionHead instance.

        Args:
            input_dim:
                Number of input dimensions.
            hidden_dim:
                Number of hidden dimensions (2048 for v2, 4096 for v3).
            output_dim:
                Number of output dimensions (128 for v2, 256 for v3).
            num_layers:
                Number of hidden layers (2 for v2, 3 for v3).
            batch_norm:
                Whether or not to use batch norms. (False for v2, True for v3).
        """
        layers: List[Tuple[int, int, Optional[nn.Module], Optional[nn.Module]]] = []
        layers.append(
            (
                input_dim,
                hidden_dim,
                nn.BatchNorm1d(hidden_dim) if batch_norm else None,
                nn.ReLU(),
            )
        )
        for _ in range(2, num_layers):
            layers.append(
                (
                    hidden_dim,
                    hidden_dim,
                    nn.BatchNorm1d(hidden_dim) if batch_norm else None,
                    nn.ReLU(),
                )
            )
        layers.append(
            (
                hidden_dim,
                output_dim,
                nn.BatchNorm1d(output_dim) if batch_norm else None,
                None,
            )
        )
        super().__init__(layers)


class NNCLRProjectionHead(ProjectionHead):
    """Projection head used for NNCLR.

    "The architectureof the projection MLP is 3 fully connected layers of sizes
    [2048,2048,d] where d is the embedding size used to apply the loss. We use
    d = 256 in the experiments unless otherwise stated. All fully-connected
    layers are followed by batch-normalization [36]. All the batch-norm layers
    except the last layer are followed by ReLU activation." [0]

    - [0]: NNCLR, 2021, https://arxiv.org/abs/2104.14548
    """

    def __init__(
        self, input_dim: int = 2048, hidden_dim: int = 2048, output_dim: int = 256
    ):
        """Initializes the NNCLRProjectionHead with the specified dimensions.

        Args:
            input_dim:
                Dimensionality of the input features.
            hidden_dim:
                Dimensionality of the hidden layers.
            output_dim:
                Dimensionality of the output features.
        """
        super(NNCLRProjectionHead, self).__init__(
            [
                (input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()),
                (hidden_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()),
                (hidden_dim, output_dim, nn.BatchNorm1d(output_dim), None),
            ]
        )


class NNCLRPredictionHead(ProjectionHead):
    """Prediction head used for NNCLR.

    "The architecture of the prediction MLP g is 2 fully-connected layers
    of size [4096,d]. The hidden layer of the prediction MLP is followed by
    batch-norm and ReLU. The last layer has no batch-norm or activation." [0]

    - [0]: NNCLR, 2021, https://arxiv.org/abs/2104.14548
    """

    def __init__(
        self, input_dim: int = 256, hidden_dim: int = 4096, output_dim: int = 256
    ):
        super(NNCLRPredictionHead, self).__init__(
            [
                (input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()),
                (hidden_dim, output_dim, None, None),
            ]
        )


class SimCLRProjectionHead(ProjectionHead):
    """Projection head used for SimCLR.

    "We use a MLP with one hidden layer to obtain zi = g(h) = W_2 * σ(W_1 * h)
    where σ is a ReLU non-linearity." [0]

    "We use a 3-layer MLP projection head on top of a ResNet encoder." [1]

    - [0] SimCLR v1, 2020, https://arxiv.org/abs/2002.05709
    - [1] SimCLR v2, 2020, https://arxiv.org/abs/2006.10029
    """

    def __init__(
        self,
        input_dim: int = 2048,
        hidden_dim: int = 2048,
        output_dim: int = 128,
        num_layers: int = 2,
        batch_norm: bool = True,
    ):
        """Initialize a new SimCLRProjectionHead instance.

        Args:
            input_dim:
                Number of input dimensions.
            hidden_dim:
                Number of hidden dimensions.
            output_dim:
                Number of output dimensions.
            num_layers:
                Number of hidden layers (2 for v1, 3+ for v2).
            batch_norm:
                Whether or not to use batch norms.
        """
        layers: List[Tuple[int, int, Optional[nn.Module], Optional[nn.Module]]] = []
        layers.append(
            (
                input_dim,
                hidden_dim,
                nn.BatchNorm1d(hidden_dim) if batch_norm else None,
                nn.ReLU(),
            )
        )
        for _ in range(2, num_layers):
            layers.append(
                (
                    hidden_dim,
                    hidden_dim,
                    nn.BatchNorm1d(hidden_dim) if batch_norm else None,
                    nn.ReLU(),
                )
            )
        layers.append(
            (
                hidden_dim,
                output_dim,
                nn.BatchNorm1d(output_dim) if batch_norm else None,
                None,
            )
        )
        super().__init__(layers)


class SimSiamProjectionHead(ProjectionHead):
    """Projection head used for SimSiam.

    "The projection MLP (in f) has BN applied to each fully-connected (fc)
    layer, including its output fc. Its output fc has no ReLU. The hidden fc is
    2048-d. This MLP has 3 layers." [0]

    - [0]: SimSiam, 2020, https://arxiv.org/abs/2011.10566
    """

    def __init__(
        self, input_dim: int = 2048, hidden_dim: int = 2048, output_dim: int = 2048
    ):
        super(SimSiamProjectionHead, self).__init__(
            [
                (input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()),
                (hidden_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()),
                (
                    hidden_dim,
                    output_dim,
                    nn.BatchNorm1d(output_dim, affine=False),
                    None,
                ),
            ]
        )


class SMoGPrototypes(nn.Module):
    """SMoG prototypes module for synchronous momentum grouping.

    Args:
        group_features:
            Tensor containing the group features.
        beta:
            Beta parameter for momentum updating.
    """

    def __init__(
        self,
        group_features: Tensor,
        beta: float,
    ):
        """Initializes the SMoGPrototypes module with the specified parameter."""
        super(SMoGPrototypes, self).__init__()
        self.group_features = nn.Parameter(group_features, requires_grad=False)
        self.beta = beta

    def forward(
        self, x: Tensor, group_features: Tensor, temperature: float = 0.1
    ) -> Tensor:
        """Computes the logits for given model outputs and group features.

        Args:
            x:
                Tensor of shape bsz x dim.
            group_features:
                Momentum updated group features of shape n_groups x dim.
            temperature:
                Temperature parameter for calculating the logits.

        Returns:
            The computed logits.
        """
        x = torch.nn.functional.normalize(x, dim=1)
        group_features = torch.nn.functional.normalize(group_features, dim=1)
        logits = torch.mm(x, group_features.t())
        return logits / temperature

    def get_updated_group_features(self, x: Tensor) -> Tensor:
        """Performs the synchronous momentum update of the group vectors.

        Args:
            x:
                Tensor of shape bsz x dim.

        Returns:
            The updated group features.
        """
        assignments = self.assign_groups(x)
        group_features = torch.clone(self.group_features.data)
        for assigned_class in torch.unique(assignments):
            mask = assignments == assigned_class
            group_features[assigned_class] = self.beta * self.group_features[
                assigned_class
            ] + (1 - self.beta) * x[mask].mean(dim=0)

        return group_features

    def set_group_features(self, x: Tensor) -> None:
        """Sets the group features and asserts they don't require gradient."""
        self.group_features.data = x.to(self.group_features.device)

    @torch.no_grad()
    def assign_groups(self, x: Tensor) -> Tensor:
        """Assigns each representation in x to a group based on cosine similarity.

        Args:
            x:
                Tensor of shape (bsz, dim).

        Returns:
            Tensor of shape (bsz,) indicating group assignments.
        """
        return torch.argmax(self.forward(x, self.group_features), dim=-1)


class SMoGProjectionHead(ProjectionHead):
    """Projection head used for SMoG.

    "The two kinds of head are both a two-layer MLP and their hidden layer is
    followed by a BatchNorm [28] and an activation function. (...) The output
    layer of projection head also has BN" [0]

    - [0]: SMoG, 2022, https://arxiv.org/pdf/2207.06167.pdf
    """

    def __init__(
        self, input_dim: int = 2048, hidden_dim: int = 2048, output_dim: int = 128
    ):
        """Initializes the SMoGProjectionHead with the specified dimensions.

        Args:
            input_dim:
                Dimensionality of the input features.
            hidden_dim:
                Dimensionality of the hidden layers.
            output_dim:
                Dimensionality of the output features.
        """
        super(SMoGProjectionHead, self).__init__(
            [
                (input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()),
                (
                    hidden_dim,
                    output_dim,
                    nn.BatchNorm1d(output_dim, affine=False),
                    None,
                ),
            ]
        )


class SMoGPredictionHead(ProjectionHead):
    """Prediction head used for SMoG.

    "The two kinds of head are both a two-layer MLP and their hidden layer is
    followed by a BatchNorm [28] and an activation function. (...) The output
    layer of projection head also has BN" [0]

    - [0]: SMoG, 2022, https://arxiv.org/pdf/2207.06167.pdf
    """

    def __init__(
        self, input_dim: int = 128, hidden_dim: int = 2048, output_dim: int = 128
    ):
        """Initializes the SMoGPredictionHead with the specified dimensions.

        Args:
            input_dim:
                Dimensionality of the input features.
            hidden_dim:
                Dimensionality of the hidden layers.
            output_dim:
                Dimensionality of the output features.
        """

        super(SMoGPredictionHead, self).__init__(
            [
                (input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()),
                (hidden_dim, output_dim, None, None),
            ]
        )


class SimSiamPredictionHead(ProjectionHead):
    """Prediction head used for SimSiam.

    "The prediction MLP (h) has BN applied to its hidden fc layers. Its output
    fc does not have BN (...) or ReLU. This MLP has 2 layers." [0]

    - [0]: SimSiam, 2020, https://arxiv.org/abs/2011.10566
    """

    def __init__(
        self, input_dim: int = 2048, hidden_dim: int = 512, output_dim: int = 2048
    ):
        """Initializes the SimSiamPredictionHead with the specified dimensions.

        Args:
            input_dim:
                Dimensionality of the input features.
            hidden_dim:
                Dimensionality of the hidden layers.
            output_dim:
                Dimensionality of the output features.
        """
        super(SimSiamPredictionHead, self).__init__(
            [
                (input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()),
                (hidden_dim, output_dim, None, None),
            ]
        )


class SwaVProjectionHead(ProjectionHead):
    """Projection head used for SwaV.

    - [0]: SwAV, 2020, https://arxiv.org/abs/2006.09882
    """

    def __init__(
        self, input_dim: int = 2048, hidden_dim: int = 2048, output_dim: int = 128
    ):
        """Initializes the SwaVProjectionHead with the specified dimensions."""
        super(SwaVProjectionHead, self).__init__(
            [
                (input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()),
                (hidden_dim, output_dim, None, None),
            ]
        )


class SwaVPrototypes(nn.Module):
    """Multihead Prototypes used for SwaV.

    Each output feature is assigned to a prototype, SwaV solves the swapped
    prediction problem where the features of one augmentation are used to
    predict the assigned prototypes of the other augmentation.

    Attributes:
        input_dim:
            The input dimension of the head.
        n_prototypes:
            Number of prototypes.
        n_steps_frozen_prototypes:
            Number of steps during which we keep the prototypes fixed.

    Examples:
        >>> # use features with 128 dimensions and 512 prototypes
        >>> prototypes = SwaVPrototypes(128, 512)
        >>>
        >>> # pass batch through backbone and projection head.
        >>> features = model(x)
        >>> features = nn.functional.normalize(features, dim=1, p=2)
        >>>
        >>> # logits has shape bsz x 512
        >>> logits = prototypes(features)
    """

    def __init__(
        self,
        input_dim: int = 128,
        n_prototypes: Union[List[int], int] = 3000,
        n_steps_frozen_prototypes: int = 0,
    ):
        """Intializes the SwaVPrototypes module with the specified parameters"""
        super(SwaVPrototypes, self).__init__()

        # Default to a list of 1 if n_prototypes is an int.
        self.n_prototypes = (
            n_prototypes if isinstance(n_prototypes, list) else [n_prototypes]
        )
        self._is_single_prototype = True if isinstance(n_prototypes, int) else False
        self.heads = nn.ModuleList(
            [nn.Linear(input_dim, prototypes) for prototypes in self.n_prototypes]
        )
        self.n_steps_frozen_prototypes = n_steps_frozen_prototypes

    def forward(
        self, x: Tensor, step: Optional[int] = None
    ) -> Union[Tensor, List[Tensor]]:
        """Forward pass of the SwaVPrototypes module.

        Args:
            x:
                Input tensor.
            step:
                Current training step.

        Returns:
            The logits after passing through the prototype heads. Returns a single tensor
            if there's one prototype head, otherwise returns a list of tensors.
        """
        self._freeze_prototypes_if_required(step)
        out = []
        for layer in self.heads:
            out.append(layer(x))
        return out[0] if self._is_single_prototype else out

    def normalize(self) -> None:
        """Normalizes the prototypes so that they are on the unit sphere."""
        for layer in self.heads:
            utils.normalize_weight(layer.weight)

    def _freeze_prototypes_if_required(self, step: Optional[int] = None) -> None:
        """Freezes the prototypes if the specified number of steps has been reached."""
        if self.n_steps_frozen_prototypes > 0:
            if step is None:
                raise ValueError(
                    "`n_steps_frozen_prototypes` is greater than 0, please"
                    " provide the `step` argument to the `forward()` method."
                )
            self.requires_grad_(step >= self.n_steps_frozen_prototypes)


class DINOProjectionHead(ProjectionHead):
    """Projection head used in DINO.

    "The projection head consists of a 3-layer multi-layer perceptron (MLP)
    with hidden dimension 2048 followed by l2 normalization and a weight
    normalized fully connected layer with K dimensions, which is similar to the
    design from SwAV [1]." [0]

    - [0]: DINO, 2021, https://arxiv.org/abs/2104.14294
    - [1]: SwAV, 2020, https://arxiv.org/abs/2006.09882

    Attributes:
        input_dim:
            The input dimension of the head.
        hidden_dim:
            The hidden dimension.
        bottleneck_dim:
            Dimension of the bottleneck in the last layer of the head.
        output_dim:
            The output dimension of the head.
        batch_norm:
            Whether to use batch norm or not. Should be set to False when using
            a vision transformer backbone.
        freeze_last_layer:
            Number of epochs during which we keep the output layer fixed.
            Typically doing so during the first epoch helps training. Try
            increasing this value if the loss does not decrease.
        norm_last_layer:
            Whether or not to weight normalize the last layer of the DINO head.
            Not normalizing leads to better performance but can make the
            training unstable.
    """

    def __init__(
        self,
        input_dim: int = 2048,
        hidden_dim: int = 2048,
        bottleneck_dim: int = 256,
        output_dim: int = 65536,
        batch_norm: bool = False,
        freeze_last_layer: int = -1,
        norm_last_layer: bool = True,
    ):
        """Initializes the DINOProjectionHead with the specified dimensions."""
        super().__init__(
            [
                (
                    input_dim,
                    hidden_dim,
                    nn.BatchNorm1d(hidden_dim) if batch_norm else None,
                    nn.GELU(),
                ),
                (
                    hidden_dim,
                    hidden_dim,
                    nn.BatchNorm1d(hidden_dim) if batch_norm else None,
                    nn.GELU(),
                ),
                (hidden_dim, bottleneck_dim, None, None),
            ]
        )
        self.apply(self._init_weights)
        self.freeze_last_layer = freeze_last_layer
        self.last_layer = nn.Linear(bottleneck_dim, output_dim, bias=False)
        self.last_layer = nn.utils.weight_norm(self.last_layer)
        # Tell mypy this is ok because fill_ is overloaded.
        self.last_layer.weight_g.data.fill_(1)  # type: ignore

        # Option to normalize last layer.
        if norm_last_layer:
            self.last_layer.weight_g.requires_grad = False

    def cancel_last_layer_gradients(self, current_epoch: int) -> None:
        """Cancel last layer gradients to stabilize the training."""
        if current_epoch >= self.freeze_last_layer:
            return
        for param in self.last_layer.parameters():
            param.grad = None

    def forward(self, x: Tensor) -> Tensor:
        """Computes one forward pass through the head."""
        x = self.layers(x)
        # l2 normalization
        x = nn.functional.normalize(x, dim=-1, p=2)
        x = self.last_layer(x)
        return x

    def _init_weights(self, module: nn.Module) -> None:
        """Initializes layers with a truncated normal distribution."""
        if isinstance(module, nn.Linear):
            utils._no_grad_trunc_normal(
                module.weight,
                mean=0,
                std=0.02,
                a=-2,
                b=2,
            )
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)


class DINOv2ProjectionHead(ProjectionHead):
    def __init__(
        self,
        input_dim: int = 2048,
        hidden_dim: int = 2048,
        bottleneck_dim: int = 256,
        output_dim: int = 65536,
        batch_norm: bool = False,
    ) -> None:
        super().__init__(
            [
                (
                    input_dim,
                    hidden_dim,
                    nn.BatchNorm1d(hidden_dim) if batch_norm else None,
                    nn.GELU(),
                ),
                (
                    hidden_dim,
                    hidden_dim,
                    nn.BatchNorm1d(hidden_dim) if batch_norm else None,
                    nn.GELU(),
                ),
                (hidden_dim, bottleneck_dim, None, None),
            ]
        )
        self.apply(self._init_weights)
        self.last_layer = nn.utils.weight_norm(
            nn.Linear(bottleneck_dim, output_dim, bias=False)
        )
        self.last_layer.weight_g.data.fill_(1)  # type: ignore[operator]

    def _init_weights(self, m: nn.Module) -> None:
        if isinstance(m, nn.Linear):
            nn.init.trunc_normal_(m.weight, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, x: Tensor) -> Tensor:
        x = self.layers(x)
        eps = 1e-6 if x.dtype == torch.float16 else 1e-12
        x = nn.functional.normalize(x, dim=-1, p=2, eps=eps)
        x = self.last_layer(x)
        return x


class MMCRProjectionHead(ProjectionHead):
    """Projection head used for MMCR.

    "Following Chen et al. (14), we append a small perceptron to the output
    of the average pooling layer of the ResNet so that zi = g(h(xi)), where
    h is the ResNet and g is the MLP." [0]

    - [0]: MMCR, 2023, https://arxiv.org/abs/2303.03307
    """

    def __init__(
        self,
        input_dim: int = 2048,
        hidden_dim: int = 8192,
        output_dim: int = 512,
        num_layers: int = 2,
        batch_norm: bool = True,
        use_bias: bool = False,
    ):
        """Initialize a new MMCRProjectionHead instance.

        Args:
            input_dim:
                Number of input dimensions.
            hidden_dim:
                Number of hidden dimensions.
            output_dim:
                Number of output dimensions.
            num_layers:
                Number of hidden layers.
            batch_norm:
                Whether or not to use batch norms.
            use_bias:
                Whether or not to use bias in the linear layers.
        """
        layers: List[
            Tuple[int, int, Optional[nn.Module], Optional[nn.Module], bool]
        ] = []

        # Add the first layer
        layers.append(
            (
                input_dim,
                hidden_dim,
                nn.BatchNorm1d(hidden_dim) if batch_norm else None,
                nn.ReLU(),
                use_bias,
            )
        )

        # Add the hidden layers
        for _ in range(num_layers - 1):
            layers.append(
                (
                    hidden_dim,
                    hidden_dim,
                    nn.BatchNorm1d(hidden_dim) if batch_norm else None,
                    nn.ReLU(),
                    use_bias,
                )
            )

        # Add the output layer
        layers.append((hidden_dim, output_dim, None, None, use_bias))
        super().__init__(layers)


class MSNProjectionHead(ProjectionHead):
    """Projection head for MSN [0].

    "We train with a 3-layer projection head with output dimension 256 and
    batch-normalization at the input and hidden layers.." [0]

    Code inspired by [1].

    - [0]: Masked Siamese Networks, 2022, https://arxiv.org/abs/2204.07141
    - [1]: https://github.com/facebookresearch/msn

    Attributes:
        input_dim:
            Input dimension, default value 768 is for a ViT base model.
        hidden_dim:
            Hidden dimension.
        output_dim:
            Output dimension.
    """

    def __init__(
        self,
        input_dim: int = 768,
        hidden_dim: int = 2048,
        output_dim: int = 256,
    ):
        """Initializes the MSNProjectionHead with the specified dimensions."""
        super().__init__(
            blocks=[
                (input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.GELU()),
                (hidden_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.GELU()),
                (hidden_dim, output_dim, None, None),
            ]
        )


class TiCoProjectionHead(ProjectionHead):
    """Projection head used for TiCo.

    "This MLP consists in a linear layer with output size 4096 followed by
    batch normalization, rectified linear units (ReLU), and a final
    linear layer with output dimension 256." [0]

    - [0]: TiCo, 2022, https://arxiv.org/pdf/2206.10698.pdf
    """

    def __init__(
        self, input_dim: int = 2048, hidden_dim: int = 4096, output_dim: int = 256
    ):
        """Initializes the TiCoProjectionHead with the specified dimensions."""
        super(TiCoProjectionHead, self).__init__(
            [
                (input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()),
                (hidden_dim, output_dim, None, None),
            ]
        )


class VICRegProjectionHead(ProjectionHead):
    """Projection head used for VICReg.

    "The projector network has three linear layers, each with 8192 output
    units. The first two layers of the projector are followed by a batch
    normalization layer and rectified linear units." [0]

    - [0]: 2022, VICReg, https://arxiv.org/pdf/2105.04906.pdf
    """

    def __init__(
        self,
        input_dim: int = 2048,
        hidden_dim: int = 8192,
        output_dim: int = 8192,
        num_layers: int = 3,
    ):
        """Initializes the VICRegProjectionHead with the specified dimensions.

        Args:
            input_dim:
                Dimensionality of the input features.
            hidden_dim:
                Dimensionality of the hidden layers.
            output_dim:
                Dimensionality of the output features.
            num_layers:
                Number of layers in the projection head.
        """
        hidden_layers = [
            (hidden_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU())
            for _ in range(num_layers - 2)  # Exclude first and last layer.
        ]
        super(VICRegProjectionHead, self).__init__(
            [
                (input_dim, hidden_dim, nn.BatchNorm1d(hidden_dim), nn.ReLU()),
                *hidden_layers,
                (hidden_dim, output_dim, None, None),
            ]
        )


class VicRegLLocalProjectionHead(ProjectionHead):
    """Projection head used for the local head of VICRegL.

    "The projector network has three linear layers. The first two layers of the projector
    are followed by a batch normalization layer and rectified linear units." [0]

    - [0]: 2022, VICRegL, https://arxiv.org/abs/2210.01571
    """

    def __init__(
        self, input_dim: int = 2048, hidden_dim: int = 8192, output_dim: int = 8192
    ):
        """Initializes the VicRegLLocalProjectionHead with the specified dimensions."""
        super(VicRegLLocalProjectionHead, self).__init__(
            [
                (input_dim, hidden_dim, nn.LayerNorm(hidden_dim), nn.ReLU()),
                (hidden_dim, hidden_dim, nn.LayerNorm(hidden_dim), nn.ReLU()),
                (hidden_dim, output_dim, None, None),
            ]
        )


class DenseCLProjectionHead(ProjectionHead):
    """Projection head for DenseCL [0].

    The projection head consists of a 2-layer MLP. It can be used for global and local
    features.

    - [0]: 2021, DenseCL: https://arxiv.org/abs/2011.09157
    """

    def __init__(
        self, input_dim: int = 2048, hidden_dim: int = 2048, output_dim: int = 128
    ):
        """Initializes the DenseCLProjectionHead with the specified dimensions."""
        super().__init__(
            [
                (input_dim, hidden_dim, None, nn.ReLU()),
                (hidden_dim, output_dim, None, None),
            ]
        )



================================================
FILE: lightly/models/modules/heads_timm.py
================================================
from typing import Type

from timm.layers import Mlp
from torch import Tensor
from torch.nn import GELU, LayerNorm, Linear, Module, Sequential


class AIMPredictionHeadBlock(Module):
    """Prediction head block for AIM [0].

    - [0]: AIM, 2024, https://arxiv.org/abs/2401.08541

    Args:
        input_dim:
            Dimensionality of the input features.
        output_dim:
            Dimensionality of the output features.
        mlp_ratio:
            Ratio used to determine the hidden layer size in the MLP.
        proj_drop:
            Dropout rate for the projection layer.
        act_layer:
            Activation layer to use.
        norm_layer:
            Normalization layer to use.
        mlp_layer:
            MLP layer to use.
    """

    def __init__(
        self,
        input_dim: int,
        output_dim: int,
        mlp_ratio: float = 4.0,
        proj_drop: float = 0.0,
        act_layer: Type[Module] = GELU,
        norm_layer: Type[Module] = LayerNorm,
        mlp_layer: Type[Module] = Mlp,
    ) -> None:
        """Initializes the AIMPredictionHeadBlock module with the specified parameters."""

        super().__init__()
        self.norm = norm_layer(input_dim)  # type: ignore[call-arg]
        self.mlp = mlp_layer(  # type: ignore[call-arg]
            in_features=input_dim,
            hidden_features=int(input_dim * mlp_ratio),
            out_features=output_dim,
            act_layer=act_layer,
            drop=proj_drop,
            bias=False,
        )

    def forward(self, x: Tensor) -> Tensor:
        """Forward pass of the AIMPredictionHeadBlock.

        Args:
            x:
                Input tensor.

        Returns:
            Output tensor after applying the MLP and normalization.
        """
        x = x + self.mlp(self.norm(x))
        return x


class AIMPredictionHead(Module):
    """Prediction head for AIM [0].

    - [0]: AIM, 2024, https://arxiv.org/abs/2401.08541

    Args:
        input_dim:
            Dimensionality of the input features.
        output_dim:
            Dimensionality of the output features.
        hidden_dim:
            Dimensionality of the hidden layer.
        num_blocks:
            Number of blocks in the prediction head.
        mlp_ratio:
            Ratio used to determine the hidden layer size in the MLP.
        proj_drop:
            Dropout rate for the projection layer.
        act_layer:
            Activation layer to use.
        norm_layer:
            Normalization layer to use.
        mlp_layer:
            MLP layer to use.
        block_fn:
            Block function to use for the prediction head.
    """

    def __init__(
        self,
        input_dim: int,
        output_dim: int,
        hidden_dim: int = 2048,
        num_blocks: int = 12,
        mlp_ratio: float = 4.0,
        proj_drop: float = 0.0,
        act_layer: Type[Module] = GELU,
        norm_layer: Type[Module] = LayerNorm,
        mlp_layer: Type[Module] = Mlp,
        block_fn: Type[Module] = AIMPredictionHeadBlock,
    ) -> None:
        """Initializes the AIMPredictionHead module with the specified parameters."""

        super().__init__()
        self.blocks = Sequential(
            # Linear layer to project the input dimension to the hidden dimension.
            Linear(input_dim, hidden_dim, bias=False),
            # Main blocks.
            *[
                block_fn(  # type: ignore[call-arg]
                    input_dim=hidden_dim,
                    output_dim=hidden_dim,
                    mlp_ratio=mlp_ratio,
                    proj_drop=proj_drop,
                    norm_layer=norm_layer,
                    act_layer=act_layer,
                    mlp_layer=mlp_layer,
                )
                for _ in range(num_blocks)
            ],
            # Linear layer to project the hidden dimension to the output dimension.
            norm_layer(hidden_dim),  # type: ignore[call-arg]
            Linear(hidden_dim, output_dim, bias=False),
        )

    def forward(self, x: Tensor) -> Tensor:
        """Forward pass of the AIMPredictionHead.

        Args:
            x:
                Input tensor.

        Returns:
            Output tensor after processing through the prediction head blocks.
        """
        x = self.blocks(x)
        return x



================================================
FILE: lightly/models/modules/ijepa.py
================================================
import math
from functools import partial
from typing import Callable, List, Optional

import numpy as np
import torch
import torch.nn as nn
from torchvision.models import vision_transformer
from torchvision.models.vision_transformer import ConvStemConfig

from lightly.models import utils


class IJEPAPredictor(vision_transformer.Encoder):
    """Predictor for the I-JEPA model [0].

    Experimental: Support for I-JEPA is experimental, there might be breaking changes
    in the future.

    Predict patch embeddings. Code inspired by [1].

    - [0]: Joint-Embedding Predictive Architecture, 2023, https://arxiv.org/abs/2301.08243
    - [1]: https://github.com/facebookresearch/ijepa

    Attributes:
        seq_length:
            Token sequence length, including the class token.
        num_layers:
            Number of transformer blocks.
        num_heads:
            Number of attention heads.
        hidden_dim:
            Dimension of the input and output tokens.
        predictor_embed_dim:
            Dimension of inner predicted tokens
        mlp_dim:
            Dimension of the MLP in the transformer block.
        dropout:
            Percentage of elements set to zero after the MLP in the transformer.
        attention_dropout:
            Percentage of elements set to zero after the attention head.
    """

    def __init__(
        self,
        seq_length: int,
        num_layers: int,
        num_heads: int,
        hidden_dim: int,
        predictor_embed_dim: int,
        num_patches: int,
        mlp_dim: int,
        dropout: float,
        attention_dropout: float,
        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),
        **kwargs,
    ):
        """Initializes the IJEPAPredictor with the specified dimensions."""

        super().__init__(
            seq_length=seq_length,
            num_layers=num_layers,
            num_heads=num_heads,
            hidden_dim=hidden_dim,
            mlp_dim=mlp_dim,
            dropout=dropout,
            attention_dropout=attention_dropout,
            norm_layer=norm_layer,
        )
        self.predictor_embed = nn.Linear(mlp_dim, predictor_embed_dim, bias=True)
        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))
        self.predictor_proj = nn.Linear(predictor_embed_dim, mlp_dim, bias=True)
        self.predictor_pos_embed = nn.Parameter(
            torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False
        )
        predictor_pos_embed = utils.get_2d_sincos_pos_embed(
            self.predictor_pos_embed.shape[-1], int(num_patches**0.5), cls_token=False
        )
        self.predictor_pos_embed.data.copy_(
            torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)
        )

    @classmethod
    def from_vit_encoder(cls, vit_encoder, num_patches):
        """Creates an I-JEPA predictor backbone (multi-head attention and layernorm) from a torchvision ViT encoder.

        Args:
            vit_encoder: The Vision Transformer encoder from torchvision.
            num_patches: The number of patches (tokens).

        Returns:
            IJEPAPredictor: An I-JEPA predictor backbone initialized from the ViT encoder.
        """

        # Create a new instance with dummy values as they will be overwritten
        # by the copied vit_encoder attributes
        encoder = cls(
            seq_length=1,
            num_layers=1,
            num_heads=1,
            hidden_dim=1,
            predictor_embed_dim=768,
            mlp_dim=768,
            num_patches=num_patches,
            dropout=0,
            attention_dropout=0,
        )

        # Copy attributes from the ViT encoder
        encoder.layers = vit_encoder.layers
        encoder.ln = vit_encoder.ln

        return encoder

    def forward(self, x, masks_x, masks):
        """Forward pass of the IJEPAPredictor.

        Args:
            x:
                Input tensor.
            masks_x:
                Mask indices for the input tensor.
            masks:
                Mask indices for the predicted tokens.

        Returns:
            The predicted output tensor.
        """
        assert (masks is not None) and (
            masks_x is not None
        ), "Cannot run predictor without mask indices"

        if not isinstance(masks_x, list):
            masks_x = [masks_x]

        if not isinstance(masks, list):
            masks = [masks]

        B = len(x) // len(masks_x)
        x = self.predictor_embed(x)
        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)

        x += utils.apply_masks(x_pos_embed, masks_x)
        _, N_ctxt, _ = x.shape

        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)
        pos_embs = utils.apply_masks(pos_embs, masks)
        pos_embs = utils.repeat_interleave_batch(pos_embs, B, repeat=len(masks_x))
        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)

        pred_tokens += pos_embs
        x = x.repeat(len(masks), 1, 1)
        x = torch.cat([x, pred_tokens], dim=1)

        x = self.ln(self.layers(x))

        x = x[:, N_ctxt:]
        x = self.predictor_proj(x)

        return x


class IJEPAEncoder(vision_transformer.Encoder):
    """Encoder for the I-JEPA model [0].

    Experimental: Support for I-JEPA is experimental, there might be breaking changes
    in the future.

    Encodes patch embeddings. Code inspired by [1].

    - [0]: Joint-Embedding Predictive Architecture, 2023, https://arxiv.org/abs/2301.08243
    - [1]: https://github.com/facebookresearch/ijepa

    Attributes:
        seq_length:
            Token sequence length, including the class token.
        num_layers:
            Number of transformer blocks.
        num_heads:
            Number of attention heads.
        hidden_dim:
            Dimension of the input and output tokens.
        mlp_dim:
            Dimension of the MLP in the transformer block.
        dropout:
            Percentage of elements set to zero after the MLP in the transformer.
        attention_dropout:
            Percentage of elements set to zero after the attention head.
    """

    def __init__(
        self,
        seq_length: int,
        num_layers: int,
        num_heads: int,
        hidden_dim: int,
        mlp_dim: int,
        dropout: float,
        attention_dropout: float,
        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),
    ):
        """Initializes the IJEPAEncoder with the specified dimensions."""

        super().__init__(
            seq_length=seq_length,
            num_layers=num_layers,
            num_heads=num_heads,
            hidden_dim=hidden_dim,
            mlp_dim=mlp_dim,
            dropout=dropout,
            attention_dropout=attention_dropout,
            norm_layer=norm_layer,
        )

    @classmethod
    def from_vit_encoder(cls, vit_encoder: vision_transformer.Encoder):
        """Creates a IJEPA encoder from a torchvision ViT encoder."""

        # Create a new instance with dummy values as they will be overwritten
        # by the copied vit_encoder attributes
        encoder = cls(
            seq_length=1,
            num_layers=1,
            num_heads=1,
            hidden_dim=1,
            mlp_dim=1,
            dropout=0,
            attention_dropout=0,
        )
        encoder.pos_embedding = vit_encoder.pos_embedding
        encoder.dropout = vit_encoder.dropout
        encoder.layers = vit_encoder.layers
        encoder.ln = vit_encoder.ln
        return encoder

    def forward(
        self, input: torch.Tensor, idx_keep: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Encode input tokens.

        Args:
            input:
                Batch of token sequences.
            idx_keep:
                Tensor with shape (batch_size, num_tokens_to_keep) where each
                entry is an index of the token to keep in the respective batch.
                If specified, only the indexed tokens will be encoded.

        Returns:
            Batch of encoded output tokens.
        """

        input = input + self.interpolate_pos_encoding(input)
        if idx_keep is not None:
            input = utils.apply_masks(input, idx_keep)
        return self.ln(self.layers(self.dropout(input)))

    def interpolate_pos_encoding(self, input: torch.Tensor):
        """Returns the interpolated positional embedding for the given input.

        This function interpolates self.pos_embedding for all tokens in the input,
        ignoring the class token. This allows encoding variable sized images.

        Args:
            input:
               Input tensor with shape (batch_size, num_sequences).

        Returns:
            Interpolated positional embedding.

        """

        # code copied from:
        # https://github.com/facebookresearch/msn/blob/4388dc1eadbe3042b85d3296d41b9b207656e043/src/deit.py#L291
        npatch = input.shape[1] - 1
        N = self.pos_embedding.shape[1] - 1
        if npatch == N:
            return self.pos_embedding
        class_emb = self.pos_embedding[:, 0]
        pos_embedding = self.pos_embedding[:, 1:]
        dim = input.shape[-1]
        pos_embedding = nn.functional.interpolate(
            pos_embedding.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(
                0, 3, 1, 2
            ),
            scale_factor=math.sqrt(npatch / N),
            mode="bicubic",
        )
        pos_embedding = pos_embedding.permute(0, 2, 3, 1).view(1, -1, dim)
        return torch.cat((class_emb.unsqueeze(0), pos_embedding), dim=1)


class IJEPABackbone(vision_transformer.VisionTransformer):
    """Encoder for the I-JEPA model [0].

    Experimental: Support for I-JEPA is experimental, there might be breaking changes
    in the future.

    Converts images into patches and encodes them. Code inspired by [1].

    Note that this implementation uses a learned positional embedding while [0]
    uses a fixed positional embedding.

    - [0]: Joint-Embedding Predictive Architecture, 2023, https://arxiv.org/abs/2301.08243
    - [1]: https://github.com/facebookresearch/ijepa

    Attributes:
        image_size:
            Input image size.
        patch_size:
            Width and height of the image patches. image_size must be a multiple
            of patch_size.
        num_layers:
            Number of transformer blocks.
        num_heads:
            Number of attention heads.
        hidden_dim:
            Dimension of the input and output tokens.
        mlp_dim:
            Dimension of the MLP in the transformer block.
        dropout:
            Percentage of elements set to zero after the MLP in the transformer.
        attention_dropout:
            Percentage of elements set to zero after the attention head.
        num_classes:
            Number of classes for the classification head. Currently not used.
        representation_size:
            If specified, an additional linear layer is added before the
            classification head to change the token dimension from hidden_dim
            to representation_size. Currently not used.
        norm_layer:
            Callable that creates a normalization layer.

    """

    def __init__(
        self,
        image_size: int,
        patch_size: int,
        num_layers: int,
        num_heads: int,
        hidden_dim: int,
        mlp_dim: int,
        dropout: float = 0,
        attention_dropout: float = 0,
        num_classes: int = 1000,
        representation_size: Optional[int] = None,
        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),
        conv_stem_configs: Optional[List[ConvStemConfig]] = None,
    ):
        super().__init__(
            image_size=image_size,
            patch_size=patch_size,
            num_layers=num_layers,
            num_heads=num_heads,
            hidden_dim=hidden_dim,
            mlp_dim=mlp_dim,
            dropout=dropout,
            attention_dropout=attention_dropout,
            num_classes=num_classes,
            representation_size=representation_size,
            norm_layer=norm_layer,
            conv_stem_configs=conv_stem_configs,
        )
        self.encoder = IJEPAEncoder(
            seq_length=self.seq_length,
            num_layers=num_layers,
            num_heads=num_heads,
            hidden_dim=hidden_dim,
            mlp_dim=mlp_dim,
            dropout=dropout,
            attention_dropout=attention_dropout,
            norm_layer=norm_layer,
        )

    @classmethod
    def from_vit(cls, vit: vision_transformer.VisionTransformer):
        """Creates a IJEPABackbone from a torchvision ViT model."""

        # Create a new instance with dummy values as they will be overwritten
        # by the copied vit_encoder attributes
        backbone = cls(
            image_size=vit.image_size,
            patch_size=vit.patch_size,
            num_layers=1,
            num_heads=1,
            hidden_dim=vit.hidden_dim,
            mlp_dim=vit.mlp_dim,
            dropout=vit.dropout,
            attention_dropout=vit.attention_dropout,
            num_classes=vit.num_classes,
            representation_size=vit.representation_size,
            norm_layer=vit.norm_layer,
        )

        # Copy attributes from the ViT model
        backbone.conv_proj = vit.conv_proj
        backbone.class_token = vit.class_token
        backbone.seq_length = vit.seq_length
        backbone.heads = vit.heads
        backbone.encoder = IJEPAEncoder.from_vit_encoder(vit.encoder)

        return backbone

    def forward(
        self, images: torch.Tensor, idx_keep: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Returns encoded class tokens from a batch of images.

        Args:
            images:
                Tensor with shape (batch_size, channels, image_size, image_size).
            idx_keep:
                Tensor with shape (batch_size, num_tokens_to_keep) where each
                entry is an index of the token to keep in the respective batch.
                If specified, only the indexed tokens will be passed to the
                encoder.

        Returns:
            Tensor with shape (batch_size, hidden_dim) containing the
            encoded class token for every image.
        """

        if idx_keep is not None:
            if not isinstance(idx_keep, list):
                idx_keep = [idx_keep]

        out = self.encode(images, idx_keep)
        return out

    def encode(
        self, images: torch.Tensor, idx_keep: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Returns encoded class and patch tokens from images.

        Args:
            images:
                Tensor with shape (batch_size, channels, image_size, image_size).
            idx_keep:
                Tensor with shape (batch_size, num_tokens_to_keep) where each
                entry is an index of the token to keep in the respective batch.
                If specified, only the indexed tokens will be passed to the
                encoder.

        Returns:
            Tensor with shape (batch_size, sequence_length, hidden_dim)
            containing the encoded class and patch tokens for every image.

        """
        out = self.images_to_tokens(images, prepend_class_token=True)
        return self.encoder(out, idx_keep)

    def images_to_tokens(
        self, images: torch.Tensor, prepend_class_token: bool
    ) -> torch.Tensor:
        """Converts images into patch tokens.

        Args:
            images:
                Tensor with shape (batch_size, channels, image_size, image_size).
            prepend_class_token:
                Whether to prepend the class token to the patch tokens.

        Returns:
            Tensor with shape (batch_size, sequence_length - 1, hidden_dim)
            containing the patch tokens.
        """
        x = self.conv_proj(images)
        tokens = x.flatten(2).transpose(1, 2)
        if prepend_class_token:
            tokens = utils.prepend_class_token(tokens, self.class_token)
        return tokens



================================================
FILE: lightly/models/modules/ijepa_timm.py
================================================
from __future__ import annotations

from functools import partial
from typing import Callable

import torch
import torch.nn as nn
from timm.models.vision_transformer import Block
from torch import Tensor

from lightly.models import utils


# Type ignore because superclass has Any types.
class IJEPAPredictorTIMM(nn.Module):  # type: ignore[misc]
    """Predictor for the I-JEPA model [0].

    Experimental: Support for I-JEPA is experimental, there might be breaking changes
    in the future.

    Predict patch embeddings. Code inspired by [1].

    - [0]: Joint-Embedding Predictive Architecture, 2023, https://arxiv.org/abs/2301.08243
    - [1]: https://github.com/facebookresearch/ijepa

    Attributes:
        num_patches:
            Number of patches (tokens), including the class token.
        depth:
            Number of transformer blocks.
        mlp_dim:
            Dimension of the MLP in the transformer block.
        predictor_embed_dim:
            Dimension of inner predicted patches(tokens).
        num_heads:
            Number of attention heads.
        qkv_bias:
            If True, add bias to the query, key, and value tensors.
        mlp_ratio:
            Ratio of mlp hidden dim to embedding dim.
        drop_path_rate:
            Drop paths (Stochastic Depth) per sample.
        proj_drop_rate:
            Percentage of elements set to zero after the MLP in the transformer.
        attn_drop_rate:
            Percentage of elements set to zero after the attention head.
        norm_layer:
            Normalization layer.
    """

    def __init__(
        self,
        num_patches: int,
        depth: int,
        mlp_dim: int,
        predictor_embed_dim: int,
        num_heads: int,
        qkv_bias: bool = True,
        mlp_ratio: float = 4.0,
        drop_path_rate: float = 0.0,
        proj_drop_rate: float = 0.0,
        attn_drop_rate: float = 0.0,
        norm_layer: Callable[..., nn.Module] = partial(nn.LayerNorm, eps=1e-6),
    ):
        """Initializes the IJEPAPredictorTIMM with the specified dimensions."""

        super().__init__()

        self.predictor_embed = nn.Linear(mlp_dim, predictor_embed_dim, bias=True)
        self.mask_token = nn.Parameter(torch.zeros(1, 1, predictor_embed_dim))
        self.predictor_proj = nn.Linear(predictor_embed_dim, mlp_dim, bias=True)
        self.predictor_norm = norm_layer(predictor_embed_dim)
        self.predictor_pos_embed = nn.Parameter(
            torch.zeros(1, num_patches, predictor_embed_dim), requires_grad=False
        )
        predictor_pos_embed = utils.get_2d_sincos_pos_embed(
            self.predictor_pos_embed.shape[-1], int(num_patches**0.5), cls_token=False
        )
        self.predictor_pos_embed.data.copy_(
            torch.from_numpy(predictor_pos_embed).float().unsqueeze(0)
        )

        # original implementation also has drop path rate
        self.predictor_blocks = nn.ModuleList(
            [
                Block(
                    dim=predictor_embed_dim,
                    num_heads=num_heads,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=qkv_bias,
                    drop_path=drop_path_rate,
                    proj_drop=proj_drop_rate,
                    attn_drop=attn_drop_rate,
                    norm_layer=norm_layer,
                )
                for _ in range(depth)
            ]
        )

    def forward(
        self,
        x: Tensor,
        masks_x: list[Tensor] | Tensor,
        masks: list[Tensor] | Tensor,
    ) -> Tensor:
        """Forward pass of the IJEPAPredictorTIMM.

        Args:
            x:
                Input tensor.
            masks_x:
                Mask indices for the input tensor.
            masks:
                Mask indices for the predicted tokens.

        Returns:
            The predicted output tensor.
        """

        assert (masks is not None) and (
            masks_x is not None
        ), "Cannot run predictor without mask indices"

        len_masks_x = len(masks_x) if isinstance(masks_x, list) else 1
        len_masks = len(masks) if isinstance(masks, list) else 1

        B = len(x) // len_masks_x
        x = self.predictor_embed(x)
        x_pos_embed = self.predictor_pos_embed.repeat(B, 1, 1)

        x += utils.apply_masks(x_pos_embed, masks_x)
        _, N_ctxt, _ = x.shape

        pos_embs = self.predictor_pos_embed.repeat(B, 1, 1)
        pos_embs = utils.apply_masks(pos_embs, masks)
        pos_embs = utils.repeat_interleave_batch(pos_embs, B, repeat=len_masks_x)
        pred_tokens = self.mask_token.repeat(pos_embs.size(0), pos_embs.size(1), 1)

        pred_tokens += pos_embs
        x = x.repeat(len_masks, 1, 1)
        x = torch.cat([x, pred_tokens], dim=1)

        for blk in self.predictor_blocks:
            x = blk(x)
        x = self.predictor_norm(x)

        x = x[:, N_ctxt:]
        x = self.predictor_proj(x)

        return x



================================================
FILE: lightly/models/modules/masked_autoencoder.py
================================================
from __future__ import annotations

import math
from functools import partial
from typing import Callable, List, Optional

import torch
import torch.nn as nn
from torch.nn import Linear, Module, Parameter

# vision_transformer requires torchvision >= 0.12
from torchvision.models import vision_transformer
from torchvision.models.vision_transformer import ConvStemConfig

from lightly.models import utils


class MAEEncoder(vision_transformer.Encoder):
    """Encoder for the Masked Autoencoder model [0].

    Encodes patch embeddings. Code inspired by [1].

    - [0]: Masked Autoencoder, 2021, https://arxiv.org/abs/2111.06377
    - [1]: https://github.com/facebookresearch/mae

    Attributes:
        seq_length:
            Token sequence length, including the class token.
        num_layers:
            Number of transformer blocks.
        num_heads:
            Number of attention heads.
        hidden_dim:
            Dimension of the input and output tokens.
        mlp_dim:
            Dimension of the MLP in the transformer block.
        dropout:
            Percentage of elements set to zero after the MLP in the transformer.
        attention_dropout:
            Percentage of elements set to zero after the attention head.
    """

    def __init__(
        self,
        seq_length: int,
        num_layers: int,
        num_heads: int,
        hidden_dim: int,
        mlp_dim: int,
        dropout: float,
        attention_dropout: float,
        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),
    ):
        """Initializes the MAEEncoder with the specified dimensions."""

        super().__init__(
            seq_length=seq_length,
            num_layers=num_layers,
            num_heads=num_heads,
            hidden_dim=hidden_dim,
            mlp_dim=mlp_dim,
            dropout=dropout,
            attention_dropout=attention_dropout,
            norm_layer=norm_layer,
        )
        self._initialize_weights()

    @classmethod
    def from_vit_encoder(
        cls, vit_encoder: vision_transformer.Encoder, initialize_weights: bool = True
    ) -> MAEEncoder:
        """Creates a MAEEncoder from a torchvision ViT encoder.

        Args:
            vit_encoder:
                A torchvision ViT encoder.
            initialize_weights:
                If True, then all weights are initialized as in MAE paper. Set this to
                False if vit_encoder is pretrained.

        Returns:
            A MAEEncoder with the same architecture as vit_encoder.
        """

        # Create a new instance with dummy values as they will be overwritten
        # by the copied vit_encoder attributes
        encoder = cls(
            seq_length=197,
            num_layers=12,
            num_heads=12,
            hidden_dim=768,
            mlp_dim=3072,
            dropout=0,
            attention_dropout=0,
        )

        # Copy attributes from the ViT encoder
        encoder.pos_embedding = vit_encoder.pos_embedding
        encoder.dropout = vit_encoder.dropout
        encoder.layers = vit_encoder.layers
        encoder.ln = vit_encoder.ln

        if initialize_weights:
            encoder._initialize_weights()
        return encoder

    def forward(
        self, input: torch.Tensor, idx_keep: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Encode input tokens.

        Args:
            input:
                Batch of token sequences.
            idx_keep:
                Tensor with shape (batch_size, num_tokens_to_keep) where each
                entry is an index of the token to keep in the respective batch.
                If specified, only the indexed tokens will be encoded.

        Returns:
            Batch of encoded output tokens.
        """
        input = input + self.interpolate_pos_encoding(input)
        if idx_keep is not None:
            input = utils.get_at_index(input, idx_keep)
        return self.ln(self.layers(self.dropout(input)))

    def interpolate_pos_encoding(self, input: torch.Tensor):
        """Returns the interpolated positional embedding for the given input.

        This function interpolates self.pos_embedding for all tokens in the input,
        ignoring the class token. This allows encoding variable sized images.

        Args:
            input:
               Input tensor with shape (batch_size, num_sequences).

        Returns:
            Interpolated positional embedding.

        """
        # code copied from:
        # https://github.com/facebookresearch/msn/blob/4388dc1eadbe3042b85d3296d41b9b207656e043/src/deit.py#L291
        npatch = input.shape[1] - 1
        N = self.pos_embedding.shape[1] - 1
        if npatch == N:
            return self.pos_embedding

        # Separate the class embedding from the positional embeddings
        class_emb = self.pos_embedding[:, 0]
        pos_embedding = self.pos_embedding[:, 1:]
        dim = input.shape[-1]

        pos_embedding = nn.functional.interpolate(
            pos_embedding.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(
                0, 3, 1, 2
            ),
            scale_factor=math.sqrt(npatch / N),
            mode="bicubic",
        )
        pos_embedding = pos_embedding.permute(0, 2, 3, 1).view(1, -1, dim)
        return torch.cat((class_emb.unsqueeze(0), pos_embedding), dim=1)

    def _initialize_weights(self) -> None:
        _initialize_2d_sine_cosine_positional_embedding(self.pos_embedding)
        _initialize_linear_layers(self)


class MAEBackbone(vision_transformer.VisionTransformer):
    """Backbone for the Masked Autoencoder model [0].

    Converts images into patches and encodes them. Code inspired by [1].
    Note that this implementation uses a learned positional embedding while [0]
    uses a fixed positional embedding.

    - [0]: Masked Autoencoder, 2021, https://arxiv.org/abs/2111.06377
    - [1]: https://github.com/facebookresearch/mae
    - [2]: Early Convolutions Help Transformers See Better, 2021, https://arxiv.org/abs/2106.14881.

    Attributes:
        image_size:
            Input image size.
        patch_size:
            Width and height of the image patches. image_size must be a multiple
            of patch_size.
        num_layers:
            Number of transformer blocks.
        num_heads:
            Number of attention heads.
        hidden_dim:
            Dimension of the input and output tokens.
        mlp_dim:
            Dimension of the MLP in the transformer block.
        dropout:
            Percentage of elements set to zero after the MLP in the transformer.
        attention_dropout:
            Percentage of elements set to zero after the attention head.
        num_classes:
            Number of classes for the classification head. Currently not used.
        representation_size:
            If specified, an additional linear layer is added before the
            classification head to change the token dimension from hidden_dim
            to representation_size. Currently not used.
        norm_layer:
            Callable that creates a normalization layer.
        conv_stem_configs:
            If specified, a convolutional stem is added at the beggining of the
            network following [2]. Not used in the original Masked Autoencoder
            paper [0].

    """

    def __init__(
        self,
        image_size: int,
        patch_size: int,
        num_layers: int,
        num_heads: int,
        hidden_dim: int,
        mlp_dim: int,
        dropout: float = 0,
        attention_dropout: float = 0,
        num_classes: int = 1000,
        representation_size: Optional[int] = None,
        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),
        conv_stem_configs: Optional[List[ConvStemConfig]] = None,
    ):
        """Initializes the MAEBackbone with the specified dimensions."""

        super().__init__(
            image_size=image_size,
            patch_size=patch_size,
            num_layers=num_layers,
            num_heads=num_heads,
            hidden_dim=hidden_dim,
            mlp_dim=mlp_dim,
            dropout=dropout,
            attention_dropout=attention_dropout,
            num_classes=num_classes,
            representation_size=representation_size,
            norm_layer=norm_layer,
            conv_stem_configs=conv_stem_configs,
        )
        self.encoder = MAEEncoder(
            seq_length=self.seq_length,
            num_layers=num_layers,
            num_heads=num_heads,
            hidden_dim=hidden_dim,
            mlp_dim=mlp_dim,
            dropout=dropout,
            attention_dropout=attention_dropout,
            norm_layer=norm_layer,
        )

    @classmethod
    def from_vit(
        cls, vit: vision_transformer.VisionTransformer, initialize_weights: bool = True
    ) -> MAEBackbone:
        """Creates a MAEBackbone from a torchvision ViT model.

        Args:
            vit:
                A torchvision ViT model.
            initialize_weights:
                If True, then all weights are initialized as in MAE paper. Set this to
                False if vit is pretrained.

        Returns:
            A MAEBackbone with the same architecture as vit.

        """
        # Create a new instance with dummy values as they will be overwritten
        # by the copied vit_encoder attributes
        backbone = cls(
            image_size=vit.image_size,
            patch_size=vit.patch_size,
            num_layers=1,
            num_heads=1,
            hidden_dim=vit.hidden_dim,
            mlp_dim=vit.mlp_dim,
            dropout=vit.dropout,
            attention_dropout=vit.attention_dropout,
            num_classes=vit.num_classes,
            representation_size=vit.representation_size,
            norm_layer=vit.norm_layer,
        )

        # Copy attributes from the ViT model
        backbone.conv_proj = vit.conv_proj
        backbone.class_token = vit.class_token
        backbone.seq_length = vit.seq_length
        backbone.heads = vit.heads
        backbone.encoder = MAEEncoder.from_vit_encoder(
            vit.encoder, initialize_weights=initialize_weights
        )
        return backbone

    def forward(
        self, images: torch.Tensor, idx_keep: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Returns encoded class tokens from a batch of images.

        Args:
            images:
                Tensor with shape (batch_size, channels, image_size, image_size).
            idx_keep:
                Tensor with shape (batch_size, num_tokens_to_keep) where each
                entry is an index of the token to keep in the respective batch.
                If specified, only the indexed tokens will be passed to the
                encoder.

        Returns:
            Tensor with shape (batch_size, hidden_dim) containing the
            encoded class token for every image.

        """
        out = self.encode(images, idx_keep)
        class_token = out[:, 0]
        return class_token

    def encode(
        self, images: torch.Tensor, idx_keep: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Returns encoded class and patch tokens from images.

        Args:
            images:
                Tensor with shape (batch_size, channels, image_size, image_size).
            idx_keep:
                Tensor with shape (batch_size, num_tokens_to_keep) where each
                entry is an index of the token to keep in the respective batch.
                If specified, only the indexed tokens will be passed to the
                encoder.

        Returns:
            Tensor with shape (batch_size, sequence_length, hidden_dim)
            containing the encoded class and patch tokens for every image.

        """
        out = self.images_to_tokens(images, prepend_class_token=True)
        return self.encoder(out, idx_keep)

    def images_to_tokens(
        self, images: torch.Tensor, prepend_class_token: bool
    ) -> torch.Tensor:
        """Converts images into patch tokens.

        Args:
            images:
                Tensor with shape (batch_size, channels, image_size, image_size).
            prepend_class_token:
                Whether to prepend the class token to the patch tokens.

        Returns:
            Tensor with shape (batch_size, sequence_length - 1, hidden_dim)
            containing the patch tokens.
        """

        x = self.conv_proj(images)
        tokens = x.flatten(2).transpose(1, 2)
        if prepend_class_token:
            tokens = utils.prepend_class_token(tokens, self.class_token)
        return tokens

    def _initialize_weights(self) -> None:
        """Initializes weights for the backbone components."""

        # Initialize the patch embedding layer like a linear layer instead of conv
        # layer.
        w = self.conv_proj.weight.data
        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))

        # Initialize the class token.
        torch.nn.init.normal_(self.class_token, std=0.02)

        self.encoder._initialize_weights()
        _initialize_linear_layers(self)


class MAEDecoder(vision_transformer.Encoder):
    """Decoder for the Masked Autoencoder model [0].

    Decodes encoded patches and predicts pixel values for every patch.
    Code inspired by [1].

    - [0]: Masked Autoencoder, 2021, https://arxiv.org/abs/2111.06377
    - [1]: https://github.com/facebookresearch/mae

    Attributes:
        seq_length:
            Token sequence length, including the class token.
        num_layers:
            Number of transformer blocks.
        num_heads:
            Number of attention heads.
        embed_input_dim:
            Dimension of the input tokens. Usually be equal to the hidden
            dimension of the MAEEncoder or MAEBackbone.
        hidden_dim:
            Dimension of the decoder tokens.
        mlp_dim:
            Dimension of the MLP in the transformer block.
        out_dim:
            Output dimension of the prediction for a single patch. Usually equal
            to (3 * patch_size ** 2).
        dropout:
            Percentage of elements set to zero after the MLP in the transformer.
        attention_dropout:
            Percentage of elements set to zero after the attention head.

    """

    def __init__(
        self,
        seq_length: int,
        num_layers: int,
        num_heads: int,
        embed_input_dim: int,
        hidden_dim: int,
        mlp_dim: int,
        out_dim: int,
        dropout: float = 0.0,
        attention_dropout: float = 0.0,
        norm_layer: Callable[..., nn.Module] = partial(nn.LayerNorm, eps=1e-6),
    ):
        """Initializes the MAEDecoder with the specified dimensions."""

        super().__init__(
            seq_length=seq_length,
            num_layers=num_layers,
            num_heads=num_heads,
            hidden_dim=hidden_dim,
            mlp_dim=mlp_dim,
            dropout=dropout,
            attention_dropout=attention_dropout,
            norm_layer=norm_layer,
        )
        self.decoder_embed = nn.Linear(embed_input_dim, hidden_dim, bias=True)
        self.prediction_head = nn.Linear(hidden_dim, out_dim)
        self._initialize_weights()

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        """Returns predicted pixel values from encoded tokens.

        Args:
            input:
                Tensor with shape (batch_size, seq_length, embed_input_dim).

        Returns:
            Tensor with shape (batch_size, seq_length, out_dim).
        """

        out = self.embed(input)
        out = self.decode(out)
        return self.predict(out)

    def embed(self, input: torch.Tensor) -> torch.Tensor:
        """Embeds encoded input tokens into decoder token dimension.

        This is a single linear layer that changes the token dimension from
        embed_input_dim to hidden_dim.

        Args:
            input:
                Tensor with shape (batch_size, seq_length, embed_input_dim)
                containing the encoded tokens.

        Returns:
            Tensor with shape (batch_size, seq_length, hidden_dim) containing
            the embedded tokens.

        """
        return self.decoder_embed(input)

    def decode(self, input: torch.Tensor) -> torch.Tensor:
        """Forward pass through the decoder transformer.

        Args:
            input:
                Tensor with shape (batch_size, seq_length, hidden_dim) containing
                the encoded tokens.

        Returns:
            Tensor with shape (batch_size, seq_length, hidden_dim) containing
            the decoded tokens.

        """
        return super().forward(input)

    def predict(self, input: torch.Tensor) -> torch.Tensor:
        """Predics pixel values from decoded tokens.

        Args:
            input:
                Tensor with shape (batch_size, seq_length, hidden_dim) containing
                the decoded tokens.

        Returns:
            Tensor with shape (batch_size, seq_length, out_dim) containing
            predictions for each token.

        """
        return self.prediction_head(input)

    def _initialize_weights(self) -> None:
        _initialize_2d_sine_cosine_positional_embedding(self.pos_embedding)
        _initialize_linear_layers(self)


def _initialize_2d_sine_cosine_positional_embedding(pos_embedding: Parameter) -> None:
    """Initializes a 2D sine-cosine positional embedding."""

    _, seq_length, hidden_dim = pos_embedding.shape
    grid_size = int((seq_length - 1) ** 0.5)
    sine_cosine_embedding = utils.get_2d_sine_cosine_positional_embedding(
        embed_dim=hidden_dim,
        grid_size=grid_size,
        cls_token=True,
    )
    pos_embedding.data.copy_(
        torch.from_numpy(sine_cosine_embedding).float().unsqueeze(0)
    )
    # Freeze positional embedding.
    pos_embedding.requires_grad = False


def _initialize_linear_layers(module: Module) -> None:
    """Initializes linear layers in the given module."""

    def init(mod: Module) -> None:
        if isinstance(mod, Linear):
            nn.init.xavier_uniform_(mod.weight)
            if mod.bias is not None:
                nn.init.constant_(mod.bias, 0)

    module.apply(init)



================================================
FILE: lightly/models/modules/masked_autoencoder_timm.py
================================================
from functools import partial
from typing import Callable, Optional

import torch
import torch.nn as nn
from timm.models.vision_transformer import Block
from torch import Tensor
from torch.nn import LayerNorm, Module, Parameter, Sequential

from lightly.models import utils
from lightly.models.modules.masked_vision_transformer_timm import init_weights


class MAEDecoderTIMM(Module):
    """Decoder for the Masked Autoencoder model [0].

    Decodes encoded patches and predicts pixel values for every patch.
    Code inspired by [1].

    - [0]: Masked Autoencoder, 2021, https://arxiv.org/abs/2111.06377
    - [1]: https://github.com/facebookresearch/mae

    Attributes:
        num_patches:
            Number of patches.
        patch_size:
            Patch size.
        in_chans:
            Number of image input channels.
        embed_dim:
            Embedding dimension of the encoder.
        decoder_embed_dim:
            Embedding dimension of the decoder.
        decoder_depth:
            Depth of transformer.
        decoder_num_heads:
            Number of attention heads.
        mlp_ratio:
            Ratio of mlp hidden dim to embedding dim.
        proj_drop_rate:
            Percentage of elements set to zero after the MLP in the transformer.
        attn_drop_rate:
            Percentage of elements set to zero after the attention head.
        norm_layer:
            Normalization layer.
        initialize_weights:
            Flag that determines if weights should be initialized.
        mask_token:
            The mask token.

    """

    def __init__(
        self,
        num_patches: int,
        patch_size: int,
        in_chans: int = 3,
        embed_dim: int = 1024,
        decoder_embed_dim: int = 512,
        decoder_depth: int = 8,
        decoder_num_heads: int = 16,
        mlp_ratio: float = 4.0,
        proj_drop_rate: float = 0.0,
        attn_drop_rate: float = 0.0,
        norm_layer: Callable[..., nn.Module] = partial(LayerNorm, eps=1e-6),
        initialize_weights: bool = True,
        mask_token: Optional[Parameter] = None,
    ):
        """Initializes the MAEDecoderTIMM with the specified parameters."""

        super().__init__()

        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)
        self.mask_token = (
            nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))
            if mask_token is None
            else mask_token
        )

        # Positional encoding of the decoder
        self.decoder_pos_embed = nn.Parameter(
            torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False
        )  # fixed sin-cos embedding

        self.decoder_blocks = Sequential(
            *[
                Block(
                    decoder_embed_dim,
                    decoder_num_heads,
                    mlp_ratio,
                    qkv_bias=True,
                    norm_layer=norm_layer,
                    proj_drop=proj_drop_rate,
                    attn_drop=attn_drop_rate,
                )
                for i in range(decoder_depth)
            ]
        )

        self.decoder_norm = norm_layer(decoder_embed_dim)
        self.decoder_pred = nn.Linear(
            decoder_embed_dim, patch_size**2 * in_chans, bias=True
        )  # decoder to patch

        if initialize_weights:
            self._initialize_weights()

    def forward(self, input: Tensor) -> Tensor:
        """Returns predicted pixel values from encoded tokens.

        Args:
            input:
                Tensor with shape (batch_size, seq_length, embed_input_dim).

        Returns:
            Tensor with shape (batch_size, seq_length, out_dim).
        """

        out = self.embed(input)
        out = self.decode(out)
        return self.predict(out)

    def embed(self, input: Tensor) -> Tensor:
        """Embeds encoded input tokens into decoder token dimension.

        This is a single linear layer that changes the token dimension from
        embed_input_dim to hidden_dim.

        Args:
            input:
                Tensor with shape (batch_size, seq_length, embed_input_dim)
                containing the encoded tokens.

        Returns:
            Tensor with shape (batch_size, seq_length, hidden_dim) containing
            the embedded tokens.

        """
        out: Tensor = self.decoder_embed(input)
        return out

    def decode(self, input: Tensor) -> Tensor:
        """Forward pass through the decoder transformer.

        Args:
            input:
                Tensor with shape (batch_size, seq_length, hidden_dim) containing
                the encoded tokens.

        Returns:
            Tensor with shape (batch_size, seq_length, hidden_dim) containing
            the decoded tokens.

        """
        output: Tensor = input + self.decoder_pos_embed
        output = self.decoder_blocks(output)
        output = self.decoder_norm(output)
        return output

    def predict(self, input: Tensor) -> Tensor:
        """Predicts pixel values from decoded tokens.

        Args:
            input:
                Tensor with shape (batch_size, seq_length, hidden_dim) containing
                the decoded tokens.

        Returns:
            Tensor with shape (batch_size, seq_length, out_dim) containing
            predictions for each token.

        """
        out: Tensor = self.decoder_pred(input)
        return out

    def _initialize_weights(self) -> None:
        """Initializes weights for the decoder components."""

        torch.nn.init.normal_(self.mask_token, std=0.02)
        utils.initialize_2d_sine_cosine_positional_embedding(
            pos_embedding=self.decoder_pos_embed, has_class_token=True
        )
        self.apply(init_weights)



================================================
FILE: lightly/models/modules/masked_causal_vision_transformer.py
================================================
from typing import Optional

import torch
import torch.nn.functional as F
from timm.models import _manipulate
from timm.models.vision_transformer import Attention, Block, VisionTransformer
from torch import Tensor, jit


# Type ignore because superclass has Any types.
class MaskedCausalAttention(Attention):  # type: ignore[misc]
    """Identical to timm.models.vision_transformer.Attention, but supports causal
    attention with masking. Please check the source code of the
    timm.models.vision_transformer.Attention class for input parameters.

    The implementation is based on AIM [0].

    - [0]: AIM, 2024, https://arxiv.org/abs/2401.08541
    """

    def forward(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:
        """Forward pass of the attention layer.

        Args:
            x:
                Input tensor of shape (batch_size, sequence_length, channels).
            mask:
                Mask of shape (batch_size, sequence_length) indicating which tokens
                should be masked. Tokens where the mask is True will only be used for
                causal attention, while unmasked tokens are used for bidirectional
                attention. If the mask is None, all tokens are used for bidirectional
                attention.
        """
        B, N, C = x.shape
        attn_mask = self._get_attention_mask(x, mask=mask)
        qkv = (
            self.qkv(x)
            .reshape(B, N, 3, self.num_heads, self.head_dim)
            .permute(2, 0, 3, 1, 4)
        )
        q, k, v = qkv.unbind(0)
        q, k = self.q_norm(q), self.k_norm(k)

        if self.fused_attn:
            # Type ignore because only new torch versions support this.
            x = F.scaled_dot_product_attention(  # type: ignore[attr-defined]
                q,
                k,
                v,
                attn_mask=attn_mask,
                dropout_p=self.attn_drop.p if self.training else 0.0,
            )
        else:
            assert False, "Only fused attention is supported for now."
            # TODO: Implement non-fused attention.
            q = q * self.scale  # type: ignore[unreachable]
            attn = q @ k.transpose(-2, -1)
            attn = attn.softmax(dim=-1)
            attn = self.attn_drop(attn)
            x = attn @ v

        x = x.transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

    def _get_attention_mask(
        self, x: Tensor, mask: Optional[Tensor]
    ) -> Optional[Tensor]:
        """Generates an attention mask for causal attention.

        Args:
            x:
                Input tensor of shape (batch_size, sequence_length, channels).
            mask:
                Mask tensor of shape (batch_size, sequence_length) indicating which tokens
                should be masked.

        Returns:
            Attention mask of shape (batch_size, 1, sequence_length, sequence_length).
        """
        B, N = x.shape[:2]

        # Only apply causal attention if mask is not None. This is a bit hacky, but it
        # allows us to use bidirectional instead of causal attention during evaluation
        # and fine-tuning.
        attn_mask = None
        if mask is not None:
            attn_mask = x.new_ones(size=(B, N, N), dtype=torch.bool).tril(diagonal=0)
            # mask has shape (B, N)
            mask = (~mask).unsqueeze(1).expand(B, N, N).bool()
            attn_mask = torch.logical_or(attn_mask, mask)
            attn_mask = attn_mask.unsqueeze(1)  # (B, 1, N, N)
        return attn_mask


# Type ignore because superclass has Any types.
class MaskedCausalBlock(Block):  # type: ignore[misc]
    """Identical to timm.models.vision_transformer.Block, but uses PrefixCausalAttention
    instead of Attention. Please check the source code of the timm.models.vision_transformer.Block
    class for input parameters.

    The implementation is based on AIM [0].

    - [0]: AIM, 2024, https://arxiv.org/abs/2401.08541
    """

    def __init__(  # type: ignore[no-untyped-def]
        self,
        *args,
        **kwargs,
    ) -> None:
        """Initializes the MaskedCausalBlock with the specified parameters. Please check the source code of the timm.models.vision_transformer.Block
        class for input parameters.

        """
        super().__init__(
            *args,
            **kwargs,
        )
        self.attn = MaskedCausalAttention(
            *args,
            **kwargs,
        )

    def forward(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:
        """Forward pass of the attention block.

        Args:
            x:
                Input tensor of shape (batch_size, sequence_length, channels).
            mask:
                Mask of shape (batch_size, sequence_length) indicating which tokens
                should be masked. Tokens where the mask is True will only be used for
                causal attention, while unmasked tokens are used for bidirectional
                attention. If the mask is None, all tokens are used for bidirectional
                attention.

        Returns:
            Output tensor after applying the attention block.
        """
        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), mask=mask)))
        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
        return x


# Type ignore because superclass has Any types.
class MaskedCausalVisionTransformer(VisionTransformer):  # type: ignore[misc]
    """Vision transformer with masked causal attention based on AIM [0]. Please check the source code of the timm.models.vision_transformer.VisionTransformer
    class for input parameters.

    - [0]: AIM, 2024, https://arxiv.org/abs/2401.08541
    """

    def __init__(  # type: ignore[no-untyped-def]
        self,
        *args,
        **kwargs,
    ) -> None:
        """Initializes the MaskedCausalVisionTransformer with the specified parameters. Please check the source code of the timm.models.vision_transformer.VisionTransformer
        class for input parameters.

        """
        kwargs.setdefault("block_fn", MaskedCausalBlock)
        super().__init__(
            *args,
            **kwargs,
        )

    def forward_features(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:
        """Forward pass of the model without the classification head.

        Args:
            x:
                Input tensor of shape (batch_size, sequence_length, channels).
            mask:
                Mask of shape (batch_size, sequence_length) indicating which tokens
                should be masked. Tokens where the mask is True will only be used for
                causal attention, while unmasked tokens are used for bidirectional
                attention. If the mask is None, all tokens are used for bidirectional
                attention.

        Returns:
            Output tensor after applying the transformer blocks.
        """
        x = self.patch_embed(x)
        x = self._pos_embed(x)
        x = self.patch_drop(x)
        x = self.norm_pre(x)
        if self.grad_checkpointing and not jit.is_scripting():
            # TODO: This probably doesn't work correctly as it doesn't consider the
            # mask.
            x = _manipulate.checkpoint_seq(self.blocks, x)
        else:
            for block in self.blocks:
                x = block(x, mask=mask)
        x = self.norm(x)
        return x



================================================
FILE: lightly/models/modules/masked_vision_transformer.py
================================================
from abc import ABC, abstractmethod
from typing import List, Optional, Tuple

from torch import Tensor
from torch.nn import Module, Parameter

from lightly.models import utils


class MaskedVisionTransformer(ABC, Module):
    """
    Abstract base class for Masked Vision Transformer models.

    Defines the interface for a Masked Vision Transformer. This class includes abstract
    methods that must be implemented by concrete subclasses to define the forward pass,
    tokenization of images, and various operations needed for the transformer.
    """

    # This is not defined as a property for backwards compatibility.
    # New models should define this as a property.
    mask_token: Parameter

    @property
    @abstractmethod
    def sequence_length(self) -> int:
        ...

    @abstractmethod
    def forward(
        self,
        images: Tensor,
        idx_mask: Optional[Tensor] = None,
        idx_keep: Optional[Tensor] = None,
        mask: Optional[Tensor] = None,
    ) -> Tensor:
        """Returns encoded class tokens from a batch of images.

        Args:
            images:
                Tensor with shape (batch_size, channels, image_size, image_size).
            idx_mask:
                Tensor with shape (batch_size, num_tokens_to_mask) where each
                entry is an index of the token to mask in the respective batch.
                Indices must be in the range [0, sequence_length).
                If set, the indexed tokens are masked with self.mask_token.
                Cannot be used in combination with mask argument.
            idx_keep:
                Tensor with shape (batch_size, num_tokens_to_keep) where each
                entry is an index of the token to keep in the respective batch.
                Indices must be in the range [0, sequence_length).
                If set, only the indexed tokens will be forwarded.
                Is applied after any masking operation.
            mask:
                Boolean tensor with shape (batch_size, sequence_length) indicating
                which tokens should be masked. Tokens where the mask is True will be
                replaced with the mask token.
                Cannot be used in combination with idx_mask argument.

        Returns:
            Tensor with shape (batch_size, embed_dim) containing the encoded class token
            for every image.

        """
        ...

    @abstractmethod
    def forward_intermediates(
        self,
        images: Tensor,
        idx_mask: Optional[Tensor] = None,
        idx_keep: Optional[Tensor] = None,
        norm: bool = False,
        mask: Optional[Tensor] = None,
    ) -> Tuple[Tensor, List[Tensor]]:
        """Encode input images and return features from the intermediate layers.

        Args:
            images:
                Tensor with shape (batch_size, channels, image_height, image_width).
            idx_mask:
                Tensor with shape (batch_size, num_tokens_to_mask) where each
                entry is an index of the token to mask in the respective batch.
                Indices must be in the range [0, sequence_length).
                If specified, the indexed tokens are masked with self.mask_token.
                Cannot be used in combination with mask argument.
            idx_keep:
                Tensor with shape (batch_size, num_tokens_to_keep) where each
                entry is an index of the token to keep in the respective batch.
                Indices must be in the range [0, sequence_length).
                If set, only the indexed tokens will be forwarded.
                Is applied after any masking operation.
            norm:
                Apply norm layer to all intermediates.
            mask:
                Boolean tensor with shape (batch_size, sequence_length) indicating
                which tokens should be masked. Tokens where the mask is True will be
                replaced with the mask token.
                Cannot be used in combination with idx_mask argument.

        Returns:
            Tuple of batch of encoded output tokens and a list of intermediate features.
            The encoded output tokens have shape (batch_size, embed_dim) and each
            intermediate feature has shape (batch_size, sequence_length, embed_dim).
            If idx_keep is set, only num_tokens_to_keep tokens per sequence are
            returned.
        """
        ...

    @abstractmethod
    def encode(
        self,
        images: Tensor,
        idx_mask: Optional[Tensor] = None,
        idx_keep: Optional[Tensor] = None,
        mask: Optional[Tensor] = None,
    ) -> Tensor:
        """Encode input images.

        Args:
            images:
                Tensor with shape (batch_size, channels, image_height, image_width).
            idx_mask:
                Tensor with shape (batch_size, num_tokens_to_mask) where each
                entry is an index of the token to mask in the respective batch.
                Indices must be in the range [0, sequence_length).
                If specified, the indexed tokens are masked with self.mask_token.
                Cannot be used in combination with mask argument.
            idx_keep:
                Tensor with shape (batch_size, num_tokens_to_keep) where each
                entry is an index of the token to keep in the respective batch.
                Indices must be in the range [0, sequence_length).
                If set, only the indexed tokens will be encoded.
                Is applied after any masking operation.
            mask:
                Boolean tensor with shape (batch_size, sequence_length) indicating
                which tokens should be masked. Tokens where the mask is True will be
                replaced with the mask token.
                Cannot be used in combination with idx_mask argument.

        Returns:
            Tensor with shape (batch_size, sequence_length, embed_dim) containing the
            encoded output tokens. If idx_keep is set, only num_tokens_to_keep tokens
            per sequence are returned.
        """
        ...

    def preprocess(
        self,
        images: Tensor,
        idx_mask: Optional[Tensor] = None,
        idx_keep: Optional[Tensor] = None,
        mask: Optional[Tensor] = None,
    ) -> Tensor:
        """Convert images to tokens, add positional embeddings, and apply masking.

        Args:
            images:
                Tensor with shape (batch_size, channels, image_height, image_width).
            idx_mask:
                Tensor with shape (batch_size, num_tokens_to_mask) where each
                entry is an index of the token to mask in the respective batch.
                Indices must be in the range [0, sequence_length).
                If specified, the indexed tokens are masked with self.mask_token.
                Cannot be used in combination with mask argument.
            idx_keep:
                Tensor with shape (batch_size, num_tokens_to_keep) where each
                entry is an index of the token to keep in the respective batch.
                Indices must be in the range [0, sequence_length).
                If set, only the indexed tokens will be returned.
                Is applied after any masking operation.
            mask:
                Tensor with shape (batch_size, sequence_length) indicating which tokens
                should be masked. Tokens where the mask is True will be masked with
                self.mask_token.

        Returns:
            Tensor with shape (batch_size, sequence_length, embed_dim) containing the
            preprocessed tokens. If idx_keep is set, only num_tokens_to_keep tokens
            per sequence are returned. Any class or prefix tokens are prepended to the
            sequence.
        """
        if idx_mask is not None and mask is not None:
            raise ValueError("idx_mask and mask cannot both be set at the same time.")

        # convert images to tokens
        tokens = self.images_to_tokens(images)
        # add prefix tokens if needed
        tokens = self.prepend_prefix_tokens(tokens)

        if idx_mask is not None:
            tokens = utils.mask_at_index(
                tokens=tokens, index=idx_mask, mask_token=self.mask_token
            )
        elif mask is not None:
            tokens = utils.mask_bool(
                tokens=tokens, mask=mask, mask_token=self.mask_token
            )

        # add positional encoding
        tokens = self.add_pos_embed(tokens)

        if idx_keep is not None:
            tokens = utils.get_at_index(tokens, idx_keep)

        return tokens

    @abstractmethod
    def images_to_tokens(self, images: Tensor) -> Tensor:
        """Converts images into patch tokens.

        Args:
            images:
                Tensor with shape (batch_size, channels, image_height, image_width).

        Returns:
            Tensor with shape (batch_size, num_patches, embed_dim) containing the
            patch tokens (excluding prefix tokens).
        """
        ...

    # Keep for backwards compatibility.
    def add_prefix_tokens(self, x: Tensor) -> Tensor:
        return self.prepend_prefix_tokens(x)

    @abstractmethod
    def prepend_prefix_tokens(self, x: Tensor) -> Tensor:
        """Prepends prefix tokens to the input patch tokens.

        Args:
            x:
                Tensor with shape (batch_size, num_patches, embed_dim) containing patch
                tokens.

        Returns:
            Tensor with shape (batch_size, sequence_length, embed_dim) containing
            the prefix and patch tokens. The prefix tokens are prepended to the
            sequence.
        """
        ...

    @abstractmethod
    def add_pos_embed(self, x: Tensor) -> Tensor:
        """Adds positional embeddings to the input tokens.

        Args:
            x:
                Tensor with shape (batch_size, sequence_length, embed_dim) containing
                the input tokens. Must include prefix tokens.

        Returns:
            Tensor after adding positional embeddings, with the same shape as the input.
        """
        ...



================================================
FILE: lightly/models/modules/masked_vision_transformer_timm.py
================================================
import math
from typing import List, Optional, Tuple

import torch
import torch.nn as nn
from timm.layers.pos_embed import resample_abs_pos_embed
from timm.models.vision_transformer import VisionTransformer
from torch import Tensor
from torch.nn import LayerNorm, Linear, Module, Parameter

from lightly.models import utils
from lightly.models.modules.masked_vision_transformer import MaskedVisionTransformer


class MaskedVisionTransformerTIMM(MaskedVisionTransformer):
    """Masked Vision Transformer class using TIMM.

    Attributes:
        vit:
            The VisionTransformer object of TIMM.
        mask_token:
            The mask token.
        weight_initialization:
            The weight initialization method. Valid options are ['', 'skip']. '' uses
            the default MAE weight initialization and 'skip' skips the weight
            initialization.
        antialias:
            Whether to use antialiasing when resampling the positional embeddings.
        pos_embed_initialization:
            The strategy to initialize the positional embeddings. Valid options are
            ['learn', 'sincos', 'skip'].

    """

    def __init__(
        self,
        vit: VisionTransformer,
        mask_token: Optional[Parameter] = None,
        weight_initialization: str = "",
        antialias: bool = True,
        pos_embed_initialization: str = "sincos",
    ) -> None:
        super().__init__()
        self.vit = vit
        self.mask_token = (
            mask_token
            if mask_token is not None
            else Parameter(torch.zeros(1, 1, self.vit.embed_dim))
        )

        if weight_initialization not in ("", "skip"):
            raise ValueError(
                f"Invalid weight initialization method: '{weight_initialization}'. "
                "Valid options are: ['', 'skip']."
            )
        if weight_initialization != "skip":
            self._initialize_weights()

        utils.initialize_positional_embedding(
            pos_embedding=self.vit.pos_embed,
            strategy=pos_embed_initialization,
            num_prefix_tokens=self.vit.num_prefix_tokens,
        )

        self.antialias = antialias

    @property
    def sequence_length(self) -> int:
        seq_len: int = self.vit.patch_embed.num_patches + self.vit.num_prefix_tokens
        return seq_len

    def forward(
        self,
        images: Tensor,
        idx_mask: Optional[Tensor] = None,
        idx_keep: Optional[Tensor] = None,
        mask: Optional[Tensor] = None,
    ) -> Tensor:
        x = self.encode(images, idx_mask=idx_mask, idx_keep=idx_keep, mask=mask)
        if self.vit.attn_pool is not None:
            x = self.vit.attn_pool(x)
        elif self.vit.global_pool == "avg":
            x = x[:, self.vit.num_prefix_tokens :].mean(dim=1)
        elif self.vit.global_pool:
            x = x[:, 0]  # class token
        return x

    def forward_intermediates(
        self,
        images: Tensor,
        idx_mask: Optional[Tensor] = None,
        idx_keep: Optional[Tensor] = None,
        norm: bool = False,
        mask: Optional[Tensor] = None,
    ) -> Tuple[Tensor, List[Tensor]]:
        # preprocess images, convert to tokens and add positional embeddings
        tokens = self.preprocess(
            images=images, idx_mask=idx_mask, idx_keep=idx_keep, mask=mask
        )
        # normalization layer
        tokens = self.vit.norm_pre(tokens)

        intermediates: List[Tensor] = []
        for blk in self.vit.blocks:
            tokens = blk(tokens)
            intermediates.append(self.vit.norm(tokens) if norm else tokens)

        # normalize
        out: Tensor = self.vit.norm(tokens)

        return out, intermediates

    def encode(
        self,
        images: Tensor,
        idx_mask: Optional[Tensor] = None,
        idx_keep: Optional[Tensor] = None,
        mask: Optional[Tensor] = None,
    ) -> Tensor:
        # preprocess images, convert to tokens and add positional embeddings
        tokens: Tensor = self.preprocess(
            images=images, idx_mask=idx_mask, idx_keep=idx_keep, mask=mask
        )
        # normalization layer
        tokens = self.vit.norm_pre(tokens)
        # apply Transformer blocks
        tokens = self.vit.blocks(tokens)
        # normalize
        tokens = self.vit.norm(tokens)
        return tokens

    def images_to_tokens(self, images: Tensor) -> Tensor:
        tokens: Tensor = self.vit.patch_embed(images)
        if self.vit.dynamic_img_size:
            tokens = tokens.permute(0, 3, 1, 2)  # NHWC -> NCHW
            tokens = tokens.flatten(2).transpose(1, 2)  # NCHW -> NLC
        return tokens

    def prepend_prefix_tokens(self, x: Tensor) -> Tensor:
        prefix_tokens = []
        if self.vit.cls_token is not None:
            prefix_tokens.append(self.vit.cls_token.expand(x.shape[0], -1, -1))
        if self.vit.reg_token is not None:
            prefix_tokens.append(self.vit.reg_token.expand(x.shape[0], -1, -1))
        if prefix_tokens:
            x = torch.cat(prefix_tokens + [x], dim=1)
        return x

    def add_pos_embed(self, x: Tensor) -> Tensor:
        x_prefix = x[:, : self.vit.num_prefix_tokens, :]
        x = x[:, self.vit.num_prefix_tokens :, :]
        if self.vit.dynamic_img_size:
            x = x.transpose(1, 2)  # NLC -> NCL
            total_size = torch.numel(x)
            batch_size = x.size(0)
            num_channels = x.size(1)
            grid_size = int(math.sqrt(total_size / (batch_size * num_channels)))
            x = x.view(
                x.size(0),
                x.size(1),
                grid_size,
                grid_size,
            )  # NCL -> NCHW

            # NCHW -> NHWC
            x = x.permute(0, 2, 3, 1)
            B, H, W, C = x.shape
            pos_embed = resample_abs_pos_embed(
                self.vit.pos_embed,
                (H, W),
                num_prefix_tokens=(
                    0 if self.vit.no_embed_class else self.vit.num_prefix_tokens
                ),
                antialias=self.antialias,
            )
            x = x.view(B, -1, C)
        else:
            pos_embed = self.vit.pos_embed

        if self.vit.no_embed_class:
            x = x + pos_embed
            if self.vit.num_prefix_tokens:
                x = torch.cat((x_prefix, x), dim=1)
        else:
            if self.vit.num_prefix_tokens:
                x = torch.cat((x_prefix, x), dim=1)
            x = x + pos_embed
        out: Tensor = self.vit.pos_drop(x)
        return out

    def _initialize_weights(self) -> None:
        # Initialize the patch embedding layer like a linear layer instead of conv
        # layer.
        w = self.vit.patch_embed.proj.weight.data
        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))

        # Initialize the class token.
        if self.vit.has_class_token:
            torch.nn.init.normal_(self.vit.cls_token, std=0.02)

        # initialize nn.Linear and nn.LayerNorm
        self.apply(init_weights)


def init_weights(module: Module) -> None:
    if isinstance(module, Linear):
        nn.init.xavier_uniform_(module.weight)
        if isinstance(module, Linear) and module.bias is not None:
            nn.init.constant_(module.bias, 0)
    elif isinstance(module, LayerNorm):
        nn.init.constant_(module.bias, 0)
        nn.init.constant_(module.weight, 1.0)



================================================
FILE: lightly/models/modules/masked_vision_transformer_torchvision.py
================================================
import math
from typing import List, Optional, Tuple

import torch
import torch.nn as nn
from torch import Tensor
from torch.nn import Linear, Module, Parameter
from torchvision.models.vision_transformer import VisionTransformer

from lightly.models import utils
from lightly.models.modules.masked_vision_transformer import MaskedVisionTransformer


class MaskedVisionTransformerTorchvision(MaskedVisionTransformer):
    """Masked Vision Transformer class using Torchvision.

    Attributes:
        vit:
            The VisionTransformer object of Torchvision.
        mask_token:
            The mask token.
        weight_initialization:
            The weight initialization method. Valid options are ['', 'skip']. '' uses
            the default MAE weight initialization and 'skip' skips the weight
        antialias:
            Whether to use antialiasing when resampling the positional embeddings.
        pos_embed_initialization:
            The strategy to initialize the positional embeddings. Valid options are
            ['learn', 'sincos', 'skip'].

    """

    def __init__(
        self,
        vit: VisionTransformer,
        mask_token: Optional[Parameter] = None,
        weight_initialization: str = "",
        antialias: bool = True,
        pos_embed_initialization: str = "sincos",
    ) -> None:
        super().__init__()
        self.vit = vit
        self.mask_token = (
            mask_token
            if mask_token is not None
            else Parameter(torch.zeros(1, 1, self.vit.hidden_dim))
        )
        if weight_initialization not in ("", "skip"):
            raise ValueError(
                f"Invalid weight initialization method: '{weight_initialization}'. "
                "Valid options are: ['', 'skip']."
            )
        if weight_initialization != "skip":
            self._initialize_weights()

        utils.initialize_positional_embedding(
            pos_embedding=self.vit.encoder.pos_embedding,
            strategy=pos_embed_initialization,
            num_prefix_tokens=1,  # class token
        )

        self.antialias = antialias

    @property
    def sequence_length(self) -> int:
        seq_len: int = self.vit.seq_length
        return seq_len

    def interpolate_pos_encoding(self, input: Tensor) -> Tensor:
        """Returns the interpolated positional embedding for the given input.

        This function interpolates self.pos_embedding for all tokens in the input,
        ignoring the class token. This allows encoding variable sized images.

        Args:
            input:
               Input tensor with shape (batch_size, num_sequences).

        """
        # code copied from:
        # https://github.com/facebookresearch/msn/blob/4388dc1eadbe3042b85d3296d41b9b207656e043/src/deit.py#L291
        npatch = input.shape[1] - 1
        N = self.vit.encoder.pos_embedding.shape[1] - 1
        if npatch == N:
            pos_embedding: Tensor = self.vit.encoder.pos_embedding
            return pos_embedding
        class_emb = self.vit.encoder.pos_embedding[:, 0]
        pos_embedding = self.vit.encoder.pos_embedding[:, 1:]
        dim = input.shape[-1]
        pos_embedding = nn.functional.interpolate(
            pos_embedding.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(
                0, 3, 1, 2
            ),
            scale_factor=math.sqrt(npatch / N),
            mode="bicubic",
            antialias=self.antialias,
        )
        pos_embedding = pos_embedding.permute(0, 2, 3, 1).view(1, -1, dim)
        return torch.cat((class_emb.unsqueeze(0), pos_embedding), dim=1)

    def forward(
        self,
        images: Tensor,
        idx_mask: Optional[Tensor] = None,
        idx_keep: Optional[Tensor] = None,
        mask: Optional[Tensor] = None,
    ) -> Tensor:
        out = self.encode(images, idx_mask=idx_mask, idx_keep=idx_keep, mask=mask)
        class_token = out[:, 0]
        return class_token

    def forward_intermediates(
        self,
        images: Tensor,
        idx_mask: Optional[Tensor] = None,
        idx_keep: Optional[Tensor] = None,
        norm: bool = False,
        mask: Optional[Tensor] = None,
    ) -> Tuple[Tensor, List[Tensor]]:
        raise NotImplementedError(
            "forward_intermediates is not implemented for this model."
        )

    def encode(
        self,
        images: Tensor,
        idx_mask: Optional[Tensor] = None,
        idx_keep: Optional[Tensor] = None,
        mask: Optional[Tensor] = None,
    ) -> Tensor:
        tokens = self.preprocess(
            images=images, idx_mask=idx_mask, idx_keep=idx_keep, mask=mask
        )
        out: Tensor = self.vit.encoder.ln(
            self.vit.encoder.layers(self.vit.encoder.dropout(tokens))
        )
        return out

    def images_to_tokens(self, images: Tensor) -> Tensor:
        x = self.vit.conv_proj(images)
        tokens: Tensor = x.flatten(2).transpose(1, 2)
        return tokens

    def prepend_prefix_tokens(
        self, x: Tensor, prepend_class_token: bool = True
    ) -> Tensor:
        if prepend_class_token:
            x = utils.prepend_class_token(x, self.vit.class_token)
        return x

    def add_pos_embed(self, x: Tensor) -> Tensor:
        # TODO(Ersi:1/24) This adds positional encoding to the prefix tokens as well.
        # Give the option of not doing so, as is the case for TIMM.
        x = x + self.interpolate_pos_encoding(x)
        return x

    def _initialize_weights(self) -> None:
        # Initialize the patch embedding layer like a linear layer instead of conv
        # layer.
        w = self.vit.conv_proj.weight.data
        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))

        # Initialize the class token.
        torch.nn.init.normal_(self.vit.class_token, std=0.02)

        # Initialize linear layers.
        _initialize_linear_layers(self)


def _initialize_linear_layers(module: Module) -> None:
    def init(mod: Module) -> None:
        if isinstance(mod, Linear):
            nn.init.xavier_uniform_(mod.weight)
            if mod.bias is not None:
                nn.init.constant_(mod.bias, 0)

    module.apply(init)



================================================
FILE: lightly/models/modules/memory_bank.py
================================================
""" Memory Bank Wrapper """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

import warnings
from typing import Optional, Sequence, Tuple, Union

import torch
from torch import Tensor
from torch.nn import Module

from lightly.models import utils
from lightly.utils import dist


class MemoryBankModule(Module):
    """Memory bank implementation

    This is a parent class to all loss functions implemented by the lightly
    Python package. This way, any loss can be used with a memory bank if
    desired.

    Attributes:
        size:
            Size of the memory bank as (num_features, dim) tuple. If num_features is 0
            then the memory bank is disabled. Deprecated: If only a single integer is
            passed, it is interpreted as the number of features and the feature
            dimension is inferred from the first batch stored in the memory bank.
            Leaving out the feature dimension might lead to errors in distributed
            training.
        gather_distributed:
            If True then negatives from all gpus are gathered before the memory bank
            is updated. This results in more frequent updates of the memory bank and
            keeps the memory bank contents independent of the number of gpus. But it has
            the drawback that synchronization between processes is required and
            diversity of the memory bank content is reduced.
        feature_dim_first:
            If True, the memory bank returns features with shape (dim, num_features).
            If False, the memory bank returns features with shape (num_features, dim).

    Examples:
        >>> class MyLossFunction(MemoryBankModule):
        >>>
        >>>     def __init__(self, memory_bank_size: Tuple[int, int] = (2 ** 16, 128)):
        >>>         super().__init__(memory_bank_size)
        >>>
        >>>     def forward(self, output: Tensor, labels: Optional[Tensor] = None):
        >>>         output, negatives = super().forward(output)
        >>>
        >>>         if negatives is not None:
        >>>             # evaluate loss with negative samples
        >>>         else:
        >>>             # evaluate loss without negative samples

    """

    def __init__(
        self,
        size: Union[int, Sequence[int]] = 65536,
        gather_distributed: bool = False,
        feature_dim_first: bool = True,
    ):
        super().__init__()
        size_tuple = (size,) if isinstance(size, int) else tuple(size)

        if any(x < 0 for x in size_tuple):
            raise ValueError(
                f"Illegal memory bank size {size}, all entries must be non-negative."
            )

        self.size = size_tuple
        self.gather_distributed = gather_distributed
        self.feature_dim_first = feature_dim_first
        self.bank: Tensor
        self.register_buffer(
            "bank",
            tensor=torch.empty(size=size_tuple, dtype=torch.float),
            persistent=False,
        )
        self.bank_ptr: Tensor
        self.register_buffer(
            "bank_ptr",
            tensor=torch.empty(1, dtype=torch.long),
            persistent=False,
        )

        if isinstance(size, int) and size > 0:
            warnings.warn(
                (
                    f"Memory bank size 'size={size}' does not specify feature "
                    "dimension. It is recommended to set the feature dimension with "
                    "'size=(n, dim)' when creating the memory bank. Distributed "
                    "training might fail if the feature dimension is not set."
                ),
                UserWarning,
            )
        elif len(size_tuple) > 1:
            self._init_memory_bank(size=size_tuple)

    def forward(
        self,
        output: Tensor,
        labels: Optional[Tensor] = None,
        update: bool = False,
    ) -> Tuple[Tensor, Union[Tensor, None]]:
        """Query memory bank for additional negative samples

        Args:
            output:
                The output of the model.
            labels:
                Should always be None, will be ignored.
            update:
                If True, the memory bank will be updated with the current output.

        Returns:
            The output if the memory bank is of size 0, otherwise the output
            and the entries from the memory bank. Entries from the memory bank have
            shape (dim, num_features) if feature_dim_first is True and
            (num_features, dim) otherwise.

        """

        # no memory bank, return the output
        if self.size[0] == 0:
            return output, None

        # Initialize the memory bank if it is not already done.
        if self.bank.ndim == 1:
            dim = output.shape[1:]
            self._init_memory_bank(size=(*self.size, *dim))

        # query and update memory bank
        bank = self.bank.clone().detach()
        if self.feature_dim_first:
            # swap bank size and feature dimension for backwards compatibility
            bank = bank.transpose(0, -1)

        # only update memory bank if we later do backward pass (gradient)
        if update:
            self._dequeue_and_enqueue(output)

        return output, bank

    @torch.no_grad()
    def _init_memory_bank(self, size: Tuple[int, ...]) -> None:
        """Initialize the memory bank.

        Args:
            size:
                Size of the memory bank as (num_features, dim) tuple.

        """
        self.bank = torch.randn(size).type_as(self.bank)
        self.bank = torch.nn.functional.normalize(self.bank, dim=-1)
        self.bank_ptr = torch.zeros(1).type_as(self.bank_ptr)

    @torch.no_grad()
    def _dequeue_and_enqueue(self, batch: Tensor) -> None:
        """Dequeue the oldest batch and add the latest one

        Args:
            batch:
                The latest batch of keys to add to the memory bank.

        """
        if self.gather_distributed and dist.world_size() > 1:
            batch = utils.concat_all_gather(batch)

        batch_size = batch.shape[0]
        ptr = int(self.bank_ptr)
        if ptr + batch_size >= self.size[0]:
            self.bank[ptr:] = batch[: self.size[0] - ptr].detach()
            self.bank_ptr.zero_()
        else:
            self.bank[ptr : ptr + batch_size] = batch.detach()
            self.bank_ptr[0] = ptr + batch_size



================================================
FILE: lightly/models/modules/nn_memory_bank.py
================================================
""" Nearest Neighbour Memory Bank Module """

# Copyright (c) 2021. Lightly AG and its affiliates.
# All Rights Reserved

from typing import Sequence, Union

import torch
from torch import Tensor

from lightly.models.modules.memory_bank import MemoryBankModule


class NNMemoryBankModule(MemoryBankModule):
    """Nearest Neighbour Memory Bank implementation

    This class implements a nearest neighbour memory bank as described in the
    NNCLR paper[0]. During the forward pass we return the nearest neighbour
    from the memory bank.

    [0] NNCLR, 2021, https://arxiv.org/abs/2104.14548

    Attributes:
        size:
            Size of the memory bank as (num_features, dim) tuple. If num_features is 0
            then the memory bank is disabled. Deprecated: If only a single integer is
            passed, it is interpreted as the number of features and the feature
            dimension is inferred from the first batch stored in the memory bank.
            Leaving out the feature dimension might lead to errors in distributed
            training.

    Examples:
        >>> model = NNCLR(backbone)
        >>> criterion = NTXentLoss(temperature=0.1)
        >>>
        >>> nn_replacer = NNmemoryBankModule(size=(2 ** 16, 128))
        >>>
        >>> # forward pass
        >>> (z0, p0), (z1, p1) = model(x0, x1)
        >>> z0 = nn_replacer(z0.detach(), update=False)
        >>> z1 = nn_replacer(z1.detach(), update=True)
        >>>
        >>> loss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))

    """

    def __init__(self, size: Union[int, Sequence[int]] = 2**16):
        super(NNMemoryBankModule, self).__init__(size)

    def forward(  # type: ignore[override] # TODO(Philipp, 11/23): Fix signature to match parent class.
        self,
        output: Tensor,
        update: bool = False,
    ) -> Tensor:
        """Returns nearest neighbour of output tensor from memory bank

        Args:
            output: The torch tensor for which you want the nearest neighbour
            update: If `True` updated the memory bank by adding output to it

        """

        output, bank = super(NNMemoryBankModule, self).forward(output, update=update)
        assert bank is not None
        bank = bank.to(output.device).t()

        output_normed = torch.nn.functional.normalize(output, dim=1)
        bank_normed = torch.nn.functional.normalize(bank, dim=1)

        similarity_matrix = torch.einsum("nd,md->nm", output_normed, bank_normed)
        index_nearest_neighbours = torch.argmax(similarity_matrix, dim=1)
        nearest_neighbours = torch.index_select(
            bank, dim=0, index=index_nearest_neighbours
        )

        return nearest_neighbours



================================================
FILE: lightly/transforms/__init__.py
================================================
"""The lightly.transforms package transforms for various self-supervised learning
methods.

It also contains some additional transforms that are not part of torchvisions
transforms.
"""

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

from lightly.transforms.aim_transform import AIMTransform
from lightly.transforms.amplitude_rescale_transform import AmplitudeRescaleTransform
from lightly.transforms.byol_transform import (
    BYOLTransform,
    BYOLView1Transform,
    BYOLView2Transform,
)
from lightly.transforms.densecl_transform import DenseCLTransform
from lightly.transforms.dino_transform import DINOTransform, DINOViewTransform
from lightly.transforms.fast_siam_transform import FastSiamTransform
from lightly.transforms.fda_transform import (
    FDATransform,
    FDAView1Transform,
    FDAView2Transform,
)
from lightly.transforms.gaussian_blur import GaussianBlur
from lightly.transforms.gaussian_mixture_masks_transform import GaussianMixtureMask
from lightly.transforms.ibot_transform import IBOTTransform, IBOTViewTransform
from lightly.transforms.irfft2d_transform import IRFFT2DTransform
from lightly.transforms.jigsaw import Jigsaw
from lightly.transforms.mae_transform import MAETransform
from lightly.transforms.mmcr_transform import MMCRTransform
from lightly.transforms.moco_transform import MoCoV1Transform, MoCoV2Transform
from lightly.transforms.msn_transform import MSNTransform, MSNViewTransform
from lightly.transforms.phase_shift_transform import PhaseShiftTransform
from lightly.transforms.pirl_transform import PIRLTransform
from lightly.transforms.random_frequency_mask_transform import (
    RandomFrequencyMaskTransform,
)
from lightly.transforms.rfft2d_transform import RFFT2DTransform
from lightly.transforms.rotation import (
    RandomRotate,
    RandomRotateDegrees,
    random_rotation_transform,
)
from lightly.transforms.simclr_transform import SimCLRTransform, SimCLRViewTransform
from lightly.transforms.simsiam_transform import SimSiamTransform, SimSiamViewTransform
from lightly.transforms.smog_transform import SMoGTransform, SmoGViewTransform
from lightly.transforms.solarize import RandomSolarization
from lightly.transforms.swav_transform import SwaVTransform, SwaVViewTransform
from lightly.transforms.tico_transform import (
    TiCoTransform,
    TiCoView1Transform,
    TiCoView2Transform,
)
from lightly.transforms.torchvision_v2_compatibility import (
    ToTensor,
    functional,
    torchvision_transforms,
)
from lightly.transforms.vicreg_transform import VICRegTransform, VICRegViewTransform
from lightly.transforms.vicregl_transform import VICRegLTransform, VICRegLViewTransform
from lightly.transforms.wmse_transform import WMSETransform
from lightly.utils.dependency import torchvision_transforms_v2_available

if torchvision_transforms_v2_available():
    from lightly.transforms.add_grid_transform import AddGridTransform
    from lightly.transforms.detcon_transform import (
        DetConSTransform,
        DetConSViewTransform,
    )
    from lightly.transforms.multi_view_transform_v2 import MultiViewTransformV2



================================================
FILE: lightly/transforms/add_grid_transform.py
================================================
# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved
from math import ceil
from typing import Any, Dict, Tuple

import torch
from lightning_utilities.core.imports import RequirementCache
from torchvision.transforms.v2 import CenterCrop, ConvertBoundingBoxFormat, Transform
from torchvision.tv_tensors import BoundingBoxes, BoundingBoxFormat, Mask

from lightly.utils import dependency as _dependency


# ignore typing due to Any type used in torchvison.transforms.v2.Transform
class AddGridTransform(Transform):  # type: ignore[misc]
    """Implements the naive segmentation into a regular grid from DetCon. [0]_

    Input to this transform:
        Any datastructure containing one or several `torchvision.tv_tensor.BoundingBoxes`
        and/or `torchvision.tv_tensor.Mask`, such as tuples or arbitrarily nested dictionaries.
        For all supported data structures check [1]_. Masks should be of size `(*, H, W)` and
        BoundingBoxes can be of arbitrary shape.

    Output of this transform:
        Leaves any images in the data structure untouched, but overwrites any bounding
        boxes and masks by a regular grid. Bounding boxes will take shape `(num_rows*num_cols, 4)`
        and masks will be of shape `(1, H, W)` with integer values in the range `[0, num_rows*num_cols-1]`.

    Example:

        .. code-block:: python

            img = torch.randn((3, 16, 16))
            bboxes = BoundingBoxes(torch.randn((1, 4)), format="XYXY", canvas_size=img.shape[-2:])
            mask = Mask(torch.randn((16, 16)).to(torch.int64))
            tr = AddGridTransform(num_rows=4, num_cols=4)
            # the image will be untouched the bounding boxes will be a regular grid
            img, bboxes, mask = tr(img, bboxes, mask)
            # bboxes of shape (num_rows*num_cols, 4)
            # mask with value range [0, num_rows*num_cols-1]

    References:
        .. [0] DetCon, 2021, https://arxiv.org/abs/2103.10957
        .. [1] torchvision Getting started with transforms v2, https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html

    Attributes:
        num_rows: number of grid rows in the segmentation
        num_cols: number of grid columns in the segmentation
    """

    def __init__(self, num_rows: int, num_cols: int) -> None:
        if not _dependency.torchvision_transforms_v2_available():
            raise ImportError(
                "AddGridTransform requires torchvision.transforms.v2, included in torchvision>=0.17"
            )
        super().__init__()
        self.num_rows = num_rows
        self.num_cols = num_cols

    def _apply_grid_transform(self, inpt: Any) -> Any:
        if isinstance(inpt, BoundingBoxes):
            return _create_bounding_boxes_grid(
                num_rows=self.num_rows,
                num_cols=self.num_cols,
                canvas_size=inpt.canvas_size,
                dtype=inpt.dtype,
                device=inpt.device,
                requires_grad=inpt.requires_grad,
                format=inpt.format,
            )
        elif isinstance(inpt, Mask):
            if inpt.dim() < 2:
                raise ValueError(
                    f"Expected mask to have at least 2 dimensions, got {inpt.dim()} instead."
                )
            return _create_mask_grid(
                num_rows=self.num_rows,
                num_cols=self.num_cols,
                canvas_size=inpt.shape[-2:],
                dtype=inpt.dtype,
                device=inpt.device,
                requires_grad=inpt.requires_grad,
            )
        else:
            return inpt

    if RequirementCache("torchvision>=0.21.0"):

        def transform(self, inpt: Any, params: Dict[str, Any]) -> Any:
            return self._apply_grid_transform(inpt)

    else:

        def _transform(self, inpt: Any, params: Dict[str, Any]) -> Any:
            return self._apply_grid_transform(inpt)


def _create_bounding_boxes_grid(
    num_rows: int,
    num_cols: int,
    canvas_size: Tuple[int, int],
    dtype: torch.dtype,
    device: torch.device,
    requires_grad: bool,
    format: BoundingBoxFormat,
) -> BoundingBoxes:
    h, w = canvas_size

    xs = torch.linspace(0, w, num_cols + 1)
    ys = torch.linspace(0, h, num_rows + 1)

    x_min, x_max = xs[:-1], xs[1:]
    y_min, y_max = ys[:-1], ys[1:]

    x_min, y_min = torch.meshgrid(x_min, y_min, indexing="ij")
    x_max, y_max = torch.meshgrid(x_max, y_max, indexing="ij")

    bboxes = torch.stack([x_min, y_min, x_max, y_max], dim=-1)
    bboxes = BoundingBoxes(
        bboxes.view(-1, 4),
        format="XYXY",
        canvas_size=canvas_size,
        dtype=dtype,
        device=device,
        requires_grad=requires_grad,
    )
    return ConvertBoundingBoxFormat(format)(bboxes)


def _create_mask_grid(
    num_rows: int,
    num_cols: int,
    canvas_size: Tuple[int, int],
    dtype: torch.dtype,
    device: torch.device,
    requires_grad: bool,
) -> Mask:
    h, w = canvas_size

    patch_h = ceil(h / num_rows)
    patch_w = ceil(w / num_cols)

    classes = torch.linspace(0, num_rows * num_cols - 1, num_rows * num_cols).to(
        torch.int64
    )
    patches = classes[:, None, None].expand(-1, patch_h, patch_w)
    patches = patches.view(num_rows, num_cols, patch_h, patch_w)
    mask = (
        patches.permute(0, 2, 1, 3)
        .contiguous()
        .view(num_rows * patch_h, num_cols * patch_w)
    )
    mask = mask.unsqueeze(0)
    mask = CenterCrop((canvas_size))(mask)

    return Mask(mask, dtype=dtype, device=device, requires_grad=requires_grad)



================================================
FILE: lightly/transforms/aim_transform.py
================================================
from typing import Dict, List, Tuple, Union

from lightly.transforms.mae_transform import MAETransform
from lightly.transforms.utils import IMAGENET_NORMALIZE


class AIMTransform(MAETransform):
    """Implements the view augmentation for AIM [0].

    Uses the same parameters as MAE [1] but with larger min_scale.

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length 1.

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip

    - [0]: AIM, 2024, https://arxiv.org/abs/2401.08541
    - [1]: Masked Autoencoder, 2021, https://arxiv.org/abs/2111.06377

    Attributes:
        input_size:
            Size of the input image in pixels.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        input_size: Union[int, Tuple[int, int]] = 224,
        min_scale: float = 0.4,
        normalize: Dict[str, List[float]] = IMAGENET_NORMALIZE,
    ):
        super().__init__(
            input_size=input_size,
            min_scale=min_scale,
            normalize=normalize,
        )



================================================
FILE: lightly/transforms/amplitude_rescale_transform.py
================================================
from typing import Tuple

import torch
from torch import Tensor
from torch.distributions import Uniform


class AmplitudeRescaleTransform:
    """Implementation of amplitude rescaling transformation.

    This transform will rescale the amplitude of the Fourier Spectrum (`freq_image`) of the image and return it.

    Attributes:
        dist:
            Uniform distribution in `[m, n)` from which the scaling value will be selected.
    """

    def __init__(self, range: Tuple[float, float] = (0.8, 1.75)) -> None:
        self.dist = Uniform(range[0], range[1])

    def __call__(self, freq_image: Tensor) -> Tensor:
        amplitude = torch.sqrt(freq_image.real**2 + freq_image.imag**2)

        phase = torch.atan2(freq_image.imag, freq_image.real)
        # p with shape (H, W)
        p = self.dist.sample(freq_image.shape[1:]).to(freq_image.device)
        # Unsqueeze to add channel dimension.
        amplitude *= p.unsqueeze(0)
        real = amplitude * torch.cos(phase)
        imag = amplitude * torch.sin(phase)
        output = torch.complex(real, imag)

        return output



================================================
FILE: lightly/transforms/byol_transform.py
================================================
from typing import Dict, List, Optional, Tuple, Union

from PIL.Image import Image
from torch import Tensor

from lightly.transforms.gaussian_blur import GaussianBlur
from lightly.transforms.multi_view_transform import MultiViewTransform
from lightly.transforms.rotation import random_rotation_transform
from lightly.transforms.solarize import RandomSolarization
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.transforms.utils import IMAGENET_NORMALIZE


class BYOLView1Transform:
    def __init__(
        self,
        input_size: int = 224,
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        cj_bright: float = 0.4,
        cj_contrast: float = 0.4,
        cj_sat: float = 0.2,
        cj_hue: float = 0.1,
        min_scale: float = 0.08,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 1.0,
        solarization_prob: float = 0.0,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        color_jitter = T.ColorJitter(
            brightness=cj_strength * cj_bright,
            contrast=cj_strength * cj_contrast,
            saturation=cj_strength * cj_sat,
            hue=cj_strength * cj_hue,
        )

        transform = [
            T.RandomResizedCrop(size=input_size, scale=(min_scale, 1.0)),
            T.RandomHorizontalFlip(p=hf_prob),
            T.RandomVerticalFlip(p=vf_prob),
            random_rotation_transform(rr_prob=rr_prob, rr_degrees=rr_degrees),
            T.RandomApply([color_jitter], p=cj_prob),
            T.RandomGrayscale(p=random_gray_scale),
            GaussianBlur(kernel_size=kernel_size, sigmas=sigmas, prob=gaussian_blur),
            RandomSolarization(prob=solarization_prob),
            T.ToTensor(),
        ]
        if normalize:
            transform += [T.Normalize(mean=normalize["mean"], std=normalize["std"])]
        self.transform = T.Compose(transform)

    def __call__(self, image: Union[Tensor, Image]) -> Tensor:
        """
        Applies the transforms to the input image.

        Args:
            image:
                The input image to apply the transforms to.

        Returns:
            The transformed image.

        """
        transformed: Tensor = self.transform(image)
        return transformed


class BYOLView2Transform:
    def __init__(
        self,
        input_size: int = 224,
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        cj_bright: float = 0.4,
        cj_contrast: float = 0.4,
        cj_sat: float = 0.2,
        cj_hue: float = 0.1,
        min_scale: float = 0.08,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 0.1,
        solarization_prob: float = 0.2,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        color_jitter = T.ColorJitter(
            brightness=cj_strength * cj_bright,
            contrast=cj_strength * cj_contrast,
            saturation=cj_strength * cj_sat,
            hue=cj_strength * cj_hue,
        )

        transform = [
            T.RandomResizedCrop(size=input_size, scale=(min_scale, 1.0)),
            T.RandomHorizontalFlip(p=hf_prob),
            T.RandomVerticalFlip(p=vf_prob),
            random_rotation_transform(rr_prob=rr_prob, rr_degrees=rr_degrees),
            T.RandomApply([color_jitter], p=cj_prob),
            T.RandomGrayscale(p=random_gray_scale),
            GaussianBlur(kernel_size=kernel_size, sigmas=sigmas, prob=gaussian_blur),
            RandomSolarization(prob=solarization_prob),
            T.ToTensor(),
        ]
        if normalize:
            transform += [T.Normalize(mean=normalize["mean"], std=normalize["std"])]
        self.transform = T.Compose(transform)

    def __call__(self, image: Union[Tensor, Image]) -> Tensor:
        """
        Applies the transforms to the input image.

        Args:
            image:
                The input image to apply the transforms to.

        Returns:
            The transformed image.

        """
        transformed: Tensor = self.transform(image)
        return transformed


class BYOLTransform(MultiViewTransform):
    """Implements the transformations for BYOL[0].

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length 2.

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip
        - Color jitter
        - Random gray scale
        - Gaussian blur
        - Solarization
        - ImageNet normalization

    Note that SimCLR v1 and v2 use similar augmentations. In detail, BYOL has
    asymmetric gaussian blur and solarization. Furthermore, BYOL has weaker
    color jitter compared to SimCLR.

    - [0]: Bootstrap Your Own Latent, 2020, https://arxiv.org/pdf/2006.07733.pdf

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of [tensor, tensor].

    Attributes:
        view_1_transform: The transform for the first view.
        view_2_transform: The transform for the second view.
    """

    def __init__(
        self,
        view_1_transform: Optional[BYOLView1Transform] = None,
        view_2_transform: Optional[BYOLView2Transform] = None,
    ):
        # We need to initialize the transforms here
        view_1_transform = view_1_transform or BYOLView1Transform()
        view_2_transform = view_2_transform or BYOLView2Transform()
        super().__init__(transforms=[view_1_transform, view_2_transform])



================================================
FILE: lightly/transforms/densecl_transform.py
================================================
from lightly.transforms.moco_transform import MoCoV2Transform


class DenseCLTransform(MoCoV2Transform):
    """Implements the transformations for DenseCL [0].

    Identical to MoCoV2Transform.

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length 2.

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip
        - Color jitter
        - Random gray scale
        - Gaussian blur
        - ImageNet normalization

    - [0]: 2021, DenseCL: https://arxiv.org/abs/2011.09157

    Attributes:
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter. `cj_bright`, `cj_contrast`, `cj_sat`, and
            `cj_hue` are multiplied by this value. For datasets with small images,
            such as CIFAR, it is recommended to set `cj_strenght` to 0.5.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        vf_prob:
            Probability that vertical flip is applied.
        hf_prob:
            Probability that horizontal flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None,
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple,
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """



================================================
FILE: lightly/transforms/detcon_transform.py
================================================
from typing import Any, Dict, List, Optional, Tuple, Union

from lightly.transforms.add_grid_transform import AddGridTransform
from lightly.transforms.multi_view_transform_v2 import MultiViewTransformV2
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.transforms.utils import IMAGENET_NORMALIZE


class DetConSTransform(MultiViewTransformV2):
    """Implements the transformations for DetConS [0], based on SimCLR.[1]

    This transform creates two views of the input data, where the second view is different
    in that it does not apply Gaussian blurring.

    Input to this transform:
        Arbitrary data structure containing images and masks, that is compatible with
        torchvision transforms v2.[2]

    Output of this transform:
        A list of two views, where each view is a transformed version of the input.

    Applies the following augmentations by default:
        - RandomResizedCrop
        - RandomHorizontalFlip
        - RandomVerticalFlip
        - RandomRotation
        - ColorJitter
        - RandomGrayscale
        - GaussianBlur (only for the first view)

    Can additionally apply a segmentation of the image into a regular grid if not provided
    with a pre-segmented image.

    References:
        - [0] DetCon, 2021, https://arxiv.org/abs/2103.10957
        - [1] SimCLR, 2020, https://arxiv.org/abs/2002.05709
        - [2] torchvision Getting started with transforms v2, https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html

    Attributes:
        grid_size: Size of the grid segmentation as a tuple (num_rows, num_cols), or None
            if the segmentation mask is to be provided by the user.
        gaussian_blur_t1:
            Probability of applying Gaussian blur to the first view.
        gaussian_blur_t2:
            Probability of applying Gaussian blur to the second view.
        input_size:
            Size of the desired model input in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter. `cj_bright`, `cj_contrast`, `cj_sat`, and
            `cj_hue` are multiplied by this value. For datasets with small images,
            such as CIFAR, it is recommended to set `cj_strength` to 0.5.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        min_scale:
            Minimum size of the randomized crop relative to the input image.
        random_gray_scale:
            Probability of conversion to grayscale.
        kernel_size:
            Size of the Gaussian kernel for Gaussian blur.
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
        vf_prob:
            Probability that vertical flip is applied.
        hf_prob:
            Probability that horizontal flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from. If degrees is a number instead of sequence
            like (min, max), the range of degrees will be (-degrees, +degrees). The rotation
            is applied counter-clockwise.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.
    """

    def __init__(
        self,
        grid_size: Optional[Tuple[int, int]] = None,
        gaussian_blur_t1: float = 1.0,
        gaussian_blur_t2: float = 0.0,
        input_size: Union[Tuple[int, int], int] = 224,
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.8,
        cj_hue: float = 0.2,
        min_scale: float = 0.08,
        random_gray_scale: float = 0.2,
        kernel_size: Tuple[float, float] = (23, 23),
        sigmas: Tuple[float, float] = (0.1, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Union[float, Tuple[float, float]] = 0.0,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ) -> None:
        self.grid_size = grid_size

        tr1: List[Union[AddGridTransform, DetConSViewTransform]] = []
        tr2: List[Union[AddGridTransform, DetConSViewTransform]] = []

        if self.grid_size is not None:
            grid_tr1 = AddGridTransform(
                num_rows=self.grid_size[0], num_cols=self.grid_size[1]
            )
            tr1 += [grid_tr1]
            grid_tr2 = AddGridTransform(
                num_rows=self.grid_size[0], num_cols=self.grid_size[1]
            )
            tr2 += [grid_tr2]

        tr1 += [
            DetConSViewTransform(
                gaussian_blur=gaussian_blur_t1,
                input_size=input_size,
                cj_prob=cj_prob,
                cj_strength=cj_strength,
                cj_bright=cj_bright,
                cj_contrast=cj_contrast,
                cj_sat=cj_sat,
                cj_hue=cj_hue,
                min_scale=min_scale,
                random_gray_scale=random_gray_scale,
                kernel_size=kernel_size,
                sigmas=sigmas,
                vf_prob=vf_prob,
                hf_prob=hf_prob,
                rr_prob=rr_prob,
                rr_degrees=rr_degrees,
                normalize=normalize,
            )
        ]
        tr2 += [
            DetConSViewTransform(
                gaussian_blur=gaussian_blur_t2,
                input_size=input_size,
                cj_prob=cj_prob,
                cj_strength=cj_strength,
                cj_bright=cj_bright,
                cj_contrast=cj_contrast,
                cj_sat=cj_sat,
                cj_hue=cj_hue,
                min_scale=min_scale,
                random_gray_scale=random_gray_scale,
                kernel_size=kernel_size,
                sigmas=sigmas,
                vf_prob=vf_prob,
                hf_prob=hf_prob,
                rr_prob=rr_prob,
                rr_degrees=rr_degrees,
                normalize=normalize,
            )
        ]

        super().__init__(transforms=[T.Compose(tr1), T.Compose(tr2)])


class DetConSViewTransform:
    def __init__(
        self,
        input_size: Union[Tuple[int, int], int] = 224,
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.8,
        cj_hue: float = 0.2,
        min_scale: float = 0.08,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 1.0,
        kernel_size: Tuple[float, float] = (23, 23),
        sigmas: Tuple[float, float] = (0.1, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Union[float, Tuple[float, float]] = 0.0,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ) -> None:
        color_jitter = T.ColorJitter(
            brightness=cj_strength * cj_bright,
            contrast=cj_strength * cj_contrast,
            saturation=cj_strength * cj_sat,
            hue=cj_strength * cj_hue,
        )

        transform = [
            T.RandomResizedCrop(size=input_size, scale=(min_scale, 1.0)),
            T.RandomHorizontalFlip(p=hf_prob),
            T.RandomVerticalFlip(p=vf_prob),
            T.RandomApply([T.RandomRotation(rr_degrees)], p=rr_prob),
            T.RandomApply([color_jitter], p=cj_prob),
            T.RandomGrayscale(p=random_gray_scale),
            T.RandomApply(
                [T.GaussianBlur(kernel_size=kernel_size, sigma=sigmas)], p=gaussian_blur
            ),
            T.ToTensor(),
        ]
        if normalize:
            transform += [T.Normalize(mean=normalize["mean"], std=normalize["std"])]
        self.tr = T.Compose(transform)

    def __call__(self, *args: Any) -> Any:
        """Applies the transforms to arbitrary positional arguments containing arbitrary
        data structures of images, bounding boxes and masks.

        Args:
            *args: Arbitrary positional arguments consisting of arbitrary data structures
                containing images, bounding boxes and masks.

        Returns:
            The transformed input in the same data structure as the input.
        """
        return self.tr(*args)



================================================
FILE: lightly/transforms/dino_transform.py
================================================
from typing import Dict, List, Optional, Tuple, Union

import PIL
from PIL.Image import Image
from torch import Tensor

from lightly.transforms.gaussian_blur import GaussianBlur
from lightly.transforms.multi_view_transform import MultiViewTransform
from lightly.transforms.rotation import random_rotation_transform
from lightly.transforms.solarize import RandomSolarization
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.transforms.utils import IMAGENET_NORMALIZE


class DINOTransform(MultiViewTransform):
    """Implements the global and local view augmentations for DINO [0].

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length 2 * global + n_local_views. (8 by default)

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip
        - Color jitter
        - Random gray scale
        - Gaussian blur
        - Random solarization
        - ImageNet normalization

    This class generates two global and a user defined number of local views
    for each image in a batch. The code is adapted from [1].

    - [0]: DINO, 2021, https://arxiv.org/abs/2104.14294
    - [1]: https://github.com/facebookresearch/dino

    Attributes:
        global_crop_size:
            Crop size of the global views.
        global_crop_scale:
            Tuple of min and max scales relative to global_crop_size.
        local_crop_size:
            Crop size of the local views.
        local_crop_scale:
            Tuple of min and max scales relative to local_crop_size.
        n_local_views:
            Number of generated local views.
        hf_prob:
            Probability that horizontal flip is applied.
        vf_prob:
            Probability that vertical flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None,
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple,
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter. `cj_bright`, `cj_contrast`, `cj_sat`, and
            `cj_hue` are multiplied by this value.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Tuple of probabilities to apply gaussian blur on the different
            views. The input is ordered as follows:
            (global_view_0, global_view_1, local_views)
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        kernel_scale:
            Old argument. Value is deprecated in favor of sigmas. If set, the old behavior applies and `sigmas` is ignored.
            Used to scale the `kernel_size` of a factor of `kernel_scale`
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        solarization:
            Probability to apply solarization on the second global view.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        global_crop_size: int = 224,
        global_crop_scale: Tuple[float, float] = (0.4, 1.0),
        local_crop_size: int = 96,
        local_crop_scale: Tuple[float, float] = (0.05, 0.4),
        n_local_views: int = 6,
        hf_prob: float = 0.5,
        vf_prob: float = 0,
        rr_prob: float = 0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        cj_prob: float = 0.8,
        cj_strength: float = 0.5,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.4,
        cj_hue: float = 0.2,
        random_gray_scale: float = 0.2,
        gaussian_blur: Tuple[float, float, float] = (1.0, 0.1, 0.5),
        kernel_size: Optional[float] = None,
        kernel_scale: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        solarization_prob: float = 0.2,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        # first global crop
        global_transform_0 = DINOViewTransform(
            crop_size=global_crop_size,
            crop_scale=global_crop_scale,
            hf_prob=hf_prob,
            vf_prob=vf_prob,
            rr_prob=rr_prob,
            rr_degrees=rr_degrees,
            cj_prob=cj_prob,
            cj_strength=cj_strength,
            cj_bright=cj_bright,
            cj_contrast=cj_contrast,
            cj_hue=cj_hue,
            cj_sat=cj_sat,
            random_gray_scale=random_gray_scale,
            gaussian_blur=gaussian_blur[0],
            kernel_size=kernel_size,
            kernel_scale=kernel_scale,
            sigmas=sigmas,
            solarization_prob=0,
            normalize=normalize,
        )

        # second global crop
        global_transform_1 = DINOViewTransform(
            crop_size=global_crop_size,
            crop_scale=global_crop_scale,
            hf_prob=hf_prob,
            vf_prob=vf_prob,
            rr_prob=rr_prob,
            rr_degrees=rr_degrees,
            cj_prob=cj_prob,
            cj_bright=cj_bright,
            cj_contrast=cj_contrast,
            cj_hue=cj_hue,
            cj_sat=cj_sat,
            random_gray_scale=random_gray_scale,
            gaussian_blur=gaussian_blur[1],
            kernel_size=kernel_size,
            kernel_scale=kernel_scale,
            sigmas=sigmas,
            solarization_prob=solarization_prob,
            normalize=normalize,
        )

        # transformation for the local small crops
        local_transform = DINOViewTransform(
            crop_size=local_crop_size,
            crop_scale=local_crop_scale,
            hf_prob=hf_prob,
            vf_prob=vf_prob,
            rr_prob=rr_prob,
            rr_degrees=rr_degrees,
            cj_prob=cj_prob,
            cj_strength=cj_strength,
            cj_bright=cj_bright,
            cj_contrast=cj_contrast,
            cj_hue=cj_hue,
            cj_sat=cj_sat,
            random_gray_scale=random_gray_scale,
            gaussian_blur=gaussian_blur[2],
            kernel_size=kernel_size,
            kernel_scale=kernel_scale,
            sigmas=sigmas,
            solarization_prob=0,
            normalize=normalize,
        )
        local_transforms = [local_transform] * n_local_views
        transforms = [global_transform_0, global_transform_1]
        transforms.extend(local_transforms)
        super().__init__(transforms)


class DINOViewTransform:
    def __init__(
        self,
        crop_size: int = 224,
        crop_scale: Tuple[float, float] = (0.4, 1.0),
        hf_prob: float = 0.5,
        vf_prob: float = 0,
        rr_prob: float = 0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        cj_prob: float = 0.8,
        cj_strength: float = 0.5,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.4,
        cj_hue: float = 0.2,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 1.0,
        kernel_size: Optional[float] = None,
        kernel_scale: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        solarization_prob: float = 0.2,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        transform = [
            T.RandomResizedCrop(
                size=crop_size,
                scale=crop_scale,
                # Type ignore needed because BICUBIC is not recognized as an attribute.
                interpolation=PIL.Image.BICUBIC,  # type: ignore[attr-defined]
            ),
            T.RandomHorizontalFlip(p=hf_prob),
            T.RandomVerticalFlip(p=vf_prob),
            random_rotation_transform(rr_prob=rr_prob, rr_degrees=rr_degrees),
            T.RandomApply(
                [
                    T.ColorJitter(
                        brightness=cj_strength * cj_bright,
                        contrast=cj_strength * cj_contrast,
                        saturation=cj_strength * cj_sat,
                        hue=cj_strength * cj_hue,
                    )
                ],
                p=cj_prob,
            ),
            T.RandomGrayscale(p=random_gray_scale),
            GaussianBlur(
                kernel_size=kernel_size,
                scale=kernel_scale,
                sigmas=sigmas,
                prob=gaussian_blur,
            ),
            RandomSolarization(prob=solarization_prob),
            T.ToTensor(),
        ]

        if normalize:
            transform += [T.Normalize(mean=normalize["mean"], std=normalize["std"])]
        self.transform = T.Compose(transform)

    def __call__(self, image: Union[Tensor, Image]) -> Tensor:
        """
        Applies the transforms to the input image.

        Args:
            image:
                The input image to apply the transforms to.

        Returns:
            The transformed image.

        """
        transformed: Tensor = self.transform(image)
        return transformed



================================================
FILE: lightly/transforms/fast_siam_transform.py
================================================
from typing import Dict, List, Optional, Tuple, Union

from lightly.transforms.multi_view_transform import MultiViewTransform
from lightly.transforms.simsiam_transform import SimSiamViewTransform
from lightly.transforms.utils import IMAGENET_NORMALIZE


class FastSiamTransform(MultiViewTransform):
    """Implements the transformations for FastSiam.

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length 4.

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip
        - Color jitter
        - Random gray scale
        - Gaussian blur
        - ImageNet normalization

    Attributes:
        num_views:
            Number of views (num_views = K+1 where K is the number of target views).
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter. `cj_bright`, `cj_contrast`, `cj_sat`, and
            `cj_hue` are multiplied by this value. For datasets with small images,
            such as CIFAR, it is recommended to set `cj_strength` to 0.5.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        vf_prob:
            Probability that vertical flip is applied.
        hf_prob:
            Probability that horizontal flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None,
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple,
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        num_views: int = 4,
        input_size: int = 224,
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        cj_bright: float = 0.4,
        cj_contrast: float = 0.4,
        cj_sat: float = 0.4,
        cj_hue: float = 0.1,
        min_scale: float = 0.2,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        transforms = [
            SimSiamViewTransform(
                input_size=input_size,
                cj_prob=cj_prob,
                cj_strength=cj_strength,
                cj_bright=cj_bright,
                cj_contrast=cj_contrast,
                cj_sat=cj_sat,
                cj_hue=cj_hue,
                min_scale=min_scale,
                random_gray_scale=random_gray_scale,
                gaussian_blur=gaussian_blur,
                kernel_size=kernel_size,
                sigmas=sigmas,
                vf_prob=vf_prob,
                hf_prob=hf_prob,
                rr_prob=rr_prob,
                rr_degrees=rr_degrees,
                normalize=normalize,
            )
            for _ in range(num_views)
        ]
        super().__init__(transforms=transforms)



================================================
FILE: lightly/transforms/fda_transform.py
================================================
from typing import Dict, List, Optional, Tuple, Union

from PIL.Image import Image
from torch import Tensor

from lightly.transforms.amplitude_rescale_transform import AmplitudeRescaleTransform
from lightly.transforms.gaussian_blur import GaussianBlur
from lightly.transforms.gaussian_mixture_masks_transform import GaussianMixtureMask
from lightly.transforms.irfft2d_transform import IRFFT2DTransform
from lightly.transforms.multi_view_transform import MultiViewTransform
from lightly.transforms.phase_shift_transform import PhaseShiftTransform
from lightly.transforms.random_frequency_mask_transform import (
    RandomFrequencyMaskTransform,
)
from lightly.transforms.rfft2d_transform import RFFT2DTransform
from lightly.transforms.rotation import random_rotation_transform
from lightly.transforms.solarize import RandomSolarization
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.transforms.utils import IMAGENET_NORMALIZE


class FDAView1Transform:
    def __init__(
        self,
        # Random resized crop
        input_size: int = 224,
        min_scale: float = 0.08,
        # Color jitter
        cj_prob: float = 0.8,
        cj_contrast: float = 0.4,
        cj_bright: float = 0.4,
        cj_sat: float = 0.2,
        cj_hue: float = 0.1,
        cj_strength: float = 1.0,
        # Grayscale
        random_gray_scale: float = 0.2,
        # Gaussian blur
        gaussian_blur: float = 1.0,
        sigmas: Tuple[float, float] = (0.1, 2),
        kernel_size: Optional[float] = 23,
        # Amplitude rescale
        ampl_rescale_range: Tuple[float, float] = (0.8, 1.75),
        ampl_rescale_prob: float = 0.2,
        # Phase shift
        phase_shift_range: Tuple[float, float] = (0.4, 0.7),
        phase_shift_prob: float = 0.2,
        # Random frequency mask
        rand_freq_mask_range: Tuple[float, float] = (0.01, 0.1),
        rand_freq_mask_prob: float = 0.5,
        # Gaussian mixture mask
        gmm_num_gaussians: int = 20,
        gmm_std_range: Tuple[float, float] = (10, 15),
        gmm_prob: float = 0.2,
        # Other
        solarization_prob: float = 0.0,
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        color_jitter = T.ColorJitter(
            brightness=cj_strength * cj_bright,
            contrast=cj_strength * cj_contrast,
            saturation=cj_strength * cj_sat,
            hue=cj_strength * cj_hue,
        )

        transform = [
            T.RandomResizedCrop(size=input_size, scale=(min_scale, 1.0)),
            T.ToTensor(),
            RFFT2DTransform(),
            T.RandomApply(
                [AmplitudeRescaleTransform(range=ampl_rescale_range)],
                p=ampl_rescale_prob,
            ),
            T.RandomApply(
                [PhaseShiftTransform(range=phase_shift_range)], p=phase_shift_prob
            ),
            T.RandomApply(
                [RandomFrequencyMaskTransform(k=rand_freq_mask_range)],
                p=rand_freq_mask_prob,
            ),
            T.RandomApply(
                [
                    GaussianMixtureMask(
                        num_gaussians=gmm_num_gaussians, std_range=gmm_std_range
                    )
                ],
                p=gmm_prob,
            ),
            IRFFT2DTransform(shape=(input_size, input_size)),
            T.ToPILImage(),
            T.RandomHorizontalFlip(p=hf_prob),
            T.RandomVerticalFlip(p=vf_prob),
            random_rotation_transform(rr_prob=rr_prob, rr_degrees=rr_degrees),
            T.RandomApply([color_jitter], p=cj_prob),
            T.RandomGrayscale(p=random_gray_scale),
            GaussianBlur(kernel_size=kernel_size, sigmas=sigmas, prob=gaussian_blur),
            RandomSolarization(prob=solarization_prob),
            T.ToTensor(),
        ]
        if normalize:
            transform += [T.Normalize(mean=normalize["mean"], std=normalize["std"])]
        self.transform = T.Compose(transform)

    def __call__(self, image: Union[Tensor, Image]) -> Tensor:
        """
        Applies the transforms to the input image.

        Args:
            image:
                The input image to apply the transforms to.

        Returns:
            The transformed image.

        """
        transformed: Tensor = self.transform(image)
        return transformed


class FDAView2Transform:
    def __init__(
        self,
        # Random resized crop
        input_size: int = 224,
        min_scale: float = 0.08,
        # Color jitter
        cj_prob: float = 0.8,
        cj_contrast: float = 0.4,
        cj_bright: float = 0.4,
        cj_sat: float = 0.2,
        cj_hue: float = 0.1,
        cj_strength: float = 1.0,
        # Grayscale
        random_gray_scale: float = 0.2,
        # Gaussian blur
        gaussian_blur: float = 0.1,
        sigmas: Tuple[float, float] = (0.1, 2),
        kernel_size: Optional[float] = 23,
        # Amplitude rescale
        ampl_rescale_range: Tuple[float, float] = (0.8, 1.75),
        ampl_rescale_prob: float = 0.2,
        # Phase shift
        phase_shift_range: Tuple[float, float] = (0.4, 0.7),
        phase_shift_prob: float = 0.2,
        # Random frequency mask
        rand_freq_mask_range: Tuple[float, float] = (0.01, 0.1),
        rand_freq_mask_prob: float = 0.5,
        # Gaussian mixture mask
        gmm_num_gaussians: int = 20,
        gmm_std_range: Tuple[float, float] = (10, 15),
        gmm_prob: float = 0.0,
        # Other
        solarization_prob: float = 0.0,
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        color_jitter = T.ColorJitter(
            brightness=cj_strength * cj_bright,
            contrast=cj_strength * cj_contrast,
            saturation=cj_strength * cj_sat,
            hue=cj_strength * cj_hue,
        )

        transform = [
            T.RandomResizedCrop(size=input_size, scale=(min_scale, 1.0)),
            T.ToTensor(),
            RFFT2DTransform(),
            T.RandomApply(
                [AmplitudeRescaleTransform(range=ampl_rescale_range)],
                p=ampl_rescale_prob,
            ),
            T.RandomApply(
                [PhaseShiftTransform(range=phase_shift_range)], p=phase_shift_prob
            ),
            T.RandomApply(
                [RandomFrequencyMaskTransform(k=rand_freq_mask_range)],
                p=rand_freq_mask_prob,
            ),
            T.RandomApply(
                [
                    GaussianMixtureMask(
                        num_gaussians=gmm_num_gaussians, std_range=gmm_std_range
                    )
                ],
                p=gmm_prob,
            ),
            IRFFT2DTransform(shape=(input_size, input_size)),
            T.ToPILImage(),
            T.RandomHorizontalFlip(p=hf_prob),
            T.RandomVerticalFlip(p=vf_prob),
            random_rotation_transform(rr_prob=rr_prob, rr_degrees=rr_degrees),
            T.RandomApply([color_jitter], p=cj_prob),
            T.RandomGrayscale(p=random_gray_scale),
            GaussianBlur(kernel_size=kernel_size, sigmas=sigmas, prob=gaussian_blur),
            RandomSolarization(prob=solarization_prob),
            T.ToTensor(),
        ]
        if normalize:
            transform += [T.Normalize(mean=normalize["mean"], std=normalize["std"])]
        self.transform = T.Compose(transform)

    def __call__(self, image: Union[Tensor, Image]) -> Tensor:
        """
        Applies the transforms to the input image.

        Args:
            image:
                The input image to apply the transforms to.

        Returns:
            The transformed image.

        """
        transformed: Tensor = self.transform(image)
        return transformed


class FDATransform(MultiViewTransform):
    """Implements the transformations for FDA[0].

        Input to this transform:
            PIL Image or Tensor.

        Output of this transform:
            List of Tensor of length 2.

        Applies the following augmentations by default:

            - Random resized crop
            - RFFT 2D transform
            - Amplitude rescale transform
            - Phase shift transform
            - Random frequency mask transform
            - Gaussian mixture mask
            - IRFFT 2D transform
            - Color jitter
            - Random grayscale
            - Gaussian blur
            - Random solarization
            - Random horizontal flip

    - [0]: Disentangling the Effects of Data Augmentation and Format Transform in
    Self-Supervised Learning of Image Representations, 2023, https://arxiv.org/pdf/2312.02205

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of [tensor, tensor].

    Attributes:
        view_1_transform: The transform for the first view.
        view_2_transform: The transform for the second view.
    """

    def __init__(
        self,
        view_1_transform: Optional[FDAView1Transform] = None,
        view_2_transform: Optional[FDAView2Transform] = None,
    ):
        # We need to initialize the transforms here
        view_1_transform = view_1_transform or FDAView1Transform()
        view_2_transform = view_2_transform or FDAView2Transform()
        super().__init__(transforms=[view_1_transform, view_2_transform])



================================================
FILE: lightly/transforms/gaussian_blur.py
================================================
# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

from typing import Optional, Tuple, Union
from warnings import warn

import numpy as np
from PIL import ImageFilter
from PIL.Image import Image
from torch import Tensor

from lightly.transforms.torchvision_v2_compatibility import functional as F


class GaussianBlur:
    """Implementation of random Gaussian blur.

    Utilizes the built-in ImageFilter method from PIL to apply a Gaussian
    blur to the input image with a certain probability. The blur is further
    randomized by sampling uniformly the values of the standard deviation of
    the Gaussian kernel.

    Attributes:
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        prob:
            Probability with which the blur is applied.
        scale:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to scale the `kernel_size` of a factor of `kernel_scale`
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.

    """

    def __init__(
        self,
        kernel_size: Optional[float] = None,
        prob: float = 0.5,
        scale: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.2, 2),
    ):
        if scale != None or kernel_size != None:
            warn(
                "The 'kernel_size' and 'scale' arguments of the GaussianBlur augmentation will be deprecated.  "
                "Please use the 'sigmas' parameter instead.",
                DeprecationWarning,
            )
        self.prob = prob
        self.sigmas = sigmas

    def __call__(self, sample: Union[Tensor, Image]) -> Union[Tensor, Image]:
        """Blurs the image with a given probability.

        Args:
            sample:
                PIL image to which blur will be applied.

        Returns:
            Blurred image or original image.
        """
        prob = np.random.random_sample()

        # Convert to PIL image if it's a tensor, otherwise use as is
        is_input_tensor = isinstance(sample, Tensor)
        sample_pil: Image = F.to_pil_image(sample) if is_input_tensor else sample

        if prob < self.prob:
            # choose randomized std for Gaussian filtering
            sigma = np.random.uniform(self.sigmas[0], self.sigmas[1])
            # PIL GaussianBlur https://github.com/python-pillow/Pillow/blob/76478c6865c78af10bf48868345db2af92f86166/src/PIL/ImageFilter.py#L154 label the
            # sigma parameter of the gaussian filter as radius. Before, the radius of the patch was passed as the argument.
            # The issue was addressed here https://github.com/lightly-ai/lightly/issues/1051 and solved by AurelienGauffre.
            sample_pil = sample_pil.filter(ImageFilter.GaussianBlur(radius=sigma))

        # Convert back to tensor if input was a tensor
        return F.to_tensor(sample_pil) if is_input_tensor else sample_pil



================================================
FILE: lightly/transforms/gaussian_mixture_masks_transform.py
================================================
from typing import Tuple

import torch
import torch.fft
from torch import Tensor


class GaussianMixtureMask:
    """Applies a Gaussian Mixture Mask in the Fourier domain to an image.

    The mask is created using random Gaussian kernels, which are applied in
    the frequency domain.

    Attributes:
        num_gaussians: Number of Gaussian kernels to generate in the mixture mask.
        std_range: Tuple containing the minimum and maximum standard deviation for the Gaussians.
    """

    def __init__(
        self, num_gaussians: int = 20, std_range: Tuple[float, float] = (10, 15)
    ):
        """Initializes GaussianMixtureMasks with the given parameters.

        Args:
            num_gaussians: Number of Gaussian kernels to generate in the mixture mask.
            std_range: Tuple containing the minimum and maximum standard deviation for the Gaussians.
        """
        self.num_gaussians = num_gaussians
        self.std_range = std_range

    def gaussian_kernel(
        self, size: Tuple[int, int], sigma: Tensor, center: Tensor
    ) -> Tensor:
        """Generates a 2D Gaussian kernel.

        Args:
            size: Tuple specifying the dimensions of the Gaussian kernel (H, W).
            sigma: Tensor specifying the standard deviation of the Gaussian.
            center: Tensor specifying the center of the Gaussian kernel.

        Returns:
            A 2D Gaussian kernel tensor.
        """
        u, v = torch.meshgrid(torch.arange(0, size[0]), torch.arange(0, size[1]))
        u = u.to(sigma.device)
        v = v.to(sigma.device)
        u0, v0 = center
        gaussian = torch.exp(
            -((u - u0) ** 2 / (2 * sigma[0] ** 2) + (v - v0) ** 2 / (2 * sigma[1] ** 2))
        )

        return gaussian

    def apply_gaussian_mixture_mask(
        self, freq_image: Tensor, num_gaussians: int, std: Tuple[float, float]
    ) -> Tensor:
        """Applies the Gaussian mixture mask to a frequency-domain image.

        Args:
            freq_image: Tensor representing the frequency-domain image of shape (C, H, W//2+1).
            num_gaussians: Number of Gaussian kernels to generate in the mask.
            std: Tuple specifying the standard deviation range for the Gaussians.

        Returns:
            Image tensor in frequency domain after applying the Gaussian mixture mask.
        """
        (C, U, V) = freq_image.shape
        mask = freq_image.new_ones(freq_image.shape)

        for _ in range(num_gaussians):
            u0 = torch.randint(0, U, (1,), device=freq_image.device)
            v0 = torch.randint(0, V, (1,), device=freq_image.device)
            center = torch.tensor((u0, v0), device=freq_image.device)
            sigma = torch.rand(2, device=freq_image.device) * (std[1] - std[0]) + std[0]

            g_kernel = self.gaussian_kernel((U, V), sigma, center)
            mask *= 1 - g_kernel.unsqueeze(0)

        filtered_freq_image = freq_image * mask
        return filtered_freq_image

    def __call__(self, freq_image: Tensor) -> Tensor:
        """Applies the Gaussian mixture mask transformation to the input frequency-domain image.

        Args:
            freq_image: Tensor representing a frequency-domain image of shape (C, H, W//2+1).

        Returns:
            Image tensor in frequency domain after applying the Gaussian mixture mask.
        """
        return self.apply_gaussian_mixture_mask(
            freq_image, self.num_gaussians, self.std_range
        )



================================================
FILE: lightly/transforms/ibot_transform.py
================================================
from typing import Dict, List, Optional, Tuple, Union

import PIL
from PIL.Image import Image
from torch import Tensor

from lightly.transforms.gaussian_blur import GaussianBlur
from lightly.transforms.multi_view_transform import MultiViewTransform
from lightly.transforms.solarize import RandomSolarization
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.transforms.utils import IMAGENET_NORMALIZE


class IBOTTransform(MultiViewTransform):
    """Implements the global and local view augmentations for iBOT [0].

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length 2 * global + n_local_views. (10 by default)

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip
        - Color jitter
        - Random gray scale
        - Gaussian blur
        - Random solarization
        - ImageNet normalization

    This class generates two global and a user defined number of local views
    for each image in a batch. The code is adapted from [1].

    - [0]: iBOT, 2021, https://arxiv.org/abs/2111.07832
    - [1]: https://github.com/bytedance/ibot/blob/main/main_ibot.py#L574

    Attributes:
        global_crop_size:
            Crop size of the global views.
        global_crop_scale:
            Tuple of min and max scales relative to global_crop_size.
        local_crop_size:
            Crop size of the local views.
        local_crop_scale:
            Tuple of min and max scales relative to local_crop_size.
        n_local_views:
            Number of generated local views.
        hf_prob:
            Probability that horizontal flip is applied.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter. `cj_bright`, `cj_contrast`, `cj_sat`, and
            `cj_hue` are multiplied by this value.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Tuple of probabilities to apply gaussian blur on the different
            views. The input is ordered as follows:
            (global_view_0, global_view_1, local_views)
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        kernel_scale:
            Old argument. Value is deprecated in favor of sigmas. If set, the old behavior applies and `sigmas` is ignored.
            Used to scale the `kernel_size` of a factor of `kernel_scale`
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        solarization:
            Probability to apply solarization on the second global view.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        global_crop_size: int = 224,
        global_crop_scale: Tuple[float, float] = (0.25, 1.0),
        local_crop_size: int = 96,
        local_crop_scale: Tuple[float, float] = (0.05, 0.25),
        n_local_views: int = 10,
        hf_prob: float = 0.5,
        cj_prob: float = 0.8,
        cj_strength: float = 0.5,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.4,
        cj_hue: float = 0.2,
        random_gray_scale: float = 0.2,
        gaussian_blur: Tuple[float, float, float] = (1.0, 0.1, 0.5),
        kernel_size: Optional[float] = None,
        kernel_scale: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        solarization_prob: float = 0.2,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        # first global crop
        global_transform_0 = IBOTViewTransform(
            crop_size=global_crop_size,
            crop_scale=global_crop_scale,
            hf_prob=hf_prob,
            cj_prob=cj_prob,
            cj_strength=cj_strength,
            cj_bright=cj_bright,
            cj_contrast=cj_contrast,
            cj_hue=cj_hue,
            cj_sat=cj_sat,
            random_gray_scale=random_gray_scale,
            gaussian_blur=gaussian_blur[0],
            kernel_size=kernel_size,
            kernel_scale=kernel_scale,
            sigmas=sigmas,
            solarization_prob=0,
            normalize=normalize,
        )

        # second global crop
        global_transform_1 = IBOTViewTransform(
            crop_size=global_crop_size,
            crop_scale=global_crop_scale,
            hf_prob=hf_prob,
            cj_prob=cj_prob,
            cj_bright=cj_bright,
            cj_contrast=cj_contrast,
            cj_hue=cj_hue,
            cj_sat=cj_sat,
            random_gray_scale=random_gray_scale,
            gaussian_blur=gaussian_blur[1],
            kernel_size=kernel_size,
            kernel_scale=kernel_scale,
            sigmas=sigmas,
            solarization_prob=solarization_prob,
            normalize=normalize,
        )

        # transformation for the local small crops
        local_transform = IBOTViewTransform(
            crop_size=local_crop_size,
            crop_scale=local_crop_scale,
            hf_prob=hf_prob,
            cj_prob=cj_prob,
            cj_strength=cj_strength,
            cj_bright=cj_bright,
            cj_contrast=cj_contrast,
            cj_hue=cj_hue,
            cj_sat=cj_sat,
            random_gray_scale=random_gray_scale,
            gaussian_blur=gaussian_blur[2],
            kernel_size=kernel_size,
            kernel_scale=kernel_scale,
            sigmas=sigmas,
            solarization_prob=0,
            normalize=normalize,
        )
        local_transforms = [local_transform] * n_local_views
        transforms = [global_transform_0, global_transform_1]
        transforms.extend(local_transforms)
        super().__init__(transforms)


class IBOTViewTransform:
    def __init__(
        self,
        crop_size: int = 224,
        crop_scale: Tuple[float, float] = (0.4, 1.0),
        hf_prob: float = 0.5,
        cj_prob: float = 0.8,
        cj_strength: float = 0.5,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.4,
        cj_hue: float = 0.2,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 1.0,
        kernel_size: Optional[float] = None,
        kernel_scale: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        solarization_prob: float = 0.2,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        transform = [
            T.RandomResizedCrop(
                size=crop_size,
                scale=crop_scale,
                # Type ignore needed because BICUBIC is not recognized as an attribute.
                interpolation=PIL.Image.BICUBIC,  # type: ignore[attr-defined]
            ),
            T.RandomHorizontalFlip(p=hf_prob),
            T.RandomApply(
                [
                    T.ColorJitter(
                        brightness=cj_strength * cj_bright,
                        contrast=cj_strength * cj_contrast,
                        saturation=cj_strength * cj_sat,
                        hue=cj_strength * cj_hue,
                    )
                ],
                p=cj_prob,
            ),
            T.RandomGrayscale(p=random_gray_scale),
            GaussianBlur(
                kernel_size=kernel_size,
                scale=kernel_scale,
                sigmas=sigmas,
                prob=gaussian_blur,
            ),
            RandomSolarization(prob=solarization_prob),
            T.ToTensor(),
        ]

        if normalize:
            transform += [T.Normalize(mean=normalize["mean"], std=normalize["std"])]
        self.transform = T.Compose(transform)

    def __call__(self, image: Union[Tensor, Image]) -> Tensor:
        """
        Applies the transforms to the input image.

        Args:
            image:
                The input image to apply the transforms to.

        Returns:
            The transformed image.

        """
        transformed: Tensor = self.transform(image)
        return transformed



================================================
FILE: lightly/transforms/ijepa_transform.py
================================================
from typing import Dict, List, Tuple, Union

from PIL.Image import Image
from torch import Tensor

from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.transforms.utils import IMAGENET_NORMALIZE


class IJEPATransform:
    """Implements the augmentations for I-JEPA [0, 1].

    Experimental: Support for I-JEPA is experimental, there might be breaking changes
    in the future.

    - [0]: Joint-Embedding Predictive Architecture, 2023, https://arxiv.org/abs/2301.08243
    - [1]: https://github.com/facebookresearch/ijepa

    Attributes:
        input_size:
            Size of the input image in pixels.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        input_size: Union[int, Tuple[int, int]] = 224,
        min_scale: float = 0.2,
        normalize: Dict[str, List[float]] = IMAGENET_NORMALIZE,
    ):
        transforms = [
            T.RandomResizedCrop(
                input_size, scale=(min_scale, 1.0), interpolation=3
            ),  # 3 is bicubic
            T.RandomHorizontalFlip(),
            T.ToTensor(),
        ]
        if normalize:
            transforms.append(T.Normalize(mean=normalize["mean"], std=normalize["std"]))

        self.transform = T.Compose(transforms)

    def __call__(self, image: Union[Tensor, Image]) -> Tensor:
        """Applies the transforms to the input image.

        Args:
            image:
                The input image to apply the transforms to.

        Returns:
            The transformed image.

        """
        transformed: Tensor = self.transform(image)
        return transformed



================================================
FILE: lightly/transforms/image_grid_transform.py
================================================
from typing import List, Sequence, Union

from PIL.Image import Image
from torch import Tensor

from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T


class ImageGridTransform:
    """Transforms an image into multiple views and grids.

    Used for VICRegL.

    Attributes:
        transforms: A sequence of (image_grid_transform, view_transform) tuples.
            The image_grid_transform creates a new view and grid from the image.
            The view_transform further augments the view. Every transform tuple
            is applied once to the image, creating len(transforms) views and
            grids.
    """

    def __init__(self, transforms: Sequence[T.Compose]):
        self.transforms = transforms

    def __call__(self, image: Union[Tensor, Image]) -> Union[List[Tensor], List[Image]]:
        """Transforms an image into multiple views.

        Every transform in self.transforms creates a new view.

        Args:
            image:
                Image to be transformed into multiple views and grids.

        Returns:
            List of views and grids tensors or PIL images. In the VICRegL implementation
            it has size:

            .. code-block:: python

                [
                    [3, global_crop_size, global_crop_size],
                    [3, local_crop_size, local_crop_size],
                    [global_grid_size, global_grid_size, 2],
                    [local_grid_size, local_grid_size, 2]
                ]

        """
        views, grids = [], []
        for image_grid_transform, view_transform in self.transforms:
            view, grid = image_grid_transform(image)
            views.append(view_transform(view))
            grids.append(grid)
        views += grids
        return views



================================================
FILE: lightly/transforms/irfft2d_transform.py
================================================
from typing import Tuple

import torch
from torch import Tensor


class IRFFT2DTransform:
    """Inverse 2D Fast Fourier Transform (IRFFT2D) Transformation.

    This transformation applies the inverse 2D Fast Fourier Transform (IRFFT2D)
    to an image in the frequency domain.

    Input:
        - Tensor of shape (C, H, W), where C is the number of channels.

    Output:
        - Tensor of shape (C, H, W), where C is the number of channels.
    """

    def __init__(self, shape: Tuple[int, int]):
        """
        Args:
            shape: The desired output shape (H, W) after applying the inverse FFT
        """
        self.shape = shape

    def __call__(self, freq_image: Tensor) -> Tensor:
        """Applies the inverse 2D Fast Fourier Transform (IRFFT2D) to the input tensor.

        Args:
            freq_image: A tensor in the frequency domain of shape (C, H, W).

        Returns:
            Tensor: Reconstructed image after applying IRFFT2D, of shape (C, H, W).
        """
        reconstructed_image: Tensor = torch.fft.irfft2(freq_image, s=self.shape)
        return reconstructed_image



================================================
FILE: lightly/transforms/jigsaw.py
================================================
# Copyright (c) 2021. Lightly AG and its affiliates.
# All Rights Reserved

from typing import TYPE_CHECKING, Callable, List

import numpy as np
import torch
from PIL import Image as Image
from PIL.Image import Image as PILImage
from torch import Tensor

from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T

if TYPE_CHECKING:
    from numpy.typing import NDArray


class Jigsaw(object):
    """Implementation of Jigsaw image augmentation, inspired from PyContrast library.

    Generates n_grid**2 random crops and returns a list.

    This augmentation is instrumental to PIRL.

    Attributes:
        n_grid:
            Side length of the meshgrid, sqrt of the number of crops.
        img_size:
            Size of image.
        crop_size:
            Size of crops.
        transform:
            Transformation to apply on each crop.

    Examples:
        >>> from lightly.transforms import Jigsaw
        >>>
        >>> jigsaw_crop = Jigsaw(n_grid=3, img_size=255, crop_size=64, transform=T.ToTensor())
        >>>
        >>> # img is a PIL image
        >>> crops = jigsaw_crops(img)
    """

    def __init__(
        self,
        n_grid: int = 3,
        img_size: int = 255,
        crop_size: int = 64,
        transform: T.Compose = T.ToTensor(),
    ):
        self.n_grid = n_grid
        self.img_size = img_size
        self.crop_size = crop_size
        self.grid_size = int(img_size / self.n_grid)
        self.side = self.grid_size - self.crop_size
        self.transform: Callable[[PILImage], Tensor] = transform

        yy, xx = np.meshgrid(np.arange(n_grid), np.arange(n_grid))
        self.yy = np.reshape(yy * self.grid_size, (n_grid * n_grid,))
        self.xx = np.reshape(xx * self.grid_size, (n_grid * n_grid,))

    def __call__(self, img: PILImage) -> Tensor:
        """Performs the Jigsaw augmentation
        Args:
            img:
                PIL image to perform Jigsaw augmentation on.

        Returns:
            Torch tensor with stacked crops.
        """
        r_x = np.random.randint(0, self.side + 1, self.n_grid * self.n_grid)
        r_y = np.random.randint(0, self.side + 1, self.n_grid * self.n_grid)
        img_arr = np.asarray(img, np.uint8)
        crops: List[NDArray[np.uint8]] = []
        for i in range(self.n_grid * self.n_grid):
            crops.append(
                img_arr[
                    self.xx[i] + r_x[i] : self.xx[i] + r_x[i] + self.crop_size,
                    self.yy[i] + r_y[i] : self.yy[i] + r_y[i] + self.crop_size,
                    :,
                ]
            )
        crop_images = [Image.fromarray(crop) for crop in crops]
        crop_tensors: Tensor = torch.stack(
            [self.transform(crop) for crop in crop_images]
        )
        permutation: List[int] = np.random.permutation(self.n_grid**2).tolist()
        return crop_tensors[permutation]



================================================
FILE: lightly/transforms/mae_transform.py
================================================
from typing import Dict, List, Tuple, Union

from PIL.Image import Image
from torch import Tensor

from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.transforms.utils import IMAGENET_NORMALIZE


class MAETransform:
    """Implements the view augmentation for MAE [0].

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length 1.

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip

    - [0]: Masked Autoencoder, 2021, https://arxiv.org/abs/2111.06377

    Attributes:
        input_size:
            Size of the input image in pixels.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        input_size: Union[int, Tuple[int, int]] = 224,
        min_scale: float = 0.2,
        normalize: Dict[str, List[float]] = IMAGENET_NORMALIZE,
    ):
        transforms = [
            T.RandomResizedCrop(
                input_size, scale=(min_scale, 1.0), interpolation=3
            ),  # 3 is bicubic
            T.RandomHorizontalFlip(),
            T.ToTensor(),
        ]
        if normalize:
            transforms.append(T.Normalize(mean=normalize["mean"], std=normalize["std"]))

        self.transform = T.Compose(transforms)

    def __call__(self, image: Union[Tensor, Image]) -> List[Tensor]:
        """
        Applies the transforms to the input image.

        Args:
            image:
                The input image to apply the transforms to.

        Returns:
            The transformed image.

        """
        return [self.transform(image)]



================================================
FILE: lightly/transforms/mmcr_transform.py
================================================
from typing import Dict, List, Optional, Tuple, Union

from lightly.transforms.byol_transform import BYOLView1Transform
from lightly.transforms.multi_view_transform import MultiViewTransform
from lightly.transforms.utils import IMAGENET_NORMALIZE


class MMCRTransform(MultiViewTransform):
    """Implements the transformations for MMCR[0], which
    are based on BYOL[1].

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length k.

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip
        - Color jitter
        - Random gray scale
        - Gaussian blur
        - Solarization
        - ImageNet normalization

    Please refer to the BYOL implementation for additional details.

    - [0]: Efficient Coding of Natural Images using Maximum Manifold Capacity
            Representations, 2023, https://arxiv.org/pdf/2303.03307.pdf
    - [1]: Bootstrap Your Own Latent, 2020, https://arxiv.org/pdf/2006.07733.pdf


    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of tensors of length k.

    Attributes:
        k: Number of views.
        transform: The transform to apply to each view.
    """

    def __init__(
        self,
        k: int = 8,
        input_size: int = 224,
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        cj_bright: float = 0.4,
        cj_contrast: float = 0.4,
        cj_sat: float = 0.2,
        cj_hue: float = 0.1,
        min_scale: float = 0.08,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 1.0,
        solarization_prob: float = 0.0,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        if k < 1:
            raise ValueError("k must be greater than or equal to 1")
        transform = BYOLView1Transform(
            input_size=input_size,
            cj_prob=cj_prob,
            cj_strength=cj_strength,
            cj_bright=cj_bright,
            cj_contrast=cj_contrast,
            cj_sat=cj_sat,
            cj_hue=cj_hue,
            min_scale=min_scale,
            random_gray_scale=random_gray_scale,
            gaussian_blur=gaussian_blur,
            solarization_prob=solarization_prob,
            kernel_size=kernel_size,
            sigmas=sigmas,
            vf_prob=vf_prob,
            hf_prob=hf_prob,
            rr_prob=rr_prob,
            rr_degrees=rr_degrees,
            normalize=normalize,
        )
        super().__init__(transforms=[transform] * k)



================================================
FILE: lightly/transforms/moco_transform.py
================================================
from typing import Dict, List, Optional, Tuple, Union

from lightly.transforms.simclr_transform import SimCLRTransform
from lightly.transforms.utils import IMAGENET_NORMALIZE


class MoCoV1Transform(SimCLRTransform):
    """Implements the transformations for MoCo v1.

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length 2.

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip
        - Color jitter
        - Random gray scale
        - ImageNet normalization

    Attributes:
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter. `cj_bright`, `cj_contrast`, `cj_sat`, and
            `cj_hue` are multiplied by this value.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        vf_prob:
            Probability that vertical flip is applied.
        hf_prob:
            Probability that horizontal flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None,
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple,
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        input_size: int = 224,
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        cj_bright: float = 0.4,
        cj_contrast: float = 0.4,
        cj_sat: float = 0.4,
        cj_hue: float = 0.4,
        min_scale: float = 0.2,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 0.0,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        super().__init__(
            input_size=input_size,
            cj_prob=cj_prob,
            cj_strength=cj_strength,
            cj_bright=cj_bright,
            cj_contrast=cj_contrast,
            cj_sat=cj_sat,
            cj_hue=cj_hue,
            min_scale=min_scale,
            random_gray_scale=random_gray_scale,
            gaussian_blur=gaussian_blur,
            kernel_size=kernel_size,
            sigmas=sigmas,
            vf_prob=vf_prob,
            hf_prob=hf_prob,
            rr_prob=rr_prob,
            rr_degrees=rr_degrees,
            normalize=normalize,
        )


class MoCoV2Transform(SimCLRTransform):
    """Implements the transformations for MoCo v2 [0].

    Similar to SimCLRTransform, but with different values for color jittering and
    minimum scale of the random resized crop.

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length 2.

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip
        - Color jitter
        - Random gray scale
        - Gaussian blur
        - ImageNet normalization

    - [0]: MoCo v2, 2020, https://arxiv.org/abs/2003.04297

    Attributes:
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter. `cj_bright`, `cj_contrast`, `cj_sat`, and
            `cj_hue` are multiplied by this value. For datasets with small images,
            such as CIFAR, it is recommended to set `cj_strenght` to 0.5.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        vf_prob:
            Probability that vertical flip is applied.
        hf_prob:
            Probability that horizontal flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None,
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple,
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        input_size: int = 224,
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        cj_bright: float = 0.4,
        cj_contrast: float = 0.4,
        cj_sat: float = 0.4,
        cj_hue: float = 0.1,
        min_scale: float = 0.2,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        super().__init__(
            input_size=input_size,
            cj_prob=cj_prob,
            cj_strength=cj_strength,
            cj_bright=cj_bright,
            cj_contrast=cj_contrast,
            cj_sat=cj_sat,
            cj_hue=cj_hue,
            min_scale=min_scale,
            random_gray_scale=random_gray_scale,
            gaussian_blur=gaussian_blur,
            kernel_size=kernel_size,
            sigmas=sigmas,
            vf_prob=vf_prob,
            hf_prob=hf_prob,
            rr_prob=rr_prob,
            rr_degrees=rr_degrees,
            normalize=normalize,
        )



================================================
FILE: lightly/transforms/msn_transform.py
================================================
from typing import Dict, List, Optional, Tuple, Union

from PIL.Image import Image
from torch import Tensor

from lightly.transforms.gaussian_blur import GaussianBlur
from lightly.transforms.multi_view_transform import MultiViewTransform
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.transforms.utils import IMAGENET_NORMALIZE


class MSNTransform(MultiViewTransform):
    """Implements the transformations for MSN [0].

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length 2 * random_views + focal_views. (12 by default)

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip
        - Color jitter
        - Random gray scale
        - Gaussian blur
        - ImageNet normalization

    Generates a set of random and focal views for each input image. The generated output
    is (views, target, filenames) where views is list with the following entries:
    [random_views_0, random_views_1, ..., focal_views_0, focal_views_1, ...].

    - [0]: Masked Siamese Networks, 2022: https://arxiv.org/abs/2204.07141

    Attributes:
        random_size:
            Size of the random image views in pixels.
        focal_size:
            Size of the focal image views in pixels.
        random_views:
            Number of random views to generate.
        focal_views:
            Number of focal views to generate.
        random_crop_scale:
            Minimum and maximum size of the randomized crops for the relative to random_size.
        focal_crop_scale:
            Minimum and maximum size of the randomized crops relative to focal_size.
        cj_prob:
            Probability that color jittering is applied.
        cj_strength:
            Strength of the color jitter. `cj_bright`, `cj_contrast`, `cj_sat`, and
            `cj_hue` are multiplied by this value.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        random_gray_scale:
            Probability of conversion to grayscale.
        hf_prob:
            Probability that horizontal flip is applied.
        vf_prob:
            Probability that vertical flip is applied.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.
    """

    def __init__(
        self,
        random_size: int = 224,
        focal_size: int = 96,
        random_views: int = 2,
        focal_views: int = 10,
        random_crop_scale: Tuple[float, float] = (0.3, 1.0),
        focal_crop_scale: Tuple[float, float] = (0.05, 0.3),
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.8,
        cj_hue: float = 0.2,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        random_gray_scale: float = 0.2,
        hf_prob: float = 0.5,
        vf_prob: float = 0.0,
        normalize: Dict[str, List[float]] = IMAGENET_NORMALIZE,
    ):
        random_view_transform = MSNViewTransform(
            crop_size=random_size,
            crop_scale=random_crop_scale,
            cj_prob=cj_prob,
            cj_strength=cj_strength,
            cj_bright=cj_bright,
            cj_contrast=cj_contrast,
            cj_sat=cj_sat,
            cj_hue=cj_hue,
            gaussian_blur=gaussian_blur,
            kernel_size=kernel_size,
            sigmas=sigmas,
            random_gray_scale=random_gray_scale,
            hf_prob=hf_prob,
            vf_prob=vf_prob,
            normalize=normalize,
        )
        focal_view_transform = MSNViewTransform(
            crop_size=focal_size,
            crop_scale=focal_crop_scale,
            cj_prob=cj_prob,
            cj_strength=cj_strength,
            gaussian_blur=gaussian_blur,
            kernel_size=kernel_size,
            sigmas=sigmas,
            random_gray_scale=random_gray_scale,
            hf_prob=hf_prob,
            vf_prob=vf_prob,
            normalize=normalize,
        )
        transforms = [random_view_transform] * random_views
        transforms += [focal_view_transform] * focal_views
        super().__init__(transforms=transforms)


class MSNViewTransform:
    def __init__(
        self,
        crop_size: int = 224,
        crop_scale: Tuple[float, float] = (0.3, 1.0),
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.8,
        cj_hue: float = 0.2,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        random_gray_scale: float = 0.2,
        hf_prob: float = 0.5,
        vf_prob: float = 0.0,
        normalize: Dict[str, List[float]] = IMAGENET_NORMALIZE,
    ):
        color_jitter = T.ColorJitter(
            brightness=cj_strength * cj_bright,
            contrast=cj_strength * cj_contrast,
            saturation=cj_strength * cj_sat,
            hue=cj_strength * cj_hue,
        )
        transform = [
            T.RandomResizedCrop(size=crop_size, scale=crop_scale),
            T.RandomHorizontalFlip(p=hf_prob),
            T.RandomVerticalFlip(p=vf_prob),
            T.RandomApply([color_jitter], p=cj_prob),
            T.RandomGrayscale(p=random_gray_scale),
            GaussianBlur(kernel_size=kernel_size, sigmas=sigmas, prob=gaussian_blur),
            T.ToTensor(),
            T.Normalize(mean=normalize["mean"], std=normalize["std"]),
        ]

        self.transform = T.Compose(transform)

    def __call__(self, image: Union[Tensor, Image]) -> Tensor:
        """
        Applies the transforms to the input image.

        Args:
            image:
                The input image to apply the transforms to.

        Returns:
            The transformed image.

        """
        transformed: Tensor = self.transform(image)
        return transformed



================================================
FILE: lightly/transforms/multi_crop_transform.py
================================================
from typing import Tuple

from lightly.transforms.multi_view_transform import MultiViewTransform
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T


class MultiCropTranform(MultiViewTransform):
    """Implements the multi-crop transformations. Used by Swav.

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length crop_counts.

    Applies the following augmentations by default:
        - Random resized crop
        - transforms passed by constructor

    Attributes:
        crop_sizes:
            Size of the input image in pixels for each crop category.
        crop_counts:
            Number of crops for each crop category.
        crop_min_scales:
            Min scales for each crop category.
        crop_max_scales:
            Max_scales for each crop category.
        transforms:
            Transforms which are applied to all crops.

    """

    def __init__(
        self,
        crop_sizes: Tuple[int, ...],
        crop_counts: Tuple[int, ...],
        crop_min_scales: Tuple[float, ...],
        crop_max_scales: Tuple[float, ...],
        transforms: T.Compose,
    ):
        if len(crop_sizes) != len(crop_counts):
            raise ValueError(
                "Length of crop_sizes and crop_counts must be equal but are"
                f" {len(crop_sizes)} and {len(crop_counts)}."
            )
        if len(crop_sizes) != len(crop_min_scales):
            raise ValueError(
                "Length of crop_sizes and crop_min_scales must be equal but are"
                f" {len(crop_sizes)} and {len(crop_min_scales)}."
            )
        if len(crop_sizes) != len(crop_max_scales):
            raise ValueError(
                "Length of crop_sizes and crop_max_scales must be equal but are"
                f" {len(crop_sizes)} and {len(crop_max_scales)}."
            )

        crop_transforms = []
        for i in range(len(crop_sizes)):
            random_resized_crop = T.RandomResizedCrop(
                crop_sizes[i], scale=(crop_min_scales[i], crop_max_scales[i])
            )

            crop_transforms.extend(
                [
                    T.Compose(
                        [
                            random_resized_crop,
                            transforms,
                        ]
                    )
                ]
                * crop_counts[i]
            )
        super().__init__(crop_transforms)



================================================
FILE: lightly/transforms/multi_view_transform.py
================================================
from typing import List, Sequence, Union

from PIL.Image import Image
from torch import Tensor

from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T


class MultiViewTransform:
    """Transforms an image into multiple views.

    Args:
        transforms:
            A sequence of transforms. Every transform creates a new view.

    """

    def __init__(self, transforms: Sequence[T.Compose]):
        self.transforms = transforms

    def __call__(self, image: Union[Tensor, Image]) -> Union[List[Tensor], List[Image]]:
        """Transforms an image into multiple views.

        Every transform in self.transforms creates a new view.

        Args:
            image:
                Image to be transformed into multiple views.

        Returns:
            List of views.

        """
        return [transform(image) for transform in self.transforms]



================================================
FILE: lightly/transforms/multi_view_transform_v2.py
================================================
from typing import Any, List, Sequence

from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T


class MultiViewTransformV2:
    """Transforms an image into multiple views and is compatible with transforms v2.

    Args:
        transforms:
            A sequence of v2 transforms. Every transform creates a new view.
    """

    def __init__(self, transforms: Sequence[T.Compose]):
        self.transforms = transforms

    def __call__(self, *args: Any) -> List[Any]:
        """Transforms a data structure containing images, bounding boxes and masks
        into a sequence of multiple views.

        Every transform in self.transforms creates a new view.

        Args:
            *args:
                Arbitary positional arguments consisting of arbitrary data structures
                containing images, bounding boxes and masks.

        Returns:
            A list of views, where each view is a transformed version of `*args`.

        """
        return [transform(*args) for transform in self.transforms]



================================================
FILE: lightly/transforms/phase_shift_transform.py
================================================
from typing import Tuple

import torch
from torch import Tensor
from torch.distributions import Uniform
from torch.distributions.bernoulli import Bernoulli


class PhaseShiftTransform:
    """Implementation of phase shifting transformation.


    Applies a random phase shift `theta` (positive or negative) to the Fourier spectrum (`freq_image`) of the image and returns the transformed spectrum.

    Attributes:
        dist:
            A uniform distribution in the range `[p, q)` from which the magnitude of the
            phase shift `theta` is selected.
        include_negatives:
            A flag indicating whether negative values of `theta` should be included.
            If `True`, both positive and negative shifts are applied.
        sign_dist:
            A Bernoulli distribution used to decide the sign of `theta`, based on a
            given probability `sign_probability`, if negative values are included.
    """

    def __init__(
        self,
        range: Tuple[float, float] = (0.4, 0.7),
        include_negatives: bool = False,
        sign_probability: float = 0.5,
    ) -> None:
        self.dist = Uniform(range[0], range[1])
        self.include_negatives = include_negatives
        if include_negatives:
            self.sign_dist = Bernoulli(sign_probability)

    def __call__(self, freq_image: Tensor) -> Tensor:
        # Calculate amplitude and phase
        amplitude = torch.sqrt(freq_image.real**2 + freq_image.imag**2)
        phase = torch.atan2(freq_image.imag, freq_image.real)

        # Sample a random phase shift θ
        theta = self.dist.sample().to(freq_image.device)

        if self.include_negatives:
            # Determine sign for shift: +θ or -θ
            sign = self.sign_dist.sample().to(freq_image.device)
            # Apply random sign directly to theta
            theta = torch.where(sign == 1, theta, -theta)

        # Adjust the phase
        phase_shifted = phase + theta

        # Recreate the complex spectrum with adjusted phase
        real = amplitude * torch.cos(phase_shifted)
        imag = amplitude * torch.sin(phase_shifted)
        output = torch.complex(real, imag)

        return output



================================================
FILE: lightly/transforms/pirl_transform.py
================================================
from typing import Dict, List, Tuple, Union

from lightly.transforms.jigsaw import Jigsaw
from lightly.transforms.multi_view_transform import MultiViewTransform
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.transforms.utils import IMAGENET_NORMALIZE


class PIRLTransform(MultiViewTransform):
    """Implements the transformations for PIRL [0]. The jigsaw augmentation
    is applied during the forward pass.

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length 2 (original, augmented).

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip
        - Color jitter
        - Random gray scale
        - Jigsaw puzzle

    - [0] PIRL, 2019: https://arxiv.org/abs/1912.01991

    Attributes:
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter. `cj_bright`, `cj_contrast`, `cj_sat`, and
            `cj_hue` are multiplied by this value.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        hf_prob:
            Probability that horizontal flip is applied.
        n_grid:
            Sqrt of the number of grids in the jigsaw image.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        input_size: Union[int, Tuple[int, int]] = 64,
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        cj_bright: float = 0.4,
        cj_contrast: float = 0.4,
        cj_sat: float = 0.4,
        cj_hue: float = 0.4,
        min_scale: float = 0.08,
        random_gray_scale: float = 0.2,
        hf_prob: float = 0.5,
        n_grid: int = 3,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        if isinstance(input_size, tuple):
            input_size_ = max(input_size)
        else:
            input_size_ = input_size

        # Cropping and normalisation for non-transformed image
        transforms_no_augment = [
            T.RandomResizedCrop(size=input_size, scale=(min_scale, 1.0)),
            T.ToTensor(),
        ]

        if normalize is not None:
            transforms_no_augment.append(
                T.Normalize(mean=normalize["mean"], std=normalize["std"])
            )

        no_augment = T.Compose(transforms_no_augment)

        color_jitter = T.ColorJitter(
            brightness=cj_strength * cj_bright,
            contrast=cj_strength * cj_contrast,
            saturation=cj_strength * cj_sat,
            hue=cj_strength * cj_hue,
        )

        # Transform for transformed jigsaw image
        transforms = [
            T.RandomHorizontalFlip(p=hf_prob),
            T.RandomApply([color_jitter], p=cj_prob),
            T.RandomGrayscale(p=random_gray_scale),
            T.ToTensor(),
        ]

        if normalize is not None:
            transforms.append(T.Normalize(mean=normalize["mean"], std=normalize["std"]))

        jigsaw = Jigsaw(
            n_grid=n_grid,
            img_size=input_size_,
            crop_size=int(input_size_ // n_grid),
            transform=T.Compose(transforms),
        )

        super().__init__([no_augment, jigsaw])



================================================
FILE: lightly/transforms/random_crop_and_flip_with_grid.py
================================================
from dataclasses import dataclass
from typing import Tuple

import torch
from PIL import Image
from torch import nn

from lightly.transforms.torchvision_v2_compatibility import functional as F
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T


@dataclass
class Location:
    # The row index of the top-left corner of the crop.
    top: float
    # The column index of the top-left corner of the crop.
    left: float
    # The height of the crop.
    height: float
    # The width of the crop.
    width: float
    # The height of the original image.
    image_height: float
    # The width of the original image.
    image_width: float
    # Whether to flip the image horizontally.
    horizontal_flip: bool = False
    # Whether to flip the image vertically.
    vertical_flip: bool = False


class RandomResizedCropWithLocation(T.RandomResizedCrop):  # type: ignore[misc] # Class cannot subclass "RandomResizedCrop" (has type "Any")
    """
    Do a random resized crop and return both the resulting image and the location. See base class.

    """

    def forward(self, img: Image.Image) -> Tuple[Image.Image, Location]:
        """
        Args:
            img (PIL Image or Tensor): Image to be cropped.
        Returns:
            PIL Image or Tensor: Randomly cropped image
            Location: Location object containing crop parameters

        """
        top, left, height, width = self.get_params(img, self.scale, self.ratio)
        image_width, image_height = F.get_image_size(img)
        location = Location(
            top=top,
            left=left,
            height=height,
            width=width,
            image_height=image_height,
            image_width=image_width,
        )
        img = F.resized_crop(
            img, top, left, height, width, self.size, self.interpolation
        )
        return img, location


class RandomHorizontalFlipWithLocation(T.RandomHorizontalFlip):  # type: ignore[misc] # Class cannot subclass "RandomHorizontalFlip" (has type "Any")
    """See base class."""

    def forward(
        self, img: Image.Image, location: Location
    ) -> Tuple[Image.Image, Location]:
        """Horizontal flip image.

        Horizontally flip the given image randomly with a given probability and
        return both the resulting image and the location.

        Args:
            img (PIL Image or Tensor): Image to be flipped..
            Location: Location object linked to the image
        Returns:
            PIL Image or Tensor: Randomly flipped image
            Location: Location object with updated location.horizontal_flip parameter
        """

        if torch.rand(1) < self.p:
            img = F.hflip(img)
            location.horizontal_flip = True
        return img, location


class RandomVerticalFlipWithLocation(T.RandomVerticalFlip):  # type: ignore[misc] # Class cannot subclass "RandomVerticalFlip" (has type "Any")
    """See base class."""

    def forward(
        self, img: Image.Image, location: Location
    ) -> Tuple[Image.Image, Location]:
        """Vertical flip image.

        Vertically flip the given image randomly with a given probability and
        return both the resulting image and the location.

        Args:
            img (PIL Image or Tensor): Image to be flipped..
            Location: Location object linked to the image
        Returns:
            PIL Image or Tensor: Randomly flipped image
            Location: Location object with updated location.vertical_flip parameter
        """

        if torch.rand(1) < self.p:
            img = F.vflip(img)
            location.vertical_flip = True
        return img, location


class RandomResizedCropAndFlip(nn.Module):  # type: ignore[misc] # Class cannot subclass "RandomResizedCropAndFlip" (has type "Any")
    """Randomly flip and crop an image.

    A PyTorch module that applies random cropping, horizontal and vertical flipping to an image,
    and returns the transformed image and a grid tensor used to map the image back to the
    original image space in an NxN grid.

    Args:
        grid_size:
            The number of grid cells in the output grid tensor.
        crop_size:
            The size (in pixels) of the random crops.
        crop_min_scale:
            The minimum scale factor for random resized crops.
        crop_max_scale:
            The maximum scale factor for random resized crops.
        hf_prob:
            The probability of applying horizontal flipping to the image.
        normalize:
            A dictionary containing the mean and std values for normalizing the image.
    """

    def __init__(
        self,
        grid_size: int = 7,
        crop_size: int = 224,
        crop_min_scale: float = 0.05,
        crop_max_scale: float = 0.2,
        hf_prob: float = 0.5,
        vf_prob: float = 0.5,
    ):
        super().__init__()
        self.grid_size = grid_size
        self.crop_size = crop_size
        self.crop_min_scale = crop_min_scale
        self.crop_max_scale = crop_max_scale
        self.hf_prob = hf_prob
        self.vf_prob = vf_prob
        self.resized_crop = RandomResizedCropWithLocation(
            size=self.crop_size, scale=(self.crop_min_scale, self.crop_max_scale)
        )
        self.horizontal_flip = RandomHorizontalFlipWithLocation(self.hf_prob)
        self.vertical_flip = RandomVerticalFlipWithLocation(self.vf_prob)

    def forward(self, img: Image.Image) -> Tuple[Image.Image, torch.Tensor]:
        """Applies random cropping and horizontal flipping to an image, and returns the
        transformed image and a grid tensor used to map the image back to the original image
        space in an NxN grid.

        Args:
            img: The input PIL image.

        Returns:
            A tuple containing the transformed PIL image and the grid tensor.
        """

        img, location = self.resized_crop.forward(img=img)
        img, location = self.horizontal_flip.forward(img, location)
        img, location = self.vertical_flip.forward(img, location)
        grid = self.location_to_NxN_grid(location=location)

        return img, grid

    def location_to_NxN_grid(self, location: Location) -> torch.Tensor:
        """Create grid from location object.

        Create a grid tensor with grid_size rows and grid_size columns, where each cell represents a region of
        the original image. The grid is used to map the cropped and transformed image back to the
        original image space.

        Args:
            location: An instance of the Location class, containing the location and size of the
                transformed image in the original image space.

        Returns:
            A grid tensor of shape (grid_size, grid_size, 2), where the last dimension represents the (x, y) coordinate
            of the center of each cell in the original image space.
        """

        cell_width = location.width / self.grid_size
        cell_height = location.height / self.grid_size
        x = torch.linspace(
            location.left, location.left + location.width, self.grid_size
        ) + (cell_width / 2)
        y = torch.linspace(
            location.top, location.top + location.height, self.grid_size
        ) + (cell_height / 2)
        if location.horizontal_flip:
            x = torch.flip(x, dims=[0])
        if location.vertical_flip:
            y = torch.flip(y, dims=[0])
        grid_x, grid_y = torch.meshgrid(x, y, indexing="xy")
        return torch.stack([grid_x, grid_y], dim=-1)



================================================
FILE: lightly/transforms/random_frequency_mask_transform.py
================================================
from typing import Tuple

import numpy as np
import torch
from torch import Tensor


class RandomFrequencyMaskTransform:
    """2D Random Frequency Mask Transformation.

    This transformation applies a binary mask on the fourier transform,
    across all channels. A proportion of k frequencies are set to 0 with this.

    Input
        - Tensor: RFFT of a 2D Image (C, H, W) C-> No. of Channels
    Output
        - Tensor: The masked RFFT of the image

    """

    def __init__(self, k: Tuple[float, float] = (0.01, 0.1)) -> None:
        self.k = k

    def __call__(self, fft_image: Tensor) -> Tensor:
        k = np.random.uniform(low=self.k[0], high=self.k[1])

        # Every mask for every channel will have same frequencies being turned off i.e. being set to zero
        mask = (
            torch.rand(fft_image.shape[1:], device=fft_image.device) > k
        )  # mask_type: (H, W)

        # Do not mask zero frequency mode to retain majority of the semantic information.
        # Please refer https://arxiv.org/abs/2312.02205
        mask[0, 0] = 1

        # Adding channel dimension
        mask = mask.unsqueeze(0)

        masked_frequency_spectrum_image = fft_image * mask

        return masked_frequency_spectrum_image



================================================
FILE: lightly/transforms/rfft2d_transform.py
================================================
from typing import Union

import torch
from torch import Tensor


class RFFT2DTransform:
    """2D Fast Fourier Transform (RFFT2D) Transformation.

    This transformation applies the 2D Fast Fourier Transform (RFFT2D)
    to an image, converting it from the spatial domain to the frequency domain.

    Input:
        - Tensor of shape (C, H, W), where C is the number of channels.

    Output:
        - Tensor of shape (C, H, W) in the frequency domain, where C is the number of channels.
    """

    def __call__(self, image: Tensor) -> Tensor:
        """Applies the 2D Fast Fourier Transform (RFFT2D) to the input image.

        Args:
            image: Input image as a Tensor of shape (C, H, W).

        Returns:
            Tensor: The image in the frequency domain after applying RFFT2D, of shape (C, H, W).
        """

        rfft_image: Tensor = torch.fft.rfft2(image)
        return rfft_image



================================================
FILE: lightly/transforms/rotation.py
================================================
# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

from typing import Callable, Tuple, Union

import numpy as np
from PIL.Image import Image
from torch import Tensor

from lightly.transforms.torchvision_v2_compatibility import functional as F
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T


class RandomRotate:
    """Implementation of random rotation.

    Randomly rotates an input image by a fixed angle. By default, we rotate
    the image by 90 degrees with a probability of 50%.

    This augmentation can be very useful for rotation invariant images such as
    in medical imaging or satellite imaginary.

    Attributes:
        prob:
            Probability with which image is rotated.
        angle:
            Angle by which the image is rotated. We recommend multiples of 90
            to prevent rasterization artifacts. If you pick numbers like
            90, 180, 270 the tensor will be rotated without introducing
            any artifacts.

    """

    def __init__(self, prob: float = 0.5, angle: int = 90):
        self.prob = prob
        self.angle = angle

    def __call__(self, image: Union[Image, Tensor]) -> Union[Image, Tensor]:
        """Rotates the image with a given probability.

        Args:
            image:
                PIL image or tensor which will be rotated.

        Returns:
            Rotated image or original image.

        """
        prob = np.random.random_sample()
        if prob < self.prob:
            image = F.rotate(image, self.angle)
        return image


class RandomRotateDegrees:
    """Random rotate image between two rotation angles with a random probability.

    Attributes:
        prob:
            Probability with which image is rotated.
        degrees:
            Range of degrees to select from. If degrees is a number instead of a sequence like (min, max),
            the range of degrees will be (-degrees, +degrees). The image is rotated counter-clockwise with
            a random angle in the (min, max) range or in the (-degrees, +degrees) range.

    """

    def __init__(self, prob: float, degrees: Union[float, Tuple[float, float]]):
        self.transform: Callable[
            [Union[Image, Tensor]], Union[Image, Tensor]
        ] = T.RandomApply([T.RandomRotation(degrees=degrees)], p=prob)

    def __call__(self, image: Union[Image, Tensor]) -> Union[Image, Tensor]:
        """Rotates the images with a given probability.

        Args:
            image:
                PIL image or tensor which will be rotated.

        Returns:
            Rotated image or original image.

        """
        return self.transform(image)


def random_rotation_transform(
    rr_prob: float,
    rr_degrees: Union[None, float, Tuple[float, float]],
) -> Union[RandomRotate, T.RandomApply]:
    if rr_degrees is None:
        # Random rotation by 90 degrees.
        return RandomRotate(prob=rr_prob, angle=90)
    else:
        # Random rotation with random angle defined by rr_degrees.
        return RandomRotateDegrees(prob=rr_prob, degrees=rr_degrees)



================================================
FILE: lightly/transforms/simclr_transform.py
================================================
from typing import Dict, List, Optional, Tuple, Union

from PIL.Image import Image
from torch import Tensor

from lightly.transforms.gaussian_blur import GaussianBlur
from lightly.transforms.multi_view_transform import MultiViewTransform
from lightly.transforms.rotation import random_rotation_transform
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.transforms.utils import IMAGENET_NORMALIZE


class SimCLRTransform(MultiViewTransform):
    """Implements the transformations for SimCLR [0, 1].

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length 2.

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip
        - Color jitter
        - Random gray scale
        - Gaussian blur
        - ImageNet normalization

    Note that SimCLR v1 and v2 use the same data augmentations.

    - [0]: SimCLR v1, 2020, https://arxiv.org/abs/2002.05709
    - [1]: SimCLR v2, 2020, https://arxiv.org/abs/2006.10029

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of [tensor, tensor].

    Attributes:
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter. `cj_bright`, `cj_contrast`, `cj_sat`, and
            `cj_hue` are multiplied by this value. For datasets with small images,
            such as CIFAR, it is recommended to set `cj_strenght` to 0.5.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        vf_prob:
            Probability that vertical flip is applied.
        hf_prob:
            Probability that horizontal flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None,
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple,
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        input_size: int = 224,
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.8,
        cj_hue: float = 0.2,
        min_scale: float = 0.08,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        view_transform = SimCLRViewTransform(
            input_size=input_size,
            cj_prob=cj_prob,
            cj_strength=cj_strength,
            cj_bright=cj_bright,
            cj_contrast=cj_contrast,
            cj_sat=cj_sat,
            cj_hue=cj_hue,
            min_scale=min_scale,
            random_gray_scale=random_gray_scale,
            gaussian_blur=gaussian_blur,
            kernel_size=kernel_size,
            sigmas=sigmas,
            vf_prob=vf_prob,
            hf_prob=hf_prob,
            rr_prob=rr_prob,
            rr_degrees=rr_degrees,
            normalize=normalize,
        )
        super().__init__(transforms=[view_transform, view_transform])


class SimCLRViewTransform:
    def __init__(
        self,
        input_size: int = 224,
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.8,
        cj_hue: float = 0.2,
        min_scale: float = 0.08,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        color_jitter = T.ColorJitter(
            brightness=cj_strength * cj_bright,
            contrast=cj_strength * cj_contrast,
            saturation=cj_strength * cj_sat,
            hue=cj_strength * cj_hue,
        )

        transform = [
            T.RandomResizedCrop(size=input_size, scale=(min_scale, 1.0)),
            T.RandomHorizontalFlip(p=hf_prob),
            T.RandomVerticalFlip(p=vf_prob),
            random_rotation_transform(rr_prob=rr_prob, rr_degrees=rr_degrees),
            T.RandomApply([color_jitter], p=cj_prob),
            T.RandomGrayscale(p=random_gray_scale),
            GaussianBlur(kernel_size=kernel_size, sigmas=sigmas, prob=gaussian_blur),
            T.ToTensor(),
        ]
        if normalize:
            transform += [T.Normalize(mean=normalize["mean"], std=normalize["std"])]
        self.transform = T.Compose(transform)

    def __call__(self, image: Union[Tensor, Image]) -> Tensor:
        """
        Applies the transforms to the input image.

        Args:
            image:
                The input image to apply the transforms to.

        Returns:
            The transformed image.

        """
        transformed: Tensor = self.transform(image)
        return transformed



================================================
FILE: lightly/transforms/simsiam_transform.py
================================================
from typing import Dict, List, Optional, Tuple, Union

from PIL.Image import Image
from torch import Tensor

from lightly.transforms.gaussian_blur import GaussianBlur
from lightly.transforms.multi_view_transform import MultiViewTransform
from lightly.transforms.rotation import random_rotation_transform
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.transforms.utils import IMAGENET_NORMALIZE


class SimSiamTransform(MultiViewTransform):
    """Implements the transformations for SimSiam.

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length 2.

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip
        - Color jitter
        - Random gray scale
        - Gaussian blur
        - ImageNet normalization

    Attributes:
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter. `cj_bright`, `cj_contrast`, `cj_sat`, and
            `cj_hue` are multiplied by this value. For datasets with small images,
            such as CIFAR, it is recommended to set `cj_strength` to 0.5.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        vf_prob:
            Probability that vertical flip is applied.
        hf_prob:
            Probability that horizontal flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None,
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple,
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        input_size: int = 224,
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        cj_bright: float = 0.4,
        cj_contrast: float = 0.4,
        cj_sat: float = 0.4,
        cj_hue: float = 0.1,
        min_scale: float = 0.2,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        view_transform = SimSiamViewTransform(
            input_size=input_size,
            cj_prob=cj_prob,
            cj_strength=cj_strength,
            cj_bright=cj_bright,
            cj_contrast=cj_contrast,
            cj_sat=cj_sat,
            cj_hue=cj_hue,
            min_scale=min_scale,
            random_gray_scale=random_gray_scale,
            gaussian_blur=gaussian_blur,
            kernel_size=kernel_size,
            sigmas=sigmas,
            vf_prob=vf_prob,
            hf_prob=hf_prob,
            rr_prob=rr_prob,
            rr_degrees=rr_degrees,
            normalize=normalize,
        )
        super().__init__(transforms=[view_transform, view_transform])


class SimSiamViewTransform:
    def __init__(
        self,
        input_size: int = 224,
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        cj_bright: float = 0.4,
        cj_contrast: float = 0.4,
        cj_sat: float = 0.4,
        cj_hue: float = 0.1,
        min_scale: float = 0.2,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        color_jitter = T.ColorJitter(
            brightness=cj_strength * cj_bright,
            contrast=cj_strength * cj_contrast,
            saturation=cj_strength * cj_sat,
            hue=cj_strength * cj_hue,
        )

        transform = [
            T.RandomResizedCrop(size=input_size, scale=(min_scale, 1.0)),
            T.RandomHorizontalFlip(p=hf_prob),
            T.RandomVerticalFlip(p=vf_prob),
            random_rotation_transform(rr_prob=rr_prob, rr_degrees=rr_degrees),
            T.RandomApply([color_jitter], p=cj_prob),
            T.RandomGrayscale(p=random_gray_scale),
            GaussianBlur(kernel_size=kernel_size, sigmas=sigmas, prob=gaussian_blur),
            T.ToTensor(),
        ]
        if normalize:
            transform += [T.Normalize(mean=normalize["mean"], std=normalize["std"])]
        self.transform = T.Compose(transform)

    def __call__(self, image: Union[Tensor, Image]) -> Tensor:
        """
        Applies the transforms to the input image.

        Args:
            image:
                The input image to apply the transforms to.

        Returns:
            The transformed image.

        """
        transformed: Tensor = self.transform(image)
        return transformed



================================================
FILE: lightly/transforms/smog_transform.py
================================================
from typing import Dict, List, Optional, Tuple, Union

from PIL.Image import Image
from torch import Tensor

from lightly.transforms.gaussian_blur import GaussianBlur
from lightly.transforms.multi_view_transform import MultiViewTransform
from lightly.transforms.solarize import RandomSolarization
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.transforms.utils import IMAGENET_NORMALIZE


class SMoGTransform(MultiViewTransform):
    """Implements the transformations for SMoG.

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length sum(crop_counts). (8 by default)

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip
        - Color jitter
        - Random gray scale
        - Gaussian blur
        - Random solarization
        - ImageNet normalization

    Attributes:
        crop_sizes:
            Size of the input image in pixels for each crop category.
        crop_counts:
            Number of crops for each crop category.
        crop_min_scales:
            Min scales for each crop category.
        crop_max_scales:
            Max_scales for each crop category.
        gaussian_blur_probs:
            Probability of Gaussian blur for each crop category.
        gaussian_blur_kernel_sizes:
            Deprecated values in favour of sigmas.
        gaussian_blur_sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
        solarize_probs:
            Probability of solarization for each crop category.
        hf_prob:
            Probability that horizontal flip is applied.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter. `cj_bright`, `cj_contrast`, `cj_sat`, and
            `cj_hue` are multiplied by this value.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        random_gray_scale:
            Probability of conversion to grayscale.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        crop_sizes: Tuple[int, int] = (224, 96),
        crop_counts: Tuple[int, int] = (4, 4),
        crop_min_scales: Tuple[float, float] = (0.2, 0.05),
        crop_max_scales: Tuple[float, float] = (1.0, 0.2),
        gaussian_blur_probs: Tuple[float, float] = (0.5, 0.1),
        gaussian_blur_kernel_sizes: Tuple[Optional[float], Optional[float]] = (
            None,
            None,
        ),
        gaussian_blur_sigmas: Tuple[float, float] = (0.1, 2),
        solarize_probs: Tuple[float, float] = (0.0, 0.2),
        hf_prob: float = 0.5,
        cj_prob: float = 1.0,
        cj_strength: float = 0.5,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.4,
        cj_hue: float = 0.2,
        random_gray_scale: float = 0.2,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        transforms = []
        for i in range(len(crop_sizes)):
            transforms.extend(
                [
                    SmoGViewTransform(
                        crop_size=crop_sizes[i],
                        crop_min_scale=crop_min_scales[i],
                        crop_max_scale=crop_max_scales[i],
                        gaussian_blur_prob=gaussian_blur_probs[i],
                        kernel_size=gaussian_blur_kernel_sizes[i],
                        sigmas=gaussian_blur_sigmas,
                        solarize_prob=solarize_probs[i],
                        hf_prob=hf_prob,
                        cj_prob=cj_prob,
                        cj_strength=cj_strength,
                        cj_bright=cj_bright,
                        cj_contrast=cj_contrast,
                        cj_sat=cj_sat,
                        cj_hue=cj_hue,
                        random_gray_scale=random_gray_scale,
                        normalize=normalize,
                    )
                ]
                * crop_counts[i]
            )

        super().__init__(transforms)


class SmoGViewTransform:
    def __init__(
        self,
        crop_size: int = 224,
        crop_min_scale: float = 0.2,
        crop_max_scale: float = 1.0,
        gaussian_blur_prob: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        solarize_prob: float = 0.0,
        hf_prob: float = 0.5,
        cj_prob: float = 1.0,
        cj_strength: float = 0.5,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.4,
        cj_hue: float = 0.2,
        random_gray_scale: float = 0.2,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        color_jitter = T.ColorJitter(
            brightness=cj_strength * cj_bright,
            contrast=cj_strength * cj_contrast,
            saturation=cj_strength * cj_sat,
            hue=cj_strength * cj_hue,
        )

        transform = [
            T.RandomResizedCrop(crop_size, scale=(crop_min_scale, crop_max_scale)),
            T.RandomHorizontalFlip(p=hf_prob),
            T.RandomApply([color_jitter], p=cj_prob),
            T.RandomGrayscale(p=random_gray_scale),
            GaussianBlur(
                kernel_size=kernel_size,
                prob=gaussian_blur_prob,
                sigmas=sigmas,
            ),
            RandomSolarization(prob=solarize_prob),
            T.ToTensor(),
        ]
        if normalize:
            transform += [T.Normalize(mean=normalize["mean"], std=normalize["std"])]
        self.transform = T.Compose(transform)

    def __call__(self, image: Union[Tensor, Image]) -> Tensor:
        """
        Applies the transforms to the input image.

        Args:
            image:
                The input image to apply the transforms to.

        Returns:
            The transformed image.

        """
        transformed: Tensor = self.transform(image)
        return transformed



================================================
FILE: lightly/transforms/solarize.py
================================================
# Copyright (c) 2021. Lightly AG and its affiliates.
# All Rights Reserved

import numpy as np
from PIL import ImageOps
from PIL.Image import Image as PILImage


class RandomSolarization(object):
    """Implementation of random image Solarization.

    Utilizes the integrated image operation `solarize` from Pillow. Solarization
    inverts all pixel values above a threshold (default: 128).

    Attributes:
        probability:
            Probability to apply the transformation
        threshold:
            Threshold for solarization.
    """

    def __init__(self, prob: float = 0.5, threshold: int = 128):
        self.prob = prob
        self.threshold = threshold

    def __call__(self, sample: PILImage) -> PILImage:
        """Solarizes the given input image

        Args:
            sample:
                PIL image to which solarize will be applied.

        Returns:
            Solarized image or original image.

        """
        prob = np.random.random_sample()
        if prob < self.prob:
            # return solarized image
            return ImageOps.solarize(sample, threshold=self.threshold)
        # return original image
        return sample



================================================
FILE: lightly/transforms/swav_transform.py
================================================
from typing import Dict, List, Optional, Tuple, Union

from PIL.Image import Image
from torch import Tensor

from lightly.transforms.gaussian_blur import GaussianBlur
from lightly.transforms.multi_crop_transform import MultiCropTranform
from lightly.transforms.rotation import random_rotation_transform
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.transforms.utils import IMAGENET_NORMALIZE


class SwaVTransform(MultiCropTranform):
    """Implements the multi-crop transformations for SwaV.

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length sum(crop_counts). (8 by default)

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip
        - Color jitter
        - Random gray scale
        - Gaussian blur
        - ImageNet normalization

    Attributes:
        crop_sizes:
            Size of the input image in pixels for each crop category.
        crop_counts:
            Number of crops for each crop category.
        crop_min_scales:
            Min scales for each crop category.
        crop_max_scales:
            Max_scales for each crop category.
        hf_prob:
            Probability that horizontal flip is applied.
        vf_prob:
            Probability that vertical flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None,
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple,
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter. `cj_bright`, `cj_contrast`, `cj_sat`, and
            `cj_hue` are multiplied by this value.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        random_gray_scale:
            Probability of conversion to grayscale.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        crop_sizes: Tuple[int, int] = (224, 96),
        crop_counts: Tuple[int, int] = (2, 6),
        crop_min_scales: Tuple[float, float] = (0.14, 0.05),
        crop_max_scales: Tuple[float, float] = (1.0, 0.14),
        hf_prob: float = 0.5,
        vf_prob: float = 0.0,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.8,
        cj_hue: float = 0.2,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        transforms = SwaVViewTransform(
            hf_prob=hf_prob,
            vf_prob=vf_prob,
            rr_prob=rr_prob,
            rr_degrees=rr_degrees,
            cj_prob=cj_prob,
            cj_strength=cj_strength,
            cj_bright=cj_bright,
            cj_contrast=cj_contrast,
            cj_sat=cj_sat,
            cj_hue=cj_hue,
            random_gray_scale=random_gray_scale,
            gaussian_blur=gaussian_blur,
            kernel_size=kernel_size,
            sigmas=sigmas,
            normalize=normalize,
        )

        super().__init__(
            crop_sizes=crop_sizes,
            crop_counts=crop_counts,
            crop_min_scales=crop_min_scales,
            crop_max_scales=crop_max_scales,
            transforms=transforms,
        )


class SwaVViewTransform:
    def __init__(
        self,
        hf_prob: float = 0.5,
        vf_prob: float = 0.0,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        cj_prob: float = 0.8,
        cj_strength: float = 1.0,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.8,
        cj_hue: float = 0.2,
        random_gray_scale: float = 0.2,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        color_jitter = T.ColorJitter(
            brightness=cj_strength * cj_bright,
            contrast=cj_strength * cj_contrast,
            saturation=cj_strength * cj_sat,
            hue=cj_strength * cj_hue,
        )

        transforms = [
            T.RandomHorizontalFlip(p=hf_prob),
            T.RandomVerticalFlip(p=vf_prob),
            random_rotation_transform(rr_prob=rr_prob, rr_degrees=rr_degrees),
            T.ColorJitter(),
            T.RandomApply([color_jitter], p=cj_prob),
            T.RandomGrayscale(p=random_gray_scale),
            GaussianBlur(kernel_size=kernel_size, sigmas=sigmas, prob=gaussian_blur),
            T.ToTensor(),
        ]
        if normalize:
            transforms += [T.Normalize(mean=normalize["mean"], std=normalize["std"])]

        self.transform = T.Compose(transforms)

    def __call__(self, image: Union[Tensor, Image]) -> Tensor:
        """
        Applies the transforms to the input image.

        Args:
            image:
                The input image to apply the transforms to.

        Returns:
            The transformed image.

        """
        transformed: Tensor = self.transform(image)
        return transformed



================================================
FILE: lightly/transforms/tico_transform.py
================================================
from lightly.transforms.byol_transform import (
    BYOLTransform,
    BYOLView1Transform,
    BYOLView2Transform,
)


class TiCoTransform(BYOLTransform):
    """Implements the transformations for TiCo[0]. These are the same as BYOL[1].

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length 2.

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip
        - Color jitter
        - Random gray scale
        - Gaussian blur
        - Solarization
        - ImageNet normalization

    Note that SimCLR v1 and v2 use similar augmentations. In detail, TiCo (and BYOL) has
    asymmetric gaussian blur and solarization. Furthermore, TiCo has weaker
    color jitter compared to SimCLR.

    - [0]: Jiachen Zhu et. al, 2022, Tico, https://arxiv.org/abs/2206.10698.pdf
    - [1]: Bootstrap Your Own Latent, 2020, https://arxiv.org/pdf/2006.07733.pdf

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of [tensor, tensor].

    Attributes:
        view_1_transform: The transform for the first view.
        view_2_transform: The transform for the second view.
    """


class TiCoView1Transform(BYOLView1Transform):
    """Alias for BYOLView1Transform."""


class TiCoView2Transform(BYOLView2Transform):
    """Alias for BYOLView2Transform."""



================================================
FILE: lightly/transforms/torchvision_v2_compatibility.py
================================================
#
# Copyright (c) Lightly AG and affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#
from typing import Any, Callable, Dict, List, Union

import torch
from PIL.Image import Image
from torch import Tensor

from lightly.utils import dependency as _dependency

if _dependency.torchvision_transforms_v2_available():
    from torchvision.transforms import v2 as _torchvision_transforms

    _TRANSFORMS_V2 = True
else:
    from torchvision import transforms as _torchvision_transforms

    _TRANSFORMS_V2 = False


class ToTensor:
    """Convert a PIL Image to a tensor with value normalization, similar to [0].

    This implementation is required since `torchvision.transforms.v2.ToTensor` is
    deprecated and will be removed in the future (see [1]).

    Input to this transform:
        PIL Image (H x W x C) of uint8 type in range [0,255]

    Output of this transform:
        torch.Tensor (C x H x W) of type torch.float32 in range [0.0, 1.0]

    - [0] https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html
    - [1] https://pytorch.org/vision/0.20/generated/torchvision.transforms.v2.ToTensor.html?highlight=totensor#torchvision.transforms.v2.ToTensor
    """

    def __init__(self) -> None:
        T = _torchvision_transforms
        if _TRANSFORMS_V2:
            self.transform: Callable[..., Tensor] = T.Compose(
                [T.ToImage(), T.ToDtype(dtype=torch.float32, scale=True)]
            )
        else:
            self.transform = T.ToTensor()

    def __call__(
        self,
        *args: Union[Tensor, Image],
        **kwargs: Dict[str, Any],
    ) -> Tensor:
        return self.transform(*args, **kwargs)

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}({self.transform})"


class DeprecatedShim:
    """Shim class to replace deprecated transforms.

    This replaces existing, (soon to be) deprecated transforms with custom
    input transforms for compatibility reasons.

    Attributes:
        custom_transforms (dict of str: Callable[..., Any])
    """

    def __init__(self, custom_transforms: Dict[str, Callable[..., Any]]):
        self._torchvision_transforms = _torchvision_transforms
        self._custom_transforms = custom_transforms

    def __getattr__(self, name: str) -> Any:
        if name in self._custom_transforms:
            return self._custom_transforms[name]
        return getattr(self._torchvision_transforms, name)

    def __dir__(self) -> List[str]:
        return dir(self._torchvision_transforms)


# Set the compatibility layer to the shim, providing
# the functions to replace.
torchvision_transforms: Any = DeprecatedShim(dict(ToTensor=ToTensor))
functional = torchvision_transforms.functional



================================================
FILE: lightly/transforms/utils.py
================================================
IMAGENET_NORMALIZE = {"mean": [0.485, 0.456, 0.406], "std": [0.229, 0.224, 0.225]}



================================================
FILE: lightly/transforms/vicreg_transform.py
================================================
from typing import Dict, List, Optional, Tuple, Union

from PIL.Image import Image
from torch import Tensor

from lightly.transforms.gaussian_blur import GaussianBlur
from lightly.transforms.multi_view_transform import MultiViewTransform
from lightly.transforms.rotation import random_rotation_transform
from lightly.transforms.solarize import RandomSolarization
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.transforms.utils import IMAGENET_NORMALIZE


class VICRegTransform(MultiViewTransform):
    """Implements the transformations for VICReg.

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length 2.

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip
        - Color jitter
        - Random gray scale
        - Random solarization
        - Gaussian blur
        - ImageNet normalization

    Similar to SimCLR transform but with extra solarization.

    Attributes:
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter. `cj_bright`, `cj_contrast`, `cj_sat`, and
            `cj_hue` are multiplied by this value.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability of conversion to grayscale.
        solarize_prob:
            Probability of solarization.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        vf_prob:
            Probability that vertical flip is applied.
        hf_prob:
            Probability that horizontal flip is applied.
        rr_prob:
            Probability that random rotation is applied.
        rr_degrees:
            Range of degrees to select from for random rotation. If rr_degrees is None,
            images are rotated by 90 degrees. If rr_degrees is a (min, max) tuple,
            images are rotated by a random angle in [min, max]. If rr_degrees is a
            single number, images are rotated by a random angle in
            [-rr_degrees, +rr_degrees]. All rotations are counter-clockwise.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.

    """

    def __init__(
        self,
        input_size: int = 224,
        cj_prob: float = 0.8,
        cj_strength: float = 0.5,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.4,
        cj_hue: float = 0.2,
        min_scale: float = 0.08,
        random_gray_scale: float = 0.2,
        solarize_prob: float = 0.1,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.1, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        view_transform = VICRegViewTransform(
            input_size=input_size,
            cj_prob=cj_prob,
            cj_strength=cj_strength,
            cj_bright=cj_bright,
            cj_contrast=cj_contrast,
            cj_sat=cj_sat,
            cj_hue=cj_hue,
            min_scale=min_scale,
            random_gray_scale=random_gray_scale,
            solarize_prob=solarize_prob,
            gaussian_blur=gaussian_blur,
            kernel_size=kernel_size,
            sigmas=sigmas,
            vf_prob=vf_prob,
            hf_prob=hf_prob,
            rr_prob=rr_prob,
            rr_degrees=rr_degrees,
            normalize=normalize,
        )
        super().__init__(transforms=[view_transform, view_transform])


class VICRegViewTransform:
    def __init__(
        self,
        input_size: int = 224,
        cj_prob: float = 0.8,
        cj_strength: float = 0.5,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.4,
        cj_hue: float = 0.2,
        min_scale: float = 0.08,
        random_gray_scale: float = 0.2,
        solarize_prob: float = 0.1,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[float] = None,
        sigmas: Tuple[float, float] = (0.2, 2),
        vf_prob: float = 0.0,
        hf_prob: float = 0.5,
        rr_prob: float = 0.0,
        rr_degrees: Optional[Union[float, Tuple[float, float]]] = None,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        color_jitter = T.ColorJitter(
            brightness=cj_strength * cj_bright,
            contrast=cj_strength * cj_contrast,
            saturation=cj_strength * cj_sat,
            hue=cj_strength * cj_hue,
        )

        transform = [
            T.RandomResizedCrop(size=input_size, scale=(min_scale, 1.0)),
            T.RandomHorizontalFlip(p=hf_prob),
            T.RandomVerticalFlip(p=vf_prob),
            random_rotation_transform(rr_prob=rr_prob, rr_degrees=rr_degrees),
            T.RandomApply([color_jitter], p=cj_prob),
            T.RandomGrayscale(p=random_gray_scale),
            RandomSolarization(prob=solarize_prob),
            GaussianBlur(kernel_size=kernel_size, sigmas=sigmas, prob=gaussian_blur),
            T.ToTensor(),
        ]
        if normalize:
            transform += [T.Normalize(mean=normalize["mean"], std=normalize["std"])]
        self.transform = T.Compose(transform)

    def __call__(self, image: Union[Tensor, Image]) -> Tensor:
        """
        Applies the transforms to the input image.

        Args:
            image:
                The input image to apply the transforms to.

        Returns:
            The transformed image.

        """
        transformed: Tensor = self.transform(image)
        return transformed



================================================
FILE: lightly/transforms/vicregl_transform.py
================================================
from typing import Dict, List, Optional, Tuple, Union

from PIL.Image import Image
from torch import Tensor

from lightly.transforms.gaussian_blur import GaussianBlur
from lightly.transforms.image_grid_transform import ImageGridTransform
from lightly.transforms.random_crop_and_flip_with_grid import RandomResizedCropAndFlip
from lightly.transforms.solarize import RandomSolarization
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.transforms.utils import IMAGENET_NORMALIZE


class VICRegLTransform(ImageGridTransform):
    """Transforms images for VICRegL.

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length n_global_views + n_local_views. (8 by default)

    Applies the following augmentations by default:
        - Random resized crop
        - Random horizontal flip
        - Color jitter
        - Random gray scale
        - Gaussian blur
        - Random solarization
        - ImageNet normalization

    - [0]: VICRegL, 2022, https://arxiv.org/abs/2210.01571

    Attributes:
        global_crop_size:
            Size of the input image in pixels for the global crop views.
        local_crop_size:
            Size of the input image in pixels for the local crop views.
        n_global_views:
            Number of global crop views to generate.
        n_local_views:
            Number of local crop views to generate. For ResNet backbones it is
            recommended to set this to 0, see [0].
        global_crop_scale:
            Min and max scales for the global crop views.
        local_crop_scale:
            Min and max scales for the local crop views.
        global_grid_size:
            Grid size for the global crop views.
        local_grid_size:
            Grid size for the local crop views.
        global_gaussian_blur_prob:
            Probability of Gaussian blur for the global crop views.
        local_gaussian_blur_prob:
            Probability of Gaussian blur for the local crop views.
        global_gaussian_blur_kernel_size:
            Will be deprecated in favor of `global_gaussian_blur_sigmas` argument.
            If set, the old behavior applies and `global_gaussian_blur_sigmas`
            is ignored. Used to calculate sigma of gaussian blur with
            global_gaussian_blur_kernel_size * input_size. Applied to global crop views.
        local_gaussian_blur_kernel_size:
            Will be deprecated in favor of `local_gaussian_blur_sigmas` argument.
            If set, the old behavior applies and `local_gaussian_blur_sigmas`
            is ignored. Used to calculate sigma of gaussian blur with
            local_gaussian_blur_kernel_size * input_size. Applied to local crop views.
        global_gaussian_blur_sigmas:
            Tuple of min and max value from which the std of the gaussian kernel
            is sampled. It is ignored if `global_gaussian_blur_kernel_size` is set.
            Applied to global crop views.
        local_gaussian_blur_sigmas:
            Tuple of min and max value from which the std of the gaussian kernel
            is sampled. It is ignored if `local_gaussian_blur_kernel_size` is set.
            Applied to local crop views.
        global_solarize_prob:
            Probability of solarization for the global crop views.
        local_solarize_prob:
            Probability of solarization for the local crop views.
        hf_prob:
            Probability that horizontal flip is applied.
        cj_prob:
            Probability that color jitter is applied.
        cj_strength:
            Strength of the color jitter. `cj_bright`, `cj_contrast`, `cj_sat`, and
            `cj_hue` are multiplied by this value.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        random_gray_scale:
            Probability of conversion to grayscale.
        normalize:
            Dictionary with mean and standard deviation for normalization.
    """

    def __init__(
        self,
        global_crop_size: int = 224,
        local_crop_size: int = 96,
        n_global_views: int = 2,
        n_local_views: int = 6,
        global_crop_scale: Tuple[float, float] = (0.2, 1.0),
        local_crop_scale: Tuple[float, float] = (0.05, 0.2),
        global_grid_size: int = 7,
        local_grid_size: int = 3,
        global_gaussian_blur_prob: float = 0.5,
        local_gaussian_blur_prob: float = 0.1,
        global_gaussian_blur_kernel_size: Optional[float] = None,
        local_gaussian_blur_kernel_size: Optional[float] = None,
        global_gaussian_blur_sigmas: Tuple[float, float] = (0.1, 2),
        local_gaussian_blur_sigmas: Tuple[float, float] = (0.1, 2),
        global_solarize_prob: float = 0.0,
        local_solarize_prob: float = 0.2,
        hf_prob: float = 0.5,
        vf_prob: float = 0.0,
        cj_prob: float = 1.0,
        cj_strength: float = 0.5,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.4,
        cj_hue: float = 0.2,
        random_gray_scale: float = 0.2,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        global_transform = (
            RandomResizedCropAndFlip(
                crop_size=global_crop_size,
                crop_min_scale=global_crop_scale[0],
                crop_max_scale=global_crop_scale[1],
                hf_prob=hf_prob,
                vf_prob=vf_prob,
                grid_size=global_grid_size,
            ),
            VICRegLViewTransform(
                gaussian_blur_prob=global_gaussian_blur_prob,
                gaussian_blur_kernel_size=global_gaussian_blur_kernel_size,
                gaussian_blur_sigmas=global_gaussian_blur_sigmas,
                solarize_prob=global_solarize_prob,
                cj_prob=cj_prob,
                cj_strength=cj_strength,
                cj_bright=cj_bright,
                cj_contrast=cj_contrast,
                cj_sat=cj_sat,
                cj_hue=cj_hue,
                random_gray_scale=random_gray_scale,
                normalize=normalize,
            ),
        )
        local_transform = (
            RandomResizedCropAndFlip(
                crop_size=local_crop_size,
                crop_min_scale=local_crop_scale[0],
                crop_max_scale=local_crop_scale[1],
                hf_prob=hf_prob,
                grid_size=local_grid_size,
            ),
            VICRegLViewTransform(
                gaussian_blur_prob=local_gaussian_blur_prob,
                gaussian_blur_kernel_size=local_gaussian_blur_kernel_size,
                gaussian_blur_sigmas=local_gaussian_blur_sigmas,
                solarize_prob=local_solarize_prob,
                cj_prob=cj_prob,
                cj_strength=cj_strength,
                random_gray_scale=random_gray_scale,
                normalize=normalize,
            ),
        )

        transforms = [global_transform] * n_global_views + [
            local_transform
        ] * n_local_views
        super().__init__(transforms=transforms)


class VICRegLViewTransform:
    def __init__(
        self,
        gaussian_blur_prob: float = 0.5,
        gaussian_blur_kernel_size: Optional[float] = None,
        gaussian_blur_sigmas: Tuple[float, float] = (0.1, 2),
        solarize_prob: float = 0.0,
        cj_prob: float = 1.0,
        cj_strength: float = 0.5,
        cj_bright: float = 0.8,
        cj_contrast: float = 0.8,
        cj_sat: float = 0.4,
        cj_hue: float = 0.2,
        random_gray_scale: float = 0.2,
        normalize: Union[None, Dict[str, List[float]]] = IMAGENET_NORMALIZE,
    ):
        color_jitter = T.ColorJitter(
            brightness=cj_strength * cj_bright,
            contrast=cj_strength * cj_contrast,
            saturation=cj_strength * cj_sat,
            hue=cj_strength * cj_hue,
        )

        transforms = [
            T.RandomApply([color_jitter], p=cj_prob),
            T.RandomGrayscale(p=random_gray_scale),
            GaussianBlur(
                kernel_size=gaussian_blur_kernel_size,
                prob=gaussian_blur_prob,
                sigmas=gaussian_blur_sigmas,
            ),
            RandomSolarization(prob=solarize_prob),
            T.ToTensor(),
        ]
        if normalize:
            transforms += [T.Normalize(mean=normalize["mean"], std=normalize["std"])]
        self.transform = T.Compose(transforms=transforms)

    def __call__(self, image: Union[Tensor, Image]) -> Tensor:
        """Applies the transforms to the input image.

        Args:
            image:
                The input image to apply the transforms to.

        Returns:
            The transformed image.
        """
        transformed: Tensor = self.transform(image)
        return transformed



================================================
FILE: lightly/transforms/wmse_transform.py
================================================
from typing import Dict, List, Optional, Tuple

from lightly.transforms.gaussian_blur import GaussianBlur
from lightly.transforms.multi_view_transform import MultiViewTransform
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T
from lightly.transforms.utils import IMAGENET_NORMALIZE


class WMSETransform(MultiViewTransform):
    """Implements the transformations for W-MSE [0].

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of Tensor of length num_samples.

    Applies the following augmentations by default:
        - Color jitter
        - Random gray scale
        - Random resized crop
        - Random horizontal flip
        - Random gaussian blur
        - ImageNet normalization

    - [0] Whitening for Self-Supervised Representation Learning, 2021, https://arxiv.org/pdf/2007.06346.pdf

    Input to this transform:
        PIL Image or Tensor.

    Output of this transform:
        List of tensors of length k.

    Attributes:
        num_samples:
            Number of views. Must be the same as num_samples in the WMSELoss.
        input_size:
            Size of the input image in pixels.
        cj_prob:
            Probability that color jitter is applied.
        cj_bright:
            How much to jitter brightness.
        cj_contrast:
            How much to jitter constrast.
        cj_sat:
            How much to jitter saturation.
        cj_hue:
            How much to jitter hue.
        min_scale:
            Minimum size of the randomized crop relative to the input_size.
        random_gray_scale:
            Probability that random gray scale is applied.
        hf_prob:
            Probability that horizontal flip is applied.
        gaussian_blur:
            Probability of Gaussian blur.
        kernel_size:
            Will be deprecated in favor of `sigmas` argument. If set, the old behavior applies and `sigmas` is ignored.
            Used to calculate sigma of gaussian blur with kernel_size * input_size.
        sigmas:
            Tuple of min and max value from which the std of the gaussian kernel is sampled.
            Is ignored if `kernel_size` is set.
        normalize:
            Dictionary with 'mean' and 'std' for torchvision.transforms.Normalize.
    """

    def __init__(
        self,
        num_samples: int = 2,
        input_size: int = 224,
        cj_prob: float = 0.8,
        cj_bright: float = 0.4,
        cj_contrast: float = 0.4,
        cj_sat: float = 0.4,
        cj_hue: float = 0.1,
        min_scale: float = 0.2,
        random_gray_scale: float = 0.1,
        hf_prob: float = 0.5,
        gaussian_blur: float = 0.5,
        kernel_size: Optional[int] = None,
        sigmas: Tuple[float, float] = (0.1, 2.0),
        normalize: Dict[str, List[float]] = IMAGENET_NORMALIZE,
    ):
        if num_samples < 1:
            raise ValueError("num_samples must be greater than or equal to 1")
        transform = T.Compose(
            [
                T.RandomApply(
                    [T.ColorJitter(cj_bright, cj_contrast, cj_sat, cj_hue)], p=cj_prob
                ),
                T.RandomGrayscale(p=random_gray_scale),
                T.RandomResizedCrop(
                    input_size,
                    scale=(min_scale, 1.0),
                    interpolation=3,
                ),
                T.RandomHorizontalFlip(p=hf_prob),
                GaussianBlur(
                    kernel_size=kernel_size, sigmas=sigmas, prob=gaussian_blur
                ),
                T.ToTensor(),
                T.Normalize(mean=normalize["mean"], std=normalize["std"]),
            ]
        )
        super().__init__(transforms=[transform] * num_samples)



================================================
FILE: lightly/utils/__init__.py
================================================



================================================
FILE: lightly/utils/bounding_box.py
================================================
"""Bounding Box Utils"""

from __future__ import annotations

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved


class BoundingBox:
    """Class which unifies different bounding box formats.

    Attributes:
        x0:
            x0 coordinate (normalized to [0, 1])
        y0:
            y0 coordinate (normalized to [0, 1])
        x1:
            x1 coordinate (normalized to [0, 1])
        y1:
            y1 coordinate (normalized to [0, 1])

    Examples:
        >>> # simple case, format (x0, y0, x1, y1)
        >>> bbox = BoundingBox(0.1, 0.2, 0.3, 0.4)
        >>>
        >>> # same bounding box in x, y, w, h format
        >>> bbox = BoundingBox.from_x_y_w_h(0.1, 0.2, 0.2, 0.2)
        >>>
        >>> # often the coordinates are not yet normalized by image size
        >>> # for example, for a 100 x 100 image, the coordinates could be
        >>> # (x0, y0, x1, y1) = (10, 20, 30, 40)
        >>> W, H = 100, 100 # get image shape
        >>> bbox = BoundingBox(10 / W, 20 / H, 30 / W, 40 / H)
    """

    def __init__(
        self, x0: float, y0: float, x1: float, y1: float, clip_values: bool = True
    ):
        """Initializes a BoundingBox object.

        Args:
            x0:
                x0 coordinate relative to image width.
            y0:
                y0 coordinate relative to image height.
            x1:
                x1 coordinate relative to image width.
            y1:
                y1 coordinate relative to image height.
            clip_values:
                If True, clips the coordinates to [0, 1].

        """
        if clip_values:

            def clip_to_0_1(value: float) -> float:
                return min(1, max(0, value))

            x0 = clip_to_0_1(x0)
            y0 = clip_to_0_1(y0)
            x1 = clip_to_0_1(x1)
            y1 = clip_to_0_1(y1)

        if x0 > 1 or x1 > 1 or y0 > 1 or y1 > 1 or x0 < 0 or x1 < 0 or y0 < 0 or y1 < 0:
            raise ValueError(
                f"Bounding Box Coordinates must be relative to "
                f"image width and height but are ({x0}, {y0}, {x1}, {y1})."
            )

        if x0 >= x1:
            raise ValueError(
                f"x0 must be smaller than x1 for bounding box [{x0}, {y0}, {x1}, {y1}]"
            )

        if y0 >= y1:
            raise ValueError(
                f"y0 must be smaller than y1 for bounding box [{x0}, {y0}, {x1}, {y1}]"
            )

        self.x0 = x0
        self.y0 = y0
        self.x1 = x1
        self.y1 = y1

    @classmethod
    def from_x_y_w_h(cls, x: float, y: float, w: float, h: float) -> BoundingBox:
        """Creates a BoundingBox from x, y, width, and height.

        Args:
            x:
                x coordinate of the top-left corner relative to image width.
            y:
                y coordinate of the top-left corner relative to image height.
            w:
                Width of the bounding box relative to image width.
            h:
                Height of the bounding box relative to image height.

        Returns:
            BoundingBox: A BoundingBox instance.

        Examples:
            >>> bbox = BoundingBox.from_x_y_w_h(0.1, 0.2, 0.2, 0.2)

        """
        return cls(x, y, x + w, y + h)

    @classmethod
    def from_yolo_label(
        cls, x_center: float, y_center: float, w: float, h: float
    ) -> BoundingBox:
        """Creates a BoundingBox from YOLO label format.

        Args:
            x_center:
                x coordinate of the center relative to image width.
            y_center:
                y coordinate of the center relative to image height.
            w:
                Width of the bounding box relative to image width.
            h:
                Height of the bounding box relative to image height.

        Returns:
            BoundingBox: A BoundingBox instance.

        Examples:
            >>> bbox = BoundingBox.from_yolo_label(0.5, 0.4, 0.2, 0.3)

        """
        return cls(
            x_center - w / 2,
            y_center - h / 2,
            x_center + w / 2,
            y_center + h / 2,
            clip_values=True,
        )

    @property
    def width(self) -> float:
        """Returns the width of the bounding box relative to the image size."""
        return self.x1 - self.x0

    @property
    def height(self) -> float:
        """Returns the height of the bounding box relative to the image size."""
        return self.y1 - self.y0

    @property
    def area(self) -> float:
        """Returns the area of the bounding box relative to the area of the image."""
        return self.width * self.height



================================================
FILE: lightly/utils/debug.py
================================================
from typing import List, Union

import torch
import torchvision
from PIL import Image

from lightly.data.collate import BaseCollateFunction, MultiViewCollateFunction
from lightly.transforms.torchvision_v2_compatibility import torchvision_transforms as T

try:
    import matplotlib.pyplot as plt
except ModuleNotFoundError:
    plt = ModuleNotFoundError(
        "Matplotlib is not installed on your system. Please install it to use the "
        "plotting functionalities. You can install it with "
        "'pip install lightly[matplotlib]'."
    )
except ImportError as ex:
    plt = ex


@torch.no_grad()
def std_of_l2_normalized(z: torch.Tensor) -> torch.Tensor:
    """Calculates the mean of the standard deviation of z along each dimension.

    This measure was used by [0] to determine the level of collapse of the
    learned representations. If the returned value is 0., the outputs z have
    collapsed to a constant vector. If the output z has a zero-mean isotropic
    Gaussian distribution [0], the returned value should be close to 1/sqrt(d),
    where d is the dimensionality of the output.

    [0]: https://arxiv.org/abs/2011.10566

    Args:
        z:
            A torch tensor of shape batch_size x dimension.

    Returns:
        The mean of the standard deviation of the l2 normalized tensor z along
        each dimension.
    """
    if len(z.shape) != 2:
        raise ValueError(
            f"Input tensor must have two dimensions but has {len(z.shape)}!"
        )

    z_norm = torch.nn.functional.normalize(z, dim=1)
    return torch.std(z_norm, dim=0).mean()


def apply_transform_without_normalize(
    image: Image.Image,
    transform,
) -> Image.Image:
    """Applies the transform to the image but skips ToTensor and Normalize.

    Args:
        image:
            The input PIL image.
        transform:
            The transformation to apply, excluding ToTensor and Normalize.

    Returns:
        The transformed image.
    """
    skippable_transforms = (
        type(T.ToTensor()),
        T.Normalize,
    )
    if isinstance(transform, T.Compose):
        for transform_ in transform.transforms:
            image = apply_transform_without_normalize(image, transform_)
    elif not isinstance(transform, skippable_transforms):
        image = transform(image)
    return image


def generate_grid_of_augmented_images(
    input_images: List[Image.Image],
    collate_function: Union[BaseCollateFunction, MultiViewCollateFunction],
) -> List[List[Image.Image]]:
    """Returns a grid of augmented images. Images in a column belong together.

    This function ignores the ToTensor and Normalize transforms for visualization purposes.

    Args:
        input_images:
            List of PIL images for which the augmentations should be plotted.
        collate_function:
            The collate function of the self-supervised learning algorithm.
            Must be of type BaseCollateFunction or MultiViewCollateFunction.

    Returns:
        A grid of augmented images. Images in a column belong together.

    """
    grid = []
    if isinstance(collate_function, BaseCollateFunction):
        for _ in range(2):
            grid.append(
                [
                    apply_transform_without_normalize(image, collate_function.transform)
                    for image in input_images
                ]
            )
    elif isinstance(collate_function, MultiViewCollateFunction):
        for transform in collate_function.transforms:
            grid.append(
                [
                    apply_transform_without_normalize(image, transform)
                    for image in input_images
                ]
            )
    else:
        raise ValueError(
            "Collate function must be one of "
            "(BaseCollateFunction, MultiViewCollateFunction) "
            f"but is {type(collate_function)}."
        )
    return grid


def plot_augmented_images(
    input_images: List[Image.Image],
    collate_function: Union[BaseCollateFunction, MultiViewCollateFunction],
):
    """Plots original images and augmented images in a figure.

    This function ignores the ToTensor and Normalize transforms for visualization purposes.

    Args:
        input_images:
            List of PIL images for which the augmentations should be plotted.
        collate_function:
            The collate function of the self-supervised learning algorithm.
            Must be of type BaseCollateFunction or MultiViewCollateFunction.

    Returns:
        A figure showing the original images in the left column and the augmented
        images to their right. If the collate_function is an instance of the
        BaseCollateFunction, two example augmentations are shown. For
        MultiViewCollateFunctions all the generated views are shown.

    """
    _check_matplotlib_available()

    if len(input_images) == 0:
        raise ValueError("There must be at least one input image.")

    grid = generate_grid_of_augmented_images(input_images, collate_function)
    grid.insert(0, input_images)
    nrows = len(input_images)
    ncols = len(grid)

    fig, axs = plt.subplots(nrows, ncols, figsize=(ncols * 1.5, nrows * 1.5))

    for i in range(nrows):
        for j in range(ncols):
            ax = axs[i][j] if len(input_images) > 1 else axs[j]
            img = grid[j][i]
            ax.imshow(img)
            ax.set_axis_off()

    ax_top_left = axs[0, 0] if len(input_images) > 1 else axs[0]
    ax_top_left.set(title="Original images")
    ax_top_left.title.set_size(8)
    ax_top_next = axs[0, 1] if len(input_images) > 1 else axs[1]
    ax_top_next.set(title="Augmented images")
    ax_top_next.title.set_size(8)
    fig.tight_layout()

    return fig


def _check_matplotlib_available() -> None:
    """Checks if matplotlib is available. Raises an error if not."""
    if isinstance(plt, Exception):
        raise plt



================================================
FILE: lightly/utils/dependency.py
================================================
import functools


@functools.lru_cache(maxsize=1)
def torchvision_vit_available() -> bool:
    """Checks if Vision Transformer (ViT) models are available in torchvision.

    This function checks if the `vision_transformer` module is available in torchvision,
    which requires torchvision version >= 0.12. It also handles exceptions related to
    CUDA version mismatches and installation issues.

    Returns:
        True if the Vision Transformer (ViT) models are available in torchvision,
        otherwise False.
    """
    try:
        import torchvision.models.vision_transformer  # Requires torchvision >=0.12.
    except (
        RuntimeError,  # Different CUDA versions for torch and torchvision.
        OSError,  # Different CUDA versions for torch and torchvision (old).
        ImportError,  # No installation or old version of torchvision.
    ):
        return False
    return True


@functools.lru_cache(maxsize=1)
def timm_vit_available() -> bool:
    """Checks if Vision Transformer (ViT) models are available in the timm library.

    This function checks if the `vision_transformer` module and `LayerType` from timm
    are available, which requires timm version >= 0.3.3 and >= 0.9.9, respectively.

    Returns:
        True if the Vision Transformer (ViT) models are available in timm,
        otherwise False.

    """
    try:
        import timm.models.vision_transformer  # Requires timm >= 0.3.3
        from timm.layers import LayerType  # Requires timm >= 0.9.9
    except ImportError:
        return False
    return True


@functools.lru_cache(maxsize=1)
def torchvision_transforms_v2_available() -> bool:
    """Checks if torchvision supports the v2 transforms API with the `tv_tensors`
    module. Checking for the availability of the `transforms.v2` is not sufficient
    since it is available in torchvision >= 0.15.1, but the `tv_tensors` module is
    only available in torchvision >= 0.16.0.

    Returns:
        True if transforms.v2 are available, False otherwise
    """
    try:
        from torchvision import tv_tensors
    except ImportError:
        return False
    return True



================================================
FILE: lightly/utils/dist.py
================================================
from typing import Any, Callable, Optional, Tuple, TypeVar

import torch
import torch.distributed as dist
from torch.autograd.function import FunctionCtx


class GatherLayer(torch.autograd.Function):
    """Gather tensors from all processes, supporting backward propagation.

    Adapted from the Solo-Learn project:
    https://github.com/vturrisi/solo-learn/blob/b69b4bd27472593919956d9ac58902a301537a4d/solo/utils/misc.py#L187

    """

    @staticmethod
    def forward(ctx: FunctionCtx, input: torch.Tensor) -> Tuple[torch.Tensor, ...]:  # type: ignore
        output = [torch.empty_like(input) for _ in range(dist.get_world_size())]
        dist.all_gather(output, input)
        return tuple(output)

    @staticmethod
    def backward(ctx: FunctionCtx, *grads: torch.Tensor) -> torch.Tensor:  # type: ignore
        all_gradients = torch.stack(grads)
        dist.all_reduce(all_gradients)
        grad_out = all_gradients[dist.get_rank()]
        return grad_out


def rank() -> int:
    """Returns the rank of the current process."""
    return dist.get_rank() if dist.is_initialized() else 0


def world_size() -> int:
    """Returns the current world size (number of distributed processes)."""
    return dist.get_world_size() if dist.is_initialized() else 1


def gather(input: torch.Tensor) -> Tuple[torch.Tensor]:
    """Gathers a tensor from all processes and supports backpropagation."""
    return GatherLayer.apply(input)  # type: ignore[no-any-return]


def eye_rank(n: int, device: Optional[torch.device] = None) -> torch.Tensor:
    """Returns an (n, n * world_size) zero matrix with the diagonal for the rank
    of this process set to 1.

    Example output where n=3, the current process has rank 1, and there are
    4 processes in total:

        rank0   rank1   rank2   rank3
        0 0 0 | 1 0 0 | 0 0 0 | 0 0 0
        0 0 0 | 0 1 0 | 0 0 0 | 0 0 0
        0 0 0 | 0 0 1 | 0 0 0 | 0 0 0

    Equivalent to torch.eye for undistributed settings or if world size == 1.

    Args:
        n:
            Size of the square matrix on a single process.
        device:
            Device on which the matrix should be created.

    Returns:
        A tensor with the appropriate diagonal filled for this rank.

    """
    rows = torch.arange(n, device=device, dtype=torch.long)
    cols = rows + rank() * n
    diag_mask = torch.zeros((n, n * world_size()), dtype=torch.bool)
    diag_mask[(rows, cols)] = True
    return diag_mask


R = TypeVar("R")


def rank_zero_only(fn: Callable[..., R]) -> Callable[..., Optional[R]]:
    """Decorator to ensure the function only runs on the process with rank 0.

    Example:
        >>> @rank_zero_only
        >>> def print_rank_zero(message: str):
        >>>     print(message)
        >>>
        >>> print_rank_zero("Hello from rank 0!")
    """

    def wrapped(*args: Any, **kwargs: Any) -> Optional[R]:
        if rank() == 0:
            return fn(*args, **kwargs)
        return None

    return wrapped


@rank_zero_only
def print_rank_zero(*args: Any, **kwargs: Any) -> None:  # type: ignore[misc]
    """Equivalent to print, but only runs on the process with rank 0."""
    print(*args, **kwargs)



================================================
FILE: lightly/utils/embeddings_2d.py
================================================
"""Transforms embeddings to two-dimensional space for visualization."""

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

from __future__ import annotations

from typing import TYPE_CHECKING, Optional

import numpy as np

if TYPE_CHECKING:
    from numpy.typing import NDArray


class PCA(object):
    """Handmade PCA to bypass sklearn dependency.

    Attributes:
        n_components:
            Number of principal components to keep.
        eps:
            Epsilon for numerical stability.
        mean:
            Mean of the data.
        w:
            Eigenvectors of the covariance matrix.

    """

    def __init__(self, n_components: int = 2, eps: float = 1e-10):
        self.n_components = n_components
        self.eps = eps
        self.mean: Optional[NDArray[np.float32]] = None
        self.w: Optional[NDArray[np.float32]] = None

    def fit(self, X: NDArray[np.float32]) -> PCA:
        """Fits PCA to data in X.

        Args:
            X:
                Datapoints stored in numpy array of size n x d.

        Returns:
            PCA: The fitted PCA object to transform data points.

        """
        X = X.astype(np.float32)
        self.mean = X.mean(axis=0)
        assert self.mean is not None
        X = (X - self.mean + self.eps).astype(
            np.float32
        )  # X is up-casted to float64 in numpy ≥2.2 (numpy#28805), See: https://github.com/numpy/numpy/issues/28805
        cov = np.cov(X.T) / X.shape[0]
        v, w = np.linalg.eig(cov)
        idx = v.argsort()[::-1]  # Sort eigenvalues in descending order
        v, w = v[idx], w[:, idx]
        self.w = w
        return self

    def transform(self, X: NDArray[np.float32]) -> NDArray[np.float32]:
        """Uses PCA to transform data in X.

        Args:
            X:
                Datapoints stored in numpy array of size n x d.

        Returns:
            Numpy array of n x p datapoints where p <= d.

        Raises:
            ValueError:
                If PCA is not fitted before calling this method.

        """
        if self.mean is None or self.w is None:
            raise ValueError("PCA not fitted yet. Call fit() before transform().")
        X = (X - self.mean + self.eps).astype(
            np.float32
        )  # X is up-casted to float64 in numpy ≥2.2 (numpy#28805), See: https://github.com/numpy/numpy/issues/28805
        transformed: NDArray[np.float32] = X.dot(self.w)[:, : self.n_components]
        return np.asarray(
            transformed, dtype=np.float32
        )  # runtime here is float64, fixes test and matches annotation


def fit_pca(
    embeddings: NDArray[np.float32],
    n_components: int = 2,
    fraction: Optional[float] = None,
) -> PCA:
    """Fits PCA to a randomly selected subset of embeddings.

    For large datasets, it can be unfeasible to perform PCA on the whole data.
    This method can fit a PCA on a fraction of the embeddings in order to save
    computational resources.

    Args:
        embeddings:
            Datapoints stored in numpy array of size n x d.
        n_components:
            Number of principal components to keep.
        fraction:
            Fraction of the dataset to fit PCA on.

    Returns:
        A transformer which can be used to transform embeddings
        to lower dimensions.

    Raises:
        If fraction ≤ 0 or fraction > 1.

    """
    if fraction is not None:
        if fraction <= 0.0 or fraction > 1.0:
            raise ValueError(f"fraction must be in (0, 1] but was {fraction}.")

    N = embeddings.shape[0]
    n = N if fraction is None else min(N, int(N * fraction))
    X = embeddings[np.random.permutation(N)][:n]
    return PCA(n_components=n_components).fit(X)



================================================
FILE: lightly/utils/hipify.py
================================================
import copy
import warnings
from typing import Optional, Type, Union


class bcolors:
    """ANSI escape sequences for colored terminal output."""

    HEADER = "\033[95m"
    OKBLUE = "\033[94m"
    OKGREEN = "\033[92m"
    WARNING = "\033[93m"
    FAIL = "\033[91m"
    ENDC = "\033[0m"
    BOLD = "\033[1m"
    UNDERLINE = "\033[4m"


def print_as_warning(message: str, warning_class: Type[Warning] = UserWarning) -> None:
    """Prints a warning message with custom formatting.

    Temporarily overrides the default warning format to apply custom styling, then
    restores the original formatting after the warning is printed.

    Args:
        message:
            The warning message to print.
        warning_class:
            The type of warning to raise.

    """
    old_format = copy.copy(warnings.formatwarning)
    warnings.formatwarning = _custom_formatwarning
    warnings.warn(message, warning_class)
    warnings.formatwarning = old_format


def _custom_formatwarning(
    message: Union[str, Warning],
    category: Type[Warning],
    filename: str,
    lineno: int,
    line: Optional[str] = None,
) -> str:
    """Custom format for warning messages.

    Only the warning message is printed, with additional styling applied.

    Args:
        message:
            The warning message or warning object.
        category:
            The warning class.
        filename:
            The file where the warning originated.
        lineno:
            The line number where the warning occurred.
        line:
            The line of code that triggered the warning (if available).

    Returns:
        str: The formatted warning message.

    """
    return f"{bcolors.WARNING}{message}{bcolors.WARNING}\n"



================================================
FILE: lightly/utils/io.py
================================================
""" I/O operations to save and load embeddings. """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved
from __future__ import annotations

import csv
import json
import re
from itertools import compress
from typing import TYPE_CHECKING, Any, Dict, List, Tuple, Union

import numpy as np

if TYPE_CHECKING:
    from numpy.typing import NDArray


def check_embeddings(path: str, remove_additional_columns: bool = False) -> None:
    """Raises an error if the embeddings csv file has not the correct format

    Use this check whenever you want to upload an embedding to the Lightly
    Platform.
    This method only checks whether the header row matches the specs:
    https://docs.lightly.ai/self-supervised-learning/getting_started/command_line_tool.html#id1

    Args:
        path:
            Path to the embedding csv file
        remove_additional_columns:
            If True, all additional columns
            which are not in {filenames, embeddings_x, labels} are removed.
            If false, they are kept unchanged.

    Raises:
        RuntimeError
    """
    with open(path, "r", newline="") as csv_file:
        reader = csv.reader(csv_file, delimiter=",")
        header: List[str] = next(reader)

        # check for whitespace in the header (we don't allow this)
        if any(x != x.strip() for x in header):
            raise RuntimeError("Embeddings csv file must not contain whitespaces.")

        # first col is `filenames`
        if header[0] != "filenames":
            raise RuntimeError(
                f"Embeddings csv file must start with `filenames` "
                f"column but had {header[0]} instead."
            )

        # `labels` exists
        try:
            header_labels_idx = header.index("labels")
        except ValueError:
            raise RuntimeError(f"Embeddings csv file has no `labels` column.")

        # cols between first and `labels` are `embedding_x`
        for embedding_header in header[1:header_labels_idx]:
            if not re.match(r"embedding_\d+", embedding_header):
                # check if we have a special column
                if not embedding_header in ["masked", "selected"]:
                    raise RuntimeError(
                        f"Embeddings csv file must have `embedding_x` columns but "
                        f"found {embedding_header} instead."
                    )

        # check for empty rows in the body of the csv file
        for i, row in enumerate(reader):
            if len(row) == 0:
                raise RuntimeError(
                    f"Embeddings csv file must not have empty rows. "
                    f"Found empty row on line {i}."
                )

    if remove_additional_columns:
        new_rows = []
        with open(path, "r", newline="") as csv_file:
            reader = csv.reader(csv_file, delimiter=",")
            header_row = next(reader)

            # create mask of columns to keep only filenames, embedding_ or labels
            regexp = r"filenames|(embedding_\d+)|labels"
            col_mask = []
            for i, col in enumerate(header_row):
                col_mask += [True] if re.match(regexp, col) else [False]

            # add header row manually here since we use an iterator
            new_rows.append(list(compress(header_row, col_mask)))

            for row in reader:
                # apply mask to only use filenames, embedding_ or labels
                new_rows.append(list(compress(row, col_mask)))

        with open(path, "w", newline="") as csv_file:
            writer = csv.writer(csv_file, delimiter=",")
            writer.writerows(new_rows)


def save_embeddings(
    path: str, embeddings: NDArray[np.float64], labels: List[int], filenames: List[str]
) -> None:
    """Saves embeddings in a csv file in a Lightly compatible format.

    Creates a csv file at the location specified by path and saves embeddings,
    labels, and filenames.

    Args:
        path:
            Path to the csv file.
        embeddings:
            Embeddings of the images as a numpy array (n x d).
        labels:
            List of integer labels.
        filenames:
            List of filenames.

    Raises:
        ValueError: If embeddings, labels, and filenames have different lengths.

    Examples:
        >>> import lightly.utils.io as io
        >>> io.save_embeddings(
        >>>     'path/to/my/embeddings.csv',
        >>>     embeddings,
        >>>     labels,
        >>>     filenames)
    """

    n_embeddings = len(embeddings)
    n_filenames = len(filenames)
    n_labels = len(labels)

    if n_embeddings != n_labels or n_filenames != n_labels:
        msg = "Length of embeddings, labels, and filenames should be equal "
        msg += f" but are not: ({n_embeddings}, {n_filenames}, {n_labels})"
        raise ValueError(msg)

    header = ["filenames"]
    header = header + [f"embedding_{i}" for i in range(embeddings.shape[-1])]
    header = header + ["labels"]
    with open(path, "w", newline="") as csv_file:
        writer = csv.writer(csv_file, delimiter=",")
        writer.writerow(header)
        for filename, embedding, label in zip(filenames, embeddings, labels):
            writer.writerow([filename] + list(embedding) + [str(label)])


def load_embeddings(path: str) -> Tuple[NDArray[np.float64], List[int], List[str]]:
    """Loads embeddings from a csv file in a Lightly compatible format.

    Args:
        path:
            Path to the csv file.

    Returns:
        The embeddings as a numpy array, labels as a list of integers, and
        filenames as a list of strings in the order they were saved.

        The embeddings will always be of the Float32 datatype.

    Examples:
        >>> import lightly.utils.io as io
        >>> embeddings, labels, filenames = io.load_embeddings(
        >>>     'path/to/my/embeddings.csv')

    """
    check_embeddings(path)

    filenames, labels = [], []
    embeddings = []
    with open(path, "r", newline="") as csv_file:
        reader = csv.reader(csv_file, delimiter=",")
        for i, row in enumerate(reader):
            # skip header
            if i == 0:
                continue
            #  read filenames and labels
            filenames.append(row[0])
            labels.append(int(row[-1]))
            # read embeddings
            embeddings.append(row[1:-1])

    embedding_array = np.array(embeddings).astype(np.float64)
    return embedding_array, labels, filenames


def load_embeddings_as_dict(
    path: str, embedding_name: str = "default", return_all: bool = False
) -> Union[Any, Tuple[Any, NDArray[np.float64], List[int], List[str]]]:
    """Loads embeddings from csv and store it in a dictionary for transfer.

    Loads embeddings to a dictionary which can be serialized and sent to the
    Lightly servers. It is recommended that the embedding_name is always
    specified because the Lightly web-app does not allow two embeddings with
    the same name.

    Args:
        path:
            Path to the csv file.
        embedding_name:
            Name of the embedding for the platform.
        return_all:
            If true, return embeddings, labels, and filenames, too.

    Returns:
        A dictionary containing the embedding information (see load_embeddings)

    Examples:
        >>> import lightly.utils.io as io
        >>> embedding_dict = io.load_embeddings_as_dict(
        >>>     'path/to/my/embeddings.csv',
        >>>     embedding_name='MyEmbeddings')
        >>>
        >>> result = io.load_embeddings_as_dict(
        >>>     'path/to/my/embeddings.csv',
        >>>     embedding_name='MyEmbeddings',
        >>>     return_all=True)
        >>> embedding_dict, embeddings, labels, filenames = result

    """
    embeddings, labels, filenames = load_embeddings(path)

    # build dictionary
    data = {
        "embeddingName": embedding_name,
        "embeddings": [
            {"fileName": filename, "value": embedding.tolist(), "label": label}
            for embedding, filename, label in zip(embeddings, filenames, labels)
        ],
    }

    # return embeddings along with dictionary
    if return_all:
        return data, embeddings, labels, filenames
    else:
        return data


class COCO_ANNOTATION_KEYS:
    """Enum of coco annotation keys complemented with a key for custom metadata.

    :meta private:  # Skip docstring generation
    """

    # image keys
    images: str = "images"
    images_id: str = "id"
    images_filename: str = "file_name"

    # metadata keys
    custom_metadata: str = "metadata"
    custom_metadata_image_id: str = "image_id"


def format_custom_metadata(
    custom_metadata: List[Tuple[str, Any]]
) -> Dict[str, List[Any]]:
    """Transforms custom metadata into a format which can be handled by Lightly.

    Args:
        custom_metadata:
            List of tuples (filename, metadata) where metadata is a dictionary.

    Returns:
        A dictionary of formatted custom metadata.

    Examples:
        >>> custom_metadata = [
        >>>     ('hello.png', {'number_of_people': 1}),
        >>>     ('world.png', {'number_of_people': 3}),
        >>> ]
        >>>
        >>> format_custom_metadata(custom_metadata)
        >>> > {
        >>> >   'images': [{'id': 0, 'file_name': 'hello.png'}, {'id': 1, 'file_name': 'world.png'}],
        >>> >   'metadata': [{'image_id': 0, 'number_of_people': 1}, {'image_id': 1, 'number_of_people': 3}]
        >>> > }

    :meta private:  # Skip docstring generation
    """
    formatted: Dict[str, List[Any]] = {
        COCO_ANNOTATION_KEYS.images: [],
        COCO_ANNOTATION_KEYS.custom_metadata: [],
    }

    for i, (filename, metadata) in enumerate(custom_metadata):
        formatted[COCO_ANNOTATION_KEYS.images].append(
            {
                COCO_ANNOTATION_KEYS.images_id: i,
                COCO_ANNOTATION_KEYS.images_filename: filename,
            }
        )
        formatted[COCO_ANNOTATION_KEYS.custom_metadata].append(
            {
                COCO_ANNOTATION_KEYS.custom_metadata_image_id: i,
                **metadata,
            }
        )

    return formatted


def save_custom_metadata(path: str, custom_metadata: List[Tuple[str, Any]]) -> None:
    """Saves custom metadata in a .json.

    Args:
        path:
            Filename of the .json file where the data should be stored.
        custom_metadata:
            List of tuples (filename, metadata) where metadata is a dictionary.

    :meta private:  # Skip docstring generation
    """
    formatted = format_custom_metadata(custom_metadata)
    with open(path, "w") as f:
        json.dump(formatted, f)


def save_tasks(
    path: str,
    tasks: List[str],
) -> None:
    """Saves a list of prediction task names in the right format.

    Args:
        path:
            Where to store the task names.
        tasks:
            List of task names.

    """
    with open(path, "w") as f:
        json.dump(tasks, f)


def save_schema(path: str, task_type: str, ids: List[int], names: List[str]) -> None:
    """Saves a prediction schema in the right format.

    Args:
        path:
            Where to store the schema.
        task_type:
            Task type (e.g. classification, object-detection).
        ids:
            List of category ids.
        names:
            List of category names.
    """
    if len(ids) != len(names):
        raise ValueError("ids and names must have same length!")

    schema = {
        "task_type": task_type,
        "categories": [{"id": id, "name": name} for id, name in zip(ids, names)],
    }
    with open(path, "w") as f:
        json.dump(schema, f)



================================================
FILE: lightly/utils/lars.py
================================================
from typing import Any, Callable, Dict, Optional, overload

import torch
from torch.optim.optimizer import Optimizer


class LARS(Optimizer):
    """Extends SGD in PyTorch with LARS scaling from the paper "Large batch training of
    Convolutional Networks" [0].

    Implementation from PyTorch Lightning Bolts [1].

    - [0]: https://arxiv.org/pdf/1708.03888.pdf
    - [1]: https://github.com/Lightning-Universe/lightning-bolts/blob/2dfe45a4cf050f120d10981c45cfa2c785a1d5e6/pl_bolts/optimizers/lars.py#L1

    Args:
        params: 
            Iterable of parameters to optimize or dicts defining parameter groups.
        lr:
            Learning rate
        momentum:
            Momentum factor.
        weight_decay:
            Weight decay (L2 penalty).
        dampening:
            Dampening for momentum.
        nesterov:
            Enables Nesterov momentum.
        trust_coefficient:
            Trust coefficient for computing learning rate.
        eps:
            Eps for division denominator.

    Example:
        >>> model = torch.nn.Linear(10, 1)
        >>> input = torch.Tensor(10)
        >>> target = torch.Tensor([1.])
        >>> loss_fn = lambda input, target: (input - target) ** 2
        >>> optimizer = LARS(model.parameters(), lr=0.1, momentum=0.9)
        >>> optimizer.zero_grad()
        >>> loss_fn(model(input), target).backward()
        >>> optimizer.step()

    .. note::
        The application of momentum in the SGD part is modified according to
        the PyTorch standards. LARS scaling fits into the equation in the
        following fashion.

        .. math::
            \begin{aligned}
                g_{t+1} & = \text{lars_lr} * (\beta * p_{t} + g_{t+1}), \\
                v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\
                p_{t+1} & = p_{t} - \text{lr} * v_{t+1},
            \\end{aligned}

        where :math:`p`, :math:`g`, :math:`v`, :math:`\\mu` and :math:`\beta` denote the
        parameters, gradient, velocity, momentum, and weight decay respectively.
        The :math:`lars_lr` is defined by Eq. 6 in the paper.
        The Nesterov version is analogously modified.

    .. warning::
        Parameters with weight decay set to 0 will automatically be excluded from
        layer-wise LR scaling. This is to ensure consistency with papers like SimCLR
        and BYOL.
    """

    def __init__(
        self,
        params: Any,
        lr: float,
        momentum: float = 0,
        dampening: float = 0,
        weight_decay: float = 0,
        nesterov: bool = False,
        trust_coefficient: float = 0.001,
        eps: float = 1e-8,
    ):
        if lr < 0.0:
            raise ValueError(f"Invalid learning rate: {lr}")
        if momentum < 0.0:
            raise ValueError(f"Invalid momentum value: {momentum}")
        if weight_decay < 0.0:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")

        defaults = dict(
            lr=lr,
            momentum=momentum,
            dampening=dampening,
            weight_decay=weight_decay,
            nesterov=nesterov,
            trust_coefficient=trust_coefficient,
            eps=eps,
        )
        if nesterov and (momentum <= 0 or dampening != 0):
            raise ValueError("Nesterov momentum requires a momentum and zero dampening")

        super().__init__(params, defaults)

    def __setstate__(self, state: Dict[str, Any]) -> None:
        super().__setstate__(state)
        for group in self.param_groups:
            group.setdefault("nesterov", False)

    # Type ignore for overloads is required for Python 3.7.
    @overload  # type: ignore[override]
    def step(self, closure: None = None) -> None:
        ...

    @overload  # type: ignore[override]
    def step(self, closure: Callable[[], float]) -> float:
        ...

    @torch.no_grad()
    def step(self, closure: Optional[Callable[[], float]] = None) -> Optional[float]:
        """Performs a single optimization step.

        Args:
            closure (callable, optional): A closure that reevaluates the model
                and returns the loss.
        """
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        # Exclude scaling for params with 0 weight decay.
        for group in self.param_groups:
            weight_decay = group["weight_decay"]
            momentum = group["momentum"]
            dampening = group["dampening"]
            nesterov = group["nesterov"]

            for p in group["params"]:
                if p.grad is None:
                    continue

                d_p = p.grad
                p_norm = torch.norm(p.data)
                g_norm = torch.norm(p.grad.data)

                # Apply Lars scaling and weight decay.
                if weight_decay != 0:
                    if p_norm != 0 and g_norm != 0:
                        lars_lr = p_norm / (
                            g_norm + p_norm * weight_decay + group["eps"]
                        )
                        lars_lr *= group["trust_coefficient"]

                        d_p = d_p.add(p, alpha=weight_decay)
                        d_p *= lars_lr

                # Apply momentum.
                if momentum != 0:
                    param_state = self.state[p]
                    if "momentum_buffer" not in param_state:
                        buf = param_state["momentum_buffer"] = torch.clone(d_p).detach()
                    else:
                        buf = param_state["momentum_buffer"]
                        buf.mul_(momentum).add_(d_p, alpha=1 - dampening)

                    if nesterov:
                        d_p = d_p.add(buf, alpha=momentum)
                    else:
                        d_p = buf

                p.add_(d_p, alpha=-group["lr"])

        return loss



================================================
FILE: lightly/utils/optim.py
================================================
from typing import Any, Dict, Iterable, Optional

from torch.optim import Optimizer


def update_param_groups(
    optimizer: Optimizer,
    default_update: Optional[Dict[str, Any]] = None,
    updates: Optional[Iterable[Dict[str, Any]]] = None,
) -> None:
    """Update the values in the param groups of the optimizer.

    This is useful to update values following a schedule during training.

    Args:
        optimizer:
            A PyTorch optimizer.
        default_update:
            Key value pairs that will be updated for all param groups. The value is only
            updated if the respective key already exists in the param group.
        updates:
            A list of dicts with key value pairs. Each dict must contain a key "name"
            that specifies the name of the param group(s) that should be updated. All
            values of the dict will be updated in the param group(s) with the same name.
            If a key is in default_update and in updates, the value in updates has
            precedence.

    Examples:
        >>> optimizer = torch.optim.SGD(
        >>>     [
        >>>         {
        >>>             "name": "model",
        >>>             "params": params,
        >>>         },
        >>>         {
        >>>             "name": "model_no_weight_decay",
        >>>             "params": params_no_weight_decay,
        >>>             "weight_decay": 0.0,
        >>>         },
        >>>         {
        >>>             "name": "head",
        >>>             "params": head_params,
        >>>             "weight_decay": 0.5,
        >>>         },
        >>>     ],
        >>>     lr=0.1,
        >>>     weight_decay=0.2,
        >>> )
        >>>
        >>> update_param_groups(
        >>>     optimizer=optimizer,
        >>>     default_update={"weight_decay": 0.3},
        >>>     updates=[
        >>>         {"name": "model_no_weight_decay", "weight_decay": 0.0},
        >>>         {"name": "head", "weight_decay": 0.6},
        >>>     ]
        >>> )
        >>>
        >>> # Param group "model" has now weight decay 0.3
        >>> # Param group "model_no_weight_decay" has still weight decay 0.0
        >>> # Param group "head" has now weight decay 0.6

    """
    if default_update is None:
        default_update = {}
    if updates is None:
        updates = []

    # Update the optimizer's param_groups with the provided default values.
    for key, value in default_update.items():
        for param_group in optimizer.param_groups:
            # Update the value only if it already exists in the param_group, we don't
            # want to accidentally add new keys/values.
            if key in param_group:
                param_group[key] = value

    # Update the optimizer's param_groups with the provided updates.
    for update in updates:
        found_group = False
        name = update["name"]
        for param_group in optimizer.param_groups:
            if param_group.get("name") == name:
                found_group = True
                for key, value in update.items():
                    if key in param_group:
                        param_group[key] = value
                    else:
                        raise ValueError(
                            f"Key '{key}' not found in param group with name '{name}'."
                        )
        if not found_group:
            raise ValueError(f"No param group with name '{name}' in optimizer.")



================================================
FILE: lightly/utils/reordering.py
================================================
from typing import List, Sequence, TypeVar

_K = TypeVar("_K")
_V = TypeVar("_V")


def sort_items_by_keys(
    keys: Sequence[_K], items: Sequence[_V], sorted_keys: Sequence[_K]
) -> List[_V]:
    """Sorts the items in the same order as the sorted keys.

    Args:
        keys:
            Keys by which items can be identified.
        items:
            Items to sort.
        sorted_keys:
            Keys in sorted order.

    Returns:
        The list of sorted items.

    Examples:
        >>> keys = [3, 2, 1]
        >>> items = ['!', 'world', 'hello']
        >>> sorted_keys = [1, 2, 3]
        >>> sorted_items = sort_items_by_keys(
        >>>     keys,
        >>>     items,
        >>>     sorted_keys,
        >>> )
        >>> print(sorted_items)
        >>> > ['hello', 'world', '!']

    """
    if len(keys) != len(items) or len(keys) != len(sorted_keys):
        raise ValueError(
            f"All inputs (keys,  items and sorted_keys) "
            f"must have the same length, "
            f"but their lengths are: ({len(keys)},"
            f"{len(items)} and {len(sorted_keys)})."
        )
    lookup = {key_: item_ for key_, item_ in zip(keys, items)}
    sorted_ = [lookup[key_] for key_ in sorted_keys]
    return sorted_



================================================
FILE: lightly/utils/scheduler.py
================================================
import warnings
from typing import Optional

import numpy as np
import torch


def cosine_schedule(
    step: int,
    max_steps: int,
    start_value: float,
    end_value: float,
    period: Optional[int] = None,
) -> float:
    """Use cosine decay to gradually modify start_value to reach target end_value.

    Args:
        step:
            Current step number.
        max_steps:
            Total number of steps.
        start_value:
            Starting value.
        end_value:
            Target value.
        period:
            The number of steps over which the cosine function completes a full cycle.
            If no period is provided, the scheduler will complete a half cycle over
            max_steps.

    Returns:
        Cosine decay value.

    """
    if step < 0:
        raise ValueError(f"Current step number {step} can't be negative.")
    if max_steps < 0:
        raise ValueError(f"Total step number {max_steps} can't be negative.")
    if period is None and step > max_steps:
        warnings.warn(
            f"Current step number {step} exceeds max_steps {max_steps}.",
            category=RuntimeWarning,
        )
    if period is not None and period <= 0:
        raise ValueError(f"Period {period} must be >= 1")

    decay: float
    if period is not None:  # "cycle" based on period, if provided
        decay = (
            end_value
            - (end_value - start_value) * (np.cos(2 * np.pi * step / period) + 1) / 2
        )
    elif max_steps <= 1:
        # Avoid division by zero
        decay = end_value
    elif step >= max_steps - 1:
        # Special case for Pytorch Lightning which updates LR scheduler also for epoch
        # after last training epoch.
        decay = end_value
    else:
        decay = (
            end_value
            - (end_value - start_value)
            * (np.cos(np.pi * step / (max_steps - 1)) + 1)
            / 2
        )
    return decay


def cosine_warmup_schedule(
    step: int,
    max_steps: int,
    start_value: float,
    end_value: float,
    warmup_steps: int,
    warmup_start_value: float,
    warmup_end_value: Optional[float] = None,
    period: Optional[int] = None,
) -> float:
    """Use cosine decay to gradually modify start_value to reach target end_value.

    Uses linear warmup for the first warmup_steps steps.

    Args:
        step:
            Current step number.
        max_steps:
            Total number of steps.
        start_value:
            Starting value.
        end_value:
            Target value.
        warmup_steps:
            Number of steps for warmup.
        warmup_start_value:
            Starting value for warmup.
        warmup_end_value:
            Target value for warmup. Defaults to start_value.
        period:
            The number of steps over which the cosine function completes a full cycle.
            If no period is provided, the scheduler will complete a half cycle over
            max_steps - warmup_steps.
    Returns:
        Cosine decay value.
    """
    if warmup_steps < 0:
        raise ValueError(f"Warmup steps {warmup_steps} can't be negative.")
    if warmup_steps > max_steps:
        raise ValueError(f"Warmup steps {warmup_steps} must be <= max_steps.")
    if step > max_steps:
        warnings.warn(
            f"Current step number {step} exceeds max_steps {max_steps}.",
            category=RuntimeWarning,
        )

    if warmup_end_value is None:
        warmup_end_value = start_value

    if step < warmup_steps:
        # Use step + 1 to reach warmup_end_value at end of warmup. This means that the
        # initial warmup_start_value is skipped which is oftentimes desired when setting
        # it to 0 as this would result in no parameter updates.
        return (
            warmup_start_value
            + (warmup_end_value - warmup_start_value) * (step + 1) / warmup_steps
        )
    else:
        max_steps = max_steps - (warmup_steps if period is None else 1)
        return cosine_schedule(
            step=step - warmup_steps,
            max_steps=max_steps,
            start_value=start_value,
            end_value=end_value,
            period=period,
        )


class CosineWarmupScheduler(torch.optim.lr_scheduler.LambdaLR):
    """Cosine warmup scheduler for learning rate.

    Args:
        optimizer:
            Optimizer object to schedule the learning rate.
        warmup_epochs:
            Number of warmup epochs or steps.
        max_epochs:
            Total number of training epochs or steps.
        last_epoch:
            The index of last epoch or step.
        start_value:
            Starting learning rate.
        end_value:
            Target learning rate.
        warmup_start_value:
            Starting learning rate for warmup.
        warmup_end_value:
            Target learning rate for warmup. Defaults to start_value.

    Note: The `epoch` arguments do not necessarily have to be epochs. Any step or index
    can be used. The naming follows the PyTorch convention to use `epoch` for the steps
    in the scheduler.
    """

    def __init__(
        self,
        optimizer: torch.optim.Optimizer,
        warmup_epochs: int,
        max_epochs: int,
        last_epoch: int = -1,
        start_value: float = 1.0,
        end_value: float = 0.001,
        period: Optional[int] = None,
        warmup_start_value: float = 0.0,
        warmup_end_value: Optional[float] = None,
    ) -> None:
        self.warmup_epochs = warmup_epochs
        self.max_epochs = max_epochs
        self.start_value = start_value
        self.end_value = end_value
        self.period = period
        self.warmup_start_value = warmup_start_value
        self.warmup_end_value = warmup_end_value

        super().__init__(
            optimizer=optimizer,
            lr_lambda=self.scale_lr,
            last_epoch=last_epoch,
        )

    def scale_lr(self, epoch: int) -> float:
        """Scale learning rate according to the current epoch number.

        Args:
            epoch:
                Current epoch number.

        Returns:
            Scaled learning rate.

        """
        return cosine_warmup_schedule(
            step=epoch,
            max_steps=self.max_epochs,
            start_value=self.start_value,
            end_value=self.end_value,
            warmup_steps=self.warmup_epochs,
            warmup_start_value=self.warmup_start_value,
            warmup_end_value=self.warmup_end_value,
            period=self.period,
        )


def linear_warmup_schedule(
    step: int,
    warmup_steps: int,
    start_value: float,
    end_value: float,
) -> float:
    if warmup_steps < 0:
        raise ValueError(f"Warmup steps {warmup_steps} can't be negative.")
    if step < 0:
        raise ValueError(f"Current step number {step} can't be negative.")
    if start_value < 0:
        raise ValueError(f"Start value {start_value} can't be negative.")
    if end_value <= 0:
        raise ValueError(f"End value {end_value} can't be non-positive.")
    if start_value > end_value:
        raise ValueError(
            f"Start value {start_value} must be less than or equal to end value {end_value}."
        )
    if step < warmup_steps:
        return start_value + step / warmup_steps * (end_value - start_value)
    else:
        return end_value



================================================
FILE: lightly/utils/version_compare.py
================================================
""" Utility method for comparing versions of libraries """

# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved


def version_compare(v0: str, v1: str) -> int:
    """Returns 1 if version of v0 is larger than v1 and -1 otherwise

    Use this method to compare Python package versions and see which one is
    newer.

    Examples:

        >>> # compare two versions
        >>> version_compare('1.2.0', '1.1.2')
        >>> 1
    """
    v0_parsed = [int(n) for n in v0.split(".")][::-1]
    v1_parsed = [int(n) for n in v1.split(".")][::-1]
    if len(v0_parsed) != 3 or len(v1_parsed) != 3:
        raise ValueError(
            f"Length of version strings is not 3 (expected pattern `x.y.z`) but is "
            f"{v0_parsed} and {v1_parsed}."
        )
    pairs = list(zip(v0_parsed, v1_parsed))[::-1]
    for x, y in pairs:
        if x < y:
            return -1
        if x > y:
            return 1
    return 0



================================================
FILE: lightly/utils/benchmarking/__init__.py
================================================
from lightly.utils.benchmarking.benchmark_module import BenchmarkModule
from lightly.utils.benchmarking.knn import knn_predict
from lightly.utils.benchmarking.knn_classifier import KNNClassifier
from lightly.utils.benchmarking.linear_classifier import (
    FinetuneClassifier,
    LinearClassifier,
)
from lightly.utils.benchmarking.metric_callback import MetricCallback
from lightly.utils.benchmarking.online_linear_classifier import OnlineLinearClassifier



================================================
FILE: lightly/utils/benchmarking/benchmark_module.py
================================================
# Copyright (c) 2020. Lightly AG and its affiliates.
# All Rights Reserved

from typing import Any, List, Optional, Tuple

import torch
import torch.distributed as dist
import torch.nn as nn
import torch.nn.functional as F
from pytorch_lightning import LightningModule
from torch import Tensor
from torch.utils.data import DataLoader

from lightly.utils.benchmarking.knn import knn_predict
from lightly.utils.dist import gather as lightly_gather


class BenchmarkModule(LightningModule):
    """A PyTorch Lightning Module for automated kNN callback

    At the end of every training epoch we create a feature bank by feeding the
    `dataloader_kNN` passed to the module through the backbone.
    At every validation step we predict features on the validation data.
    After all predictions on validation data (validation_epoch_end) we evaluate
    the predictions on a kNN classifier on the validation data using the
    feature_bank features from the train data.

    We can access the highest test accuracy during a kNN prediction
    using the `max_accuracy` attribute.

    Attributes:
        backbone:
            The backbone model used for kNN validation. Make sure that you set the
            backbone when inheriting from `BenchmarkModule`.
        max_accuracy:
            Floating point number between 0.0 and 1.0 representing the maximum
            test accuracy the benchmarked model has achieved.
        dataloader_kNN:
            Dataloader to be used after each training epoch to create feature bank.
        num_classes:
            Number of classes. E.g. for cifar10 we have 10 classes. (default: 10)
        knn_k:
            Number of nearest neighbors for kNN
        knn_t:
            Temperature parameter for kNN

    Examples:
        >>> class SimSiamModel(BenchmarkingModule):
        >>>     def __init__(dataloader_kNN, num_classes):
        >>>         super().__init__(dataloader_kNN, num_classes)
        >>>         resnet = lightly.models.ResNetGenerator('resnet-18')
        >>>         self.backbone = nn.Sequential(
        >>>             *list(resnet.children())[:-1],
        >>>             nn.AdaptiveAvgPool2d(1),
        >>>         )
        >>>         self.resnet_simsiam =
        >>>             lightly.models.SimSiam(self.backbone, num_ftrs=512)
        >>>         self.criterion = lightly.loss.SymNegCosineSimilarityLoss()
        >>>
        >>>     def forward(self, x):
        >>>         self.resnet_simsiam(x)
        >>>
        >>>     def training_step(self, batch, batch_idx):
        >>>         (x0, x1), _, _ = batch
        >>>         x0, x1 = self.resnet_simsiam(x0, x1)
        >>>         loss = self.criterion(x0, x1)
        >>>         return loss
        >>>     def configure_optimizers(self):
        >>>         optim = torch.optim.SGD(
        >>>             self.resnet_simsiam.parameters(), lr=6e-2, momentum=0.9
        >>>         )
        >>>         return [optim]
        >>>
        >>> model = SimSiamModel(dataloader_train_kNN)
        >>> trainer = pl.Trainer()
        >>> trainer.fit(
        >>>     model,
        >>>     train_dataloader=dataloader_train_ssl,
        >>>     val_dataloaders=dataloader_test
        >>> )
        >>> # you can get the peak accuracy using
        >>> print(model.max_accuracy)

    """

    def __init__(
        self,
        dataloader_kNN: DataLoader[Any],
        num_classes: int,
        knn_k: int = 200,
        knn_t: float = 0.1,
    ):
        super().__init__()
        self.backbone = nn.Module()
        self.max_accuracy = 0.0
        self.dataloader_kNN = dataloader_kNN
        self.num_classes = num_classes
        self.knn_k = knn_k
        self.knn_t = knn_t

        self._train_features: Optional[Tensor] = None
        self._train_targets: Optional[Tensor] = None
        self._val_predicted_labels: List[Tensor] = []
        self._val_targets: List[Tensor] = []

    def on_validation_epoch_start(self) -> None:
        train_features = []
        train_targets = []
        with torch.no_grad():
            for data in self.dataloader_kNN:
                img, target, _ = data
                img = img.to(self.device)
                target = target.to(self.device)
                feature = self.backbone(img).squeeze()
                feature = F.normalize(feature, dim=1)
                train_features.append(feature)
                train_targets.append(target)
        self._train_features = torch.cat(train_features, dim=0).t().contiguous()
        self._train_targets = torch.cat(train_targets, dim=0).t().contiguous()

    def validation_step(
        self, batch: Tuple[List[Tensor], Tensor, List[str]], batch_idx: int
    ) -> None:
        # we can only do kNN predictions once we have a feature bank
        if self._train_features is not None and self._train_targets is not None:
            images, targets, _ = batch
            feature = self.backbone(images).squeeze()
            feature = F.normalize(feature, dim=1)
            predicted_labels = knn_predict(
                feature,
                self._train_features,
                self._train_targets,
                self.num_classes,
                self.knn_k,
                self.knn_t,
            )

            if dist.is_initialized() and dist.get_world_size() > 0:
                # gather predictions and targets from all processes

                predicted_labels = torch.cat(lightly_gather(predicted_labels), dim=0)
                targets = torch.cat(lightly_gather(targets), dim=0)

            self._val_predicted_labels.append(predicted_labels.cpu())
            self._val_targets.append(targets.cpu())

    def on_validation_epoch_end(self) -> None:
        if self._val_predicted_labels and self._val_targets:
            predicted_labels = torch.cat(self._val_predicted_labels, dim=0)
            targets = torch.cat(self._val_targets, dim=0)
            top1 = (predicted_labels[:, 0] == targets).float().sum()
            acc = top1 / len(targets)
            if acc > self.max_accuracy:
                self.max_accuracy = float(acc.item())
            self.log("kNN_accuracy", acc * 100.0, prog_bar=True)

        self._val_predicted_labels.clear()
        self._val_targets.clear()



================================================
FILE: lightly/utils/benchmarking/knn.py
================================================
import torch
from torch import Tensor

# code for kNN prediction from here:
# https://colab.research.google.com/github/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb


def knn_predict(
    feature: Tensor,
    feature_bank: Tensor,
    feature_labels: Tensor,
    num_classes: int,
    knn_k: int = 200,
    knn_t: float = 0.1,
) -> Tensor:
    """Run kNN predictions on features based on a feature bank.

    This method is commonly used to monitor the performance of self-supervised
    learning methods. The default parameters are the ones
    used in https://arxiv.org/pdf/1805.01978v1.pdf.

    Args:
        feature:
            Tensor of shape (B, D) for which you want predictions, where B is the
            batch size and D is the feature dimension.
        feature_bank:
            Tensor of shape (D, N) representing a database of features used for kNN,
            where N is the number of stored feature vectors.
        feature_labels:
            Tensor of shape (N,) containing labels for the corresponding
            feature vectors in the feature_bank.
        num_classes:
            Number of classes (e.g., `10` for CIFAR-10).
        knn_k:
            Number of k nearest neighbors used for kNN.
        knn_t:
            Temperature parameter to reweight similarities for kNN.

    Returns:
        Tensor of shape (B, num_classes) with the predicted class indices sorted
        by probability in descending order for each sample. The first index
        corresponds to the most probable class. To get the top-1 prediction,
        you can access `pred_labels[:, 0]`.

    Examples:
        >>> images, targets, _ = batch
        >>> feature = backbone(images).squeeze()
        >>> # we recommend normalizing the features
        >>> feature = F.normalize(feature, dim=1)
        >>> pred_labels = knn_predict(
        >>>     feature,
        >>>     feature_bank,
        >>>     targets_bank,
        >>>     num_classes=10,
        >>> )
        >>> # top-1 prediction
        >>> top1_pred = pred_labels[:, 0]
    """
    # compute cos similarity between each feature vector and feature bank ---> (B, N)
    sim_matrix = torch.mm(feature, feature_bank)
    # (B, K)
    sim_weight, sim_indices = sim_matrix.topk(k=knn_k, dim=-1)
    # (B, K)
    sim_labels = torch.gather(
        feature_labels.expand(feature.size(0), -1), dim=-1, index=sim_indices
    )
    # we do a reweighting of the similarities
    sim_weight = (sim_weight / knn_t).exp()
    # counts for each class
    one_hot_label = torch.zeros(
        feature.size(0) * knn_k, num_classes, device=sim_labels.device
    )
    # (B*K, C)
    one_hot_label = one_hot_label.scatter(
        dim=-1, index=sim_labels.view(-1, 1), value=1.0
    )
    # weighted score ---> (B, C)
    pred_scores = torch.sum(
        one_hot_label.view(feature.size(0), -1, num_classes)
        * sim_weight.unsqueeze(dim=-1),
        dim=1,
    )
    pred_labels = pred_scores.argsort(dim=-1, descending=True)
    return pred_labels



================================================
FILE: lightly/utils/benchmarking/knn_classifier.py
================================================
from typing import Optional, Tuple

import torch
import torch.nn.functional as F
from pytorch_lightning import LightningModule
from torch import Tensor
from torch.nn import Module

from lightly.utils.benchmarking import knn_predict
from lightly.utils.benchmarking.topk import mean_topk_accuracy


class KNNClassifier(LightningModule):
    def __init__(
        self,
        model: Module,
        num_classes: int,
        knn_k: int,
        knn_t: float,
        topk: Tuple[int, ...] = (1, 5),
        feature_dtype: torch.dtype = torch.float32,
        normalize: bool = True,
    ):
        """KNN classifier for benchmarking.

        Settings based on InstDisc [0]. Code adapted from MoCo [1].

        - [0]: InstDisc, 2018, https://arxiv.org/pdf/1805.01978v1.pdf
        - [1]: MoCo, 2019, https://github.com/facebookresearch/moco

        Args:
            model:
                Model used for feature extraction. Must define a forward(images) method
                that returns a feature tensor.
            num_classes:
                Number of classes in the dataset.
            knn_k:
                Number of neighbors used for KNN search.
            knn_t:
                Temperature parameter to reweights similarities.
            topk:
                Tuple of integers defining the top-k accuracy metrics to compute.
            feature_dtype:
                Torch data type of the features used for KNN search. Reduce to float16
                for memory-efficient KNN search.
            normalize:
                Whether to normalize the features for KNN search.

        Examples:
            >>> from pytorch_lightning import Trainer
            >>> from torch import nn
            >>> import torchvision
            >>> from lightly.models import LinearClassifier
            >>> from lightly.modles.modules import SimCLRProjectionHead
            >>>
            >>> class SimCLR(nn.Module):
            >>>     def __init__(self):
            >>>         super().__init__()
            >>>         self.backbone = torchvision.models.resnet18()
            >>>         self.backbone.fc = nn.Identity() # Ignore classification layer
            >>>         self.projection_head = SimCLRProjectionHead(512, 512, 128)
            >>>
            >>>     def forward(self, x):
            >>>         # Forward must return image features.
            >>>         features = self.backbone(x).flatten(start_dim=1)
            >>>         return features
            >>>
            >>> # Initialize a model.
            >>> model = SimCLR()
            >>>
            >>>
            >>> # Wrap it with a KNNClassifier.
            >>> knn_classifier = KNNClassifier(resnet, num_classes=10)
            >>>
            >>> # Extract features and evaluate.
            >>> trainer = Trainer(max_epochs=1)
            >>> trainer.validate(
                    model=knn_classifier,
                    dataloaders=[train_dataloader, val_dataloader],
                )
        """
        super().__init__()
        self.save_hyperparameters(
            {
                "num_classes": num_classes,
                "knn_k": knn_k,
                "knn_t": knn_t,
                "topk": topk,
                "feature_dtype": str(feature_dtype),
            }
        )
        self.model = model
        self.num_classes = num_classes
        self.knn_k = knn_k
        self.knn_t = knn_t
        self.topk = topk
        self.feature_dtype = feature_dtype
        self.normalize = normalize

        self._train_features = []
        self._train_targets = []
        self._train_features_tensor: Optional[Tensor] = None
        self._train_targets_tensor: Optional[Tensor] = None

    def forward(self, images: Tensor) -> Tensor:
        features = self.model.forward(images).flatten(start_dim=1)
        if self.normalize:
            features = F.normalize(features, dim=1)
        features = features.to(self.feature_dtype)
        return features

    def append_train_features(self, features: Tensor, targets: Tensor) -> None:
        self._train_features.append(features.cpu())
        self._train_targets.append(targets.cpu())

    def concat_train_features(self) -> None:
        if self._train_features and self._train_targets:
            # Features and targets have size (world_size, batch_size, dim) and
            # (world_size, batch_size) after gather. For non-distributed training,
            # features and targets have size (batch_size, dim) and (batch_size,).
            features = self.all_gather(torch.cat(self._train_features, dim=0))
            self._train_features = []
            targets = self.all_gather(torch.cat(self._train_targets, dim=0))
            self._train_targets = []
            # Reshape to (dim, world_size * batch_size)
            features = features.flatten(end_dim=-2).t().contiguous()
            self._train_features_tensor = features.to(self.device)
            # Reshape to (world_size * batch_size,)
            targets = targets.flatten().t().contiguous()
            self._train_targets_tensor = targets.to(self.device)

    @torch.no_grad()
    def training_step(self, batch, batch_idx) -> None:
        pass

    def validation_step(self, batch, batch_idx: int, dataloader_idx: int) -> None:
        images, targets = batch[0], batch[1]
        features = self(images)

        if dataloader_idx == 0:
            # The first dataloader is the training dataloader.
            self.append_train_features(features=features, targets=targets)
        else:
            if batch_idx == 0 and dataloader_idx == 1:
                # Concatenate train features when starting the validation dataloader.
                self.concat_train_features()

            assert self._train_features_tensor is not None
            assert self._train_targets_tensor is not None
            predicted_classes = knn_predict(
                feature=features,
                feature_bank=self._train_features_tensor,
                feature_labels=self._train_targets_tensor,
                num_classes=self.num_classes,
                knn_k=self.knn_k,
                knn_t=self.knn_t,
            )
            topk = mean_topk_accuracy(
                predicted_classes=predicted_classes, targets=targets, k=self.topk
            )
            log_dict = {f"val_top{k}": acc for k, acc in topk.items()}
            self.log_dict(
                log_dict, prog_bar=True, sync_dist=True, batch_size=len(targets)
            )

    def configure_optimizers(self) -> None:
        # configure_optimizers must be implemented for PyTorch Lightning. Returning None
        # means that no optimization is performed.
        pass



================================================
FILE: lightly/utils/benchmarking/linear_classifier.py
================================================
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Tuple, Union

import torch
from pytorch_lightning import LightningModule
from torch import Tensor
from torch.nn import CrossEntropyLoss, Linear, Module, Parameter, Sequential
from torch.optim import SGD, Optimizer

from lightly.utils.benchmarking.topk import mean_topk_accuracy
from lightly.utils.scheduler import CosineWarmupScheduler


class BaseClassifier(LightningModule, ABC):
    def __init__(
        self,
        model: Module,
        batch_size_per_device: int,
        lr: float,
        feature_dim: int,
        num_classes: int,
        topk: Tuple[int, ...],
    ) -> None:
        """Base classifier for benchmarking. Can be used for linear evaluation or finetuning evaluation.

        Settings based on SimCLR [0].

        - [0]: https://arxiv.org/abs/2002.05709

        Args:
            model:
                Model used for feature extraction. Must define a forward(images) method
                that returns a feature tensor.
            batch_size_per_device:
                Batch size per device.
            feature_dim:
                Dimension of features returned by forward method of model.
            num_classes:
                Number of classes in the dataset.
            topk:
                Tuple of integers defining the top-k accuracy metrics to compute.

        Examples:

            >>> from pytorch_lightning import Trainer
            >>> from torch import nn
            >>> import torchvision
            >>> from lightly.models import LinearClassifier
            >>> from lightly.modles.modules import SimCLRProjectionHead
            >>>
            >>> class SimCLR(nn.Module):
            >>>     def __init__(self):
            >>>         super().__init__()
            >>>         self.backbone = torchvision.models.resnet18()
            >>>         self.backbone.fc = nn.Identity() # Ignore classification layer
            >>>         self.projection_head = SimCLRProjectionHead(512, 512, 128)
            >>>
            >>>     def forward(self, x):
            >>>         # Forward must return image features.
            >>>         features = self.backbone(x).flatten(start_dim=1)
            >>>         return features
            >>>
            >>> # Initialize a model.
            >>> model = SimCLR()
            >>>
            >>> # Wrap it with a LinearClassifier.
            >>> linear_classifier = LinearClassifier(
            >>>     model,
            >>>     batch_size=256,
            >>>     num_classes=10,
            >>> )
            >>>
            >>> # Train the linear classifier.
            >>> trainer = Trainer(max_epochs=90)
            >>> trainer.fit(linear_classifier, train_dataloader, val_dataloader)

        """
        super().__init__()
        self.save_hyperparameters(ignore="model")

        self.model = model
        self.batch_size_per_device = batch_size_per_device
        self.lr = lr
        self.feature_dim = feature_dim
        self.num_classes = num_classes
        self.topk = topk

        self.classification_head: Union[Linear, Sequential] = Linear(
            feature_dim, num_classes
        )
        self.criterion = CrossEntropyLoss()

    @abstractmethod
    def forward(self, images: Tensor) -> Tensor:
        """Implement in subclass."""
        pass

    def shared_step(
        self, batch: Tuple[Tensor, ...], batch_idx: int
    ) -> Tuple[Tensor, Dict[int, Tensor]]:
        images, targets = batch[0], batch[1]
        predictions = self.forward(images)
        loss = self.criterion(predictions, targets)
        _, predicted_labels = predictions.topk(max(self.topk))
        topk = mean_topk_accuracy(predicted_labels, targets, k=self.topk)

        return loss, topk

    def training_step(self, batch: Tuple[Tensor, ...], batch_idx: int) -> Tensor:
        loss, topk = self.shared_step(batch=batch, batch_idx=batch_idx)
        batch_size = len(batch[1])
        log_dict = {f"train_top{k}": acc for k, acc in topk.items()}
        self.log(
            "train_loss", loss, prog_bar=True, sync_dist=True, batch_size=batch_size
        )
        self.log_dict(log_dict, sync_dist=True, batch_size=batch_size)

        return loss

    def validation_step(self, batch: Tuple[Tensor, ...], batch_idx: int) -> Tensor:
        loss, topk = self.shared_step(batch=batch, batch_idx=batch_idx)
        batch_size = len(batch[1])
        log_dict = {f"val_top{k}": acc for k, acc in topk.items()}
        self.log("val_loss", loss, prog_bar=True, sync_dist=True, batch_size=batch_size)
        self.log_dict(log_dict, prog_bar=True, sync_dist=True, batch_size=batch_size)

        return loss

    def get_effective_lr(self) -> float:
        """Compute the effective learning rate based on batch size and world size."""
        return self.lr * self.batch_size_per_device * self.trainer.world_size / 256

    @abstractmethod
    def get_trainable_parameters(self) -> List[torch.nn.Parameter]:
        """Return the parameters that should be updated during training."""
        pass

    # Type ignore is needed because return type of LightningModule.configure_optimizers
    # is complicated and typing changes between versions.
    def configure_optimizers(  # type: ignore[override]
        self,
    ) -> Tuple[List[Optimizer], List[Dict[str, Union[Any, str]]]]:
        parameters = list(self.get_trainable_parameters())

        optimizer = SGD(
            parameters,
            lr=self.get_effective_lr(),
            momentum=0.9,
            weight_decay=0.0,
        )
        scheduler = {
            "scheduler": CosineWarmupScheduler(
                optimizer=optimizer,
                warmup_epochs=0,
                max_epochs=int(self.trainer.estimated_stepping_batches),
            ),
            "interval": "step",
        }

        return [optimizer], [scheduler]


class LinearClassifier(BaseClassifier):
    def __init__(
        self,
        model: Module,
        batch_size_per_device: int,
        lr: float = 0.1,
        feature_dim: int = 2048,
        num_classes: int = 1000,
        topk: Tuple[int, ...] = (1, 5),
    ) -> None:
        super().__init__(
            model=model,
            batch_size_per_device=batch_size_per_device,
            lr=lr,
            feature_dim=feature_dim,
            num_classes=num_classes,
            topk=topk,
        )

    def forward(self, images: Tensor) -> Tensor:
        # For linear evaluation, we want to freeze the feature extractor.
        with torch.no_grad():
            features = self.model(images).flatten(start_dim=1)

        output: Tensor = self.classification_head(features)

        return output

    def get_trainable_parameters(self) -> List[Parameter]:
        # Only update the classification head.
        return list(self.classification_head.parameters())

    def on_train_epoch_start(self) -> None:
        # Set model to eval mode to disable norm layer updates.
        self.model.eval()


class FinetuneClassifier(BaseClassifier):
    def __init__(
        self,
        model: Module,
        batch_size_per_device: int,
        lr: float = 0.05,
        feature_dim: int = 2048,
        num_classes: int = 1000,
        topk: Tuple[int, ...] = (1, 5),
    ) -> None:
        super().__init__(
            model=model,
            batch_size_per_device=batch_size_per_device,
            lr=lr,
            feature_dim=feature_dim,
            num_classes=num_classes,
            topk=topk,
        )

    def forward(self, images: Tensor) -> Tensor:
        # For finetuning, we want to update the feature extractor.
        features = self.model(images).flatten(start_dim=1)

        output: Tensor = self.classification_head(features)

        return output

    def get_trainable_parameters(self) -> List[Parameter]:
        # Update both the classification head and the feature extractor.
        return list(self.classification_head.parameters()) + list(
            self.model.parameters()
        )



================================================
FILE: lightly/utils/benchmarking/metric_callback.py
================================================
from typing import Dict, List, Union

from pytorch_lightning import LightningModule, Trainer
from pytorch_lightning.callbacks import Callback
from torch import Tensor

MetricValue = Union[Tensor, float]


class MetricCallback(Callback):
    """Callback that collects log metrics from the LightningModule and stores them after
    every epoch.

    Attributes:
        train_metrics:
            Dictionary that stores the last logged metrics after every train epoch.
        val_metrics:
            Dictionary that stores the last logged metrics after every validation epoch.

    Example::

        >>> from lightly.utils.benchmarking import MetricCallback
        >>> from pytorch_lightning import LightningModule, Trainer
        >>>
        >>> class Model(LightningModule):
        >>>     def training_step(self, batch, batch_idx):
        >>>         ...
        >>>         self.log("train_acc", acc)
        >>>         ...
        >>>
        >>>     def validation_step(self, batch, batch_idx):
        >>>         ...
        >>>         self.log("val_acc", acc)
        >>>         ...
        >>>
        >>> metric_callback = MetricCallback()
        >>> trainer = Trainer(callbacks=[metric_callback], max_epochs=10)
        >>> trainer.fit(Model(), train_dataloder, val_dataloader)
        >>>
        >>> max_train_acc = max(metric_callback.train_metrics["train_acc"])
        >>> max_val_acc = max(metric_callback.val_metrics["val_acc"])
    """

    def __init__(self) -> None:
        super().__init__()
        self.train_metrics: Dict[str, List[float]] = {}
        self.val_metrics: Dict[str, List[float]] = {}

    def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule) -> None:
        if not trainer.sanity_checking:
            self._append_metrics(metrics_dict=self.train_metrics, trainer=trainer)

    def on_validation_epoch_end(
        self, trainer: Trainer, pl_module: LightningModule
    ) -> None:
        if not trainer.sanity_checking:
            self._append_metrics(metrics_dict=self.val_metrics, trainer=trainer)

    def _append_metrics(
        self, metrics_dict: Dict[str, List[float]], trainer: Trainer
    ) -> None:
        for name, value in trainer.callback_metrics.items():
            if isinstance(value, Tensor) and value.numel() != 1:
                # Skip non-scalar tensors.
                continue
            metrics_dict.setdefault(name, []).append(float(value))



================================================
FILE: lightly/utils/benchmarking/online_linear_classifier.py
================================================
from typing import Dict, Tuple

from pytorch_lightning import LightningModule
from torch import Tensor
from torch.nn import CrossEntropyLoss, Linear

from lightly.utils.benchmarking.topk import mean_topk_accuracy


class OnlineLinearClassifier(LightningModule):
    def __init__(
        self,
        feature_dim: int = 2048,
        num_classes: int = 1000,
        topk: Tuple[int, ...] = (1, 5),
    ) -> None:
        super().__init__()
        self.feature_dim = feature_dim
        self.num_classes = num_classes
        self.topk = topk

        self.classification_head = Linear(feature_dim, num_classes)
        self.criterion = CrossEntropyLoss()

    def forward(self, x: Tensor) -> Tensor:
        return self.classification_head(x.detach().flatten(start_dim=1))

    def shared_step(self, batch, batch_idx) -> Tuple[Tensor, Dict[int, Tensor]]:
        features, targets = batch[0], batch[1]
        predictions = self.forward(features)
        loss = self.criterion(predictions, targets)
        _, predicted_classes = predictions.topk(max(self.topk))
        topk = mean_topk_accuracy(predicted_classes, targets, k=self.topk)
        return loss, topk

    def training_step(self, batch, batch_idx) -> Tuple[Tensor, Dict[str, Tensor]]:
        loss, topk = self.shared_step(batch=batch, batch_idx=batch_idx)
        log_dict = {"train_online_cls_loss": loss}
        log_dict.update({f"train_online_cls_top{k}": acc for k, acc in topk.items()})
        return loss, log_dict

    def validation_step(self, batch, batch_idx) -> Tuple[Tensor, Dict[str, Tensor]]:
        loss, topk = self.shared_step(batch=batch, batch_idx=batch_idx)
        log_dict = {"val_online_cls_loss": loss}
        log_dict.update({f"val_online_cls_top{k}": acc for k, acc in topk.items()})
        return loss, log_dict



================================================
FILE: lightly/utils/benchmarking/topk.py
================================================
from typing import Dict, Sequence

import torch
from torch import Tensor


def mean_topk_accuracy(
    predicted_classes: Tensor, targets: Tensor, k: Sequence[int]
) -> Dict[int, Tensor]:
    """Computes the mean accuracy for the specified values of k.

    The mean is calculated over the batch dimension.

    Args:
        predicted_classes:
            Tensor of shape (batch_size, num_classes) with the predicted classes sorted
            in descending order of confidence.
        targets:
            Tensor of shape (batch_size) containing the target classes.
        k:
            Sequence of integers specifying the values of k for which the accuracy
            should be computed.

    Returns:
        Dictionary containing the mean accuracy for each value of k. For example for
        k=(1, 5) the dictionary could look like this: {1: 0.4, 5: 0.6}.
    """
    accuracy = {}
    targets = targets.unsqueeze(1)
    with torch.no_grad():
        for num_k in k:
            correct = torch.eq(predicted_classes[:, :num_k], targets)
            accuracy[num_k] = correct.float().sum() / targets.shape[0]
    return accuracy



================================================
FILE: lightly/utils/cropping/__init__.py
================================================



================================================
FILE: lightly/utils/cropping/crop_image_by_bounding_boxes.py
================================================
import os.path
import warnings
from pathlib import Path
from typing import List

from PIL import Image
from tqdm import tqdm

from lightly.data import LightlyDataset
from lightly.utils.bounding_box import BoundingBox


def crop_dataset_by_bounding_boxes_and_save(
    dataset: LightlyDataset,
    output_dir: str,
    bounding_boxes_list_list: List[List[BoundingBox]],
    class_indices_list_list: List[List[int]],
    class_names: List[str] = None,
) -> List[List[str]]:
    """Crops all images in a dataset by the bounding boxes and saves them in the output dir

    Args:
        dataset:
            The dataset with the images to be cropped. Must contain M images.
        output_dir:
            The output directory to saved the cropped images to.
        bounding_boxes_list_list:
            The bounding boxes of the detections for each image. Must have M sublists, one for each image.
            Each sublist contains the bounding boxes for each detection, thus N_m elements.
        class_indices_list_list:
            The object class ids of the detections for each image. Must have M sublists, one for each image.
            Each sublist contains the bounding boxes for each detection, thus N_m elements.
        class_names:
            The names of the classes, used to map the class id to the class name.


    Returns:
        The filepaths to all saved cropped images. Has M sublists, one for each image.
        Each sublist contains the filepath of the crop each detection, thus N_m elements.

    """
    filenames_images = dataset.get_filenames()
    if len(filenames_images) != len(bounding_boxes_list_list) or len(
        filenames_images
    ) != len(class_indices_list_list):
        raise ValueError(
            "There must be one bounding box and class index list for each image in the datasets,"
            "but the lengths dont align."
        )

    cropped_image_filepath_list_list: List[List[str]] = []

    print(f"Cropping objects out of {len(filenames_images)} images...")
    for filename_image, class_indices, bounding_boxes in tqdm(
        zip(filenames_images, class_indices_list_list, bounding_boxes_list_list)
    ):
        if not len(class_indices) == len(bounding_boxes):
            warnings.warn(
                UserWarning(
                    f"Length of class indices ({len(class_indices)} does not equal length of bounding boxes"
                    f"({len(bounding_boxes)}. This is an error in the input arguments. "
                    f"Skipping this image {filename_image}."
                )
            )
            continue

        filepath_image = dataset.get_filepath_from_filename(filename_image)
        filepath_image_base, image_extension = os.path.splitext(filepath_image)

        filepath_out_dir = os.path.join(output_dir, filename_image).replace(
            image_extension, ""
        )
        Path(filepath_out_dir).mkdir(parents=True, exist_ok=True)

        image = Image.open(filepath_image)

        cropped_images_filepaths = []
        # For every image, crop out multiple cropped images, one for each
        # bounding box
        for index, (class_index, bbox) in enumerate(
            (zip(class_indices, bounding_boxes))
        ):
            # determine the filename and filepath of the cropped image
            if class_names:
                class_name = class_names[class_index]
            else:
                class_name = f"class{class_index}"
            cropped_image_last_filename = f"{index}_{class_name}{image_extension}"
            cropped_image_filepath = os.path.join(
                filepath_out_dir, cropped_image_last_filename
            )

            # crop out the image and save it
            w, h = image.size
            crop_box = (w * bbox.x0, h * bbox.y0, w * bbox.x1, h * bbox.y1)
            crop_box = tuple(int(i) for i in crop_box)
            cropped_image = image.crop(crop_box)
            cropped_image.save(cropped_image_filepath)

            # add the filename of the cropped image to the corresponding list
            cropped_image_filename: str = os.path.join(
                filename_image.replace(image_extension, ""), cropped_image_last_filename
            )
            cropped_images_filepaths.append(cropped_image_filename)

        cropped_image_filepath_list_list.append(cropped_images_filepaths)

    return cropped_image_filepath_list_list



================================================
FILE: lightly/utils/cropping/read_yolo_label_file.py
================================================
from typing import List, Tuple

from lightly.utils.bounding_box import BoundingBox


def read_yolo_label_file(
    filepath: str, padding: float, separator: str = " "
) -> Tuple[List[int], List[BoundingBox]]:
    """Reads a file in the yolo file format

    Args:
        filepath:
            The path to the yolo file, usually a .txt file.
        padding:
            The relative passepartout / padding to be added around the bounding box
        separator:
            The separator character between the 5 values (class id, x, y, w, h) per row in the label file.

    Returns:
        Two lists, each with one element per row in the label file:
            The class indices.
            The bounding boxes.

    """
    with open(filepath, "r") as f:
        lines = f.readlines()

    class_indices = []
    bounding_boxes = []
    for line in lines:
        values = line.split(sep=separator)
        class_id, x_norm, y_norm, w_norm, h_norm = (float(val) for val in values)
        class_id = int(class_id)
        class_indices.append(class_id)

        w_norm *= 1 + padding
        h_norm *= 1 + padding
        bbox = BoundingBox.from_yolo_label(x_norm, y_norm, w_norm, h_norm)
        bounding_boxes.append(bbox)
    return class_indices, bounding_boxes



================================================
FILE: requirements/README.md
================================================
All dependencies are tracked in `pyproject.toml` and no new files should be added to
the `requirements` directory.

We maintain `base.txt` to allow installing the package only with the dependencies
necessary to use the API part of the package. The package can be installed with API
only dependencies by running:
```
pip install -r requirements/base.txt
pip install lightly --no-deps
```
This is also documented in our Lightly Worker docs:
https://docs.lightly.ai/docs/install-lightly#install-the-lightly-python-client

It is currently not possible to move these dependencies to an optional dependency
group in `pyproject.toml` because pip does not support installing only optional
dependencies. See https://github.com/pypa/pip/issues/11440

`openapi.txt` is automatically created by the API generator and should not be modified
manually.

There are tests in [`tests/test_requirements.py`](../tests/test_requirements.py) that
check that the dependencies in `base.txt` are in sync with the dependencies in
`pyproject.toml` and `openapi.txt`.

There is also a [GitHub Action](../.github/workflows/test_api_deps_only.yml) that
verifies that installing only the API part of the package works correctly. 



================================================
FILE: requirements/base.txt
================================================
# Minimal dependencies to use the package with the API client.
# See requirements/README.md for more information.
certifi>=14.05.14
hydra-core>=1.0.0
lightly_utils~=0.0.0
numpy>=1.18.1
python_dateutil>=2.5.3
requests>=2.27.0
six>=1.10
tqdm>=4.44
urllib3 >= 1.25.3
pydantic >= 1.10.5
aenum >= 3.1.11


================================================
FILE: tests/UNMOCKED_end2end_tests/README.md
================================================
This repository contains scripts to test the python package with a server. 

## Testing the Server API with CLI commands and active learning
You only need an account on the server.
Once you have a token from our production server `https://app.lightly.ai`, you can run:
```bash
cd ../../../lightly # ensure you are in the top directory
pip uninstall lightly -y

pip install . 
bash tests/UNMOCKED_end2end_tests/run_all_unmocked_tests.sh LIGHTLY_TOKEN
```

## Testing the Server API with CLI commands
You only need an account on the server and a dataset.
Once you have a token from our production server `https://app.lightly.ai`, you can run:
```bash
bash test_api_on_branch.sh path/to/dataset LIGHTLY_TOKEN
```

## Testing the API latency
This needs a token, but no dataset
```bash
LIGHTLY_TOKEN="MY_TOKEN" && python tests/UNMOCKED_end2end_tests/test_api_latency.py
```

## Testing the upload speed
Use the pycharm profile with yappi to run the function [benchmark_upload.py:benchmark_upload()](benchmark_upload.py). 
You can use the following script for example: [call_benchmark_upload](call_benchmark_upload.py)

